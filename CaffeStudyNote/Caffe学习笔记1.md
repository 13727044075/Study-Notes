# Caffe学习笔记1

标签（空格分隔）： Caffe 深度学习

---

这里记录下学习深度学习的一个开源框架**Caffe**过程中的笔记，主要是源码阅读过程中的笔记，顺便记录一些卷积神经网络的知识点。

参考的书是《深度学习--21天实战Caffe》

### Caffe简介

Caffe是一个被广泛使用的开源深度学习框架（在TensorFlow出现之前一直是深度学习领域GitHub star最多的项目），目前由伯克利视觉学中心（Berkeley Vision and Learning Center，BVLC）进行维护。Caffe的创始人是加州大学伯克利的Ph.D.贾扬清，他同时也是TensorFlow的作者之一，曾工作于MSRA、NEC和Google Brain，目前就职于Facebook FAIR实验室。**Caffe的主要优势包括如下几点：**

1. **容易上手，网络结构都是以配置文件形式定义，不需要用代码设计网络。**
2. **训练速度快，能够训练state-of-the-art的模型与大规模的数据。**
3. **组件模块化，可以方便地拓展到新的模型和学习任务上。**

**Caffe的核心概念是Layer，每一个神经网络的模块都是一个Layer。**Layer接收输入数据，同时经过内部计算产生输出数据。设计网络结构时，只需要把各个Layer拼接在一起构成完整的网络（通过写protobuf配置文件定义）。比如卷积的Layer，它的输入就是图片的全部像素点，内部进行的操作是各种像素值与Layer参数的convolution操作，最后输出的是所有卷积核filter的结果。

**每一个Layer需要定义两种运算，一种是正向（forward）的运算，即从输入数据计算输出结果，也就是模型的预测过程；另一种是反向（backward）的运算，从输出端的gradient求解相对于输入的gradient，即反向传播算法，这部分也就是模型的训练过程。**实现新Layer时，需要将正向和反向两种计算过程的函数都实现，这部分计算需要用户自己写C++或者CUDA（当需要运行在GPU时）代码，**对普通用户来说还是非常难上手的。**

正如它的名字Convolutional Architecture for Fast Feature Embedding所描述的，Caffe最开始设计时的目标只针对于图像，没有考虑文本、语音或者时间序列的数据，**因此Caffe对卷积神经网络的支持非常好，但对时间序列RNN、LSTM等支持得不是特别充分。同时，基于Layer的模式也对RNN不是非常友好，定义RNN结构时比较麻烦。在模型结构非常复杂时，可能需要写非常冗长的配置文件才能设计好网络，而且阅读时也比较费力。**

**Caffe的一大优势是拥有大量的训练好的经典模型（AlexNet、VGG、Inception）乃至其他state-of-the-art（ResNet等）的模型，收藏在它的Model Zoo（github.com/BVLC/ caffe/wiki/Model-Zoo）。**因为知名度较高，Caffe被广泛地应用于前沿的工业界和学术界，许多提供源码的深度学习的论文都是使用Caffe来实现其模型的。在计算机视觉领域Caffe应用尤其多，可以用来做人脸识别、图片分类、位置检测、目标追踪等。

虽然Caffe主要是面向学术圈和研究者的，但**它的程序运行非常稳定，代码质量比较高，所以也很适合对稳定性要求严格的生产环境，可以算是第一个主流的工业级深度学习框架。**因为Caffe的底层是基于C++的，因此可以在各种硬件环境编译并具有良好的移植性，支持Linux、Mac和Windows系统，也可以编译部署到移动设备系统如Android和iOS上。和其他主流深度学习库类似，Caffe也提供了Python语言接口pycaffe，在接触新任务，设计新网络时可以使用其Python接口简化操作。

不过，通常用户还是使用Protobuf配置文件定义神经网络结构，再使用command line进行训练或者预测。Caffe的配置文件是一个JSON类型的.prototxt文件，其中使用许多顺序连接的Layer来描述神经网络结构。Caffe的二进制可执行程序会提取这些.prototxt文件并按其定义来训练神经网络。**理论上，Caffe的用户可以完全不写代码，只是定义网络结构就可以完成模型训练了。**

Caffe完成训练之后，用户可以把模型文件打包制作成简单易用的接口，比如可以封装成Python或MATLAB的API。不过在.prototxt文件内部设计网络节构可能会比较受限，没有像TensorFlow或者Keras那样在Python中设计网络结构方便、自由。

更重要的是，**Caffe的配置文件不能用编程的方式调整超参数，也没有提供像Scikit-learn那样好用的estimator可以方便地进行交叉验证、超参数的Grid Search等操作。**Caffe在GPU上训练的性能很好（使用单块GTX 1080训练AlexNet时一天可以训练上百万张图片），但是目前仅支持单机多GPU的训练，没有原生支持分布式的训练。庆幸的是，现在有很多第三方的支持，比如雅虎开源的CaffeOnSpark，可以借助Spark的分布式框架实现Caffe的大规模分布式训练。

### Caffe 代码梳理
#### Caffe 目录结构
  首先在Caffe的根目录下执行`tree -d`命令可以查看Caffe目录结构，其目录结构如下图所示。

  ![此处输入图片的描述][1]

  其中**cmake**目录是编译时用的，不需要关注；而**data**是存放原始数据及数据获取脚本，主要是存放了包括**cifar10，mnist**原始图片数据以及**ilsvrc12**的Meta数据；**docker**是为了便于迁移，使用了Docker工具；**docs**存放了官方文档以及doxygen工程文件；**examples**是存放一些简单例程，包括**cifar10，mnist，imageNet**的例程，也就是存放了训练这些数据用的模型，solver文件以及训练脚本；**include**存放Caffe头文件；**matlab**适用于Matlab做Warpper；**models**存放示例模型，包括经典的AlexNet模型；**python**是用于python做Wrapper；**scripts**存放脚本；**src**放的就是Caffe源码；**tools**是存放常用工具源码。

  其中需要关注的主要是**include，src和tools**三个子目录。

#### 如何有效阅读Caffe源码
  阅读源码的最好路线如下：

1. 首先从`src/caffe/proto/caffe.proto`开始，了解基本数据内存对象和磁盘文件的一一映射关系（如何从磁盘文件加载一个数据结构到内存对象，以及如何将内存对象保存为磁盘文件，这中间的过程其实都是由**ProtoBuffer**工具自动完成的）。
2. 第二步就是看头文件。先通过头文件类声明理解整个框架，从基类向派生类顺藤摸瓜看下去。
3. 第三步就是有针对性地区看cpp和cu文件了。这一阶段关注点在算法实现上，相应的测试和正确性验证手段是必须的。
4. 第四步就很自由了，可以编写各类工具，集成到Caffe内部。在**tools**文件夹下面已经有很多实用工具（如训练模型，测试模型，特征提取，转换数据格式等），可以根据需要修改。另外，还可以学习用Python或者Matlab包装Caffe的方法，便于调节模型训练效果。

**Linux使用技巧之阅读代码时如何快速追踪某个关键词**

**方法一：**打开多个终端，或者用水平/垂直切分命令，将窗口切分成多分，将cpp文件包含的所有头文件都打开，利用**vi**的查找命令进行追踪。这种方法在工程较简单，头文件较少，或者对工程比较熟悉，了解每个头文件的内容时比较好用；而在工程较大，包含头文件较多，以及递归层次较多的场合下就不实用了。

**方法二：**使用 **Linux 的 grep**命令。例如在 src/caffe/layer_factory.cpp 中有个宏调用： REGISTER_LAYER_CREATOR(Pooling, GetPoolingLayer);，这里可以在 Caffe 根目录下运行以下命令：

```
 grep -n -H -R "REGISTER_LAYER_CREATOR"
```

命令行参数解释：
-n   ——显示行号，便于定位
-H   ——显示文件名，便于定位
-R   ——递归查找每个子目录，适合工程较大、分多个目录存放的场景

### 卷积神经网络知识点
  Caffe最主要支持的深度学习算法就是卷积神经网络(CNN)。在CNN中，卷积层用于提取低维到高维特征，经典的AlexNet网络结构中，在前面5层都是卷积层；全连接层是在网络的最后2层或者3层，它类似于多层感知器（MLP），用于对前面提取的特征进行分类**。卷积层和全连接层可称为权值层，因为这两种层都具有可学习参数(权值）,是网络训练的对象，**下面首先介绍这两种层的具体实现。

#### 卷积层
  卷积是大自然中最常见的运算，一切信号观测、采集、传输和处理都可以用卷积过程实现，其用公式表达如下：
$$
\begin{align}
Y(m,n) & =X(m,n)*H(m,n) \\ 
&= \sum_{i=-\infty}^{+\infty}\sum_{j=-\infty}^{+\infty}X(i,j)H(m-i,n-j) \\ &=\sum_{i=-\infty}^{+\infty}\sum_{j=-\infty}^{+\infty}X(m-i,n-j)H(i,j)
\end{align}
$$
上述公式中$H(m,n)$表示卷积核。

在CNN中的卷积层的计算步骤与上述公式定义的二维卷积有点差异，首先是维度升至三维、四维卷积，跟二维卷积相比多了一个**“通道”(channel)**，每个通道还是按照二维卷积方式计算，而多个通道与多个卷积核分别进行二维卷积，得到多通道输出，需要“合并”为一个通道；**其次是卷积核在卷积计算时没有“翻转”，而是与输入图片做滑动窗口“相关”计算**。用公式重新表达如下：
$$
Y^l(m,n) =X^k(m,n)*H^{kl}(m,n) = \sum_{k=0}^{K-1}\sum_{i=0}^{I-1}\sum_{j=0}^{J-1}X^k(m+i,n+j)H^{kl}(i,j)
$$

这里假定卷积层有$L$个输出通道和$K$个输入通道，于是需要有$KL$个卷积核实现通道数目的转换。其中$X^k$表示第$k$个输入通道的二维特征图，$Y^l$表示第$l$个输出通道的二维特征图，$H^{kl}$表示第$k$行、第$l$列二维卷积核。假定卷积核大小是$I*J$,每个输出通道的特征图大小是$M*N$，则该层每个样本做一次前向传播时卷积层的计算量是$Calculations(MAC)=I*J*M*N*K*L$。

这些参数可以在训练的网络结构模型中找到或者计算得到，比如以 LeNet-5 第一个卷积层为例，在文件`examples/mnist/lenet_train_test.prototxt`中找到卷积参数描述，如下所示：

![此处输入图片的描述][2]

这里的`num_output: 50`对应的就是输出通道$L$,`kernel_size: 5 `表示的就是卷积核大小$I,J$,而输出的特征图大小可以通过公式$outputW = \frac{inputW+2*pad-kernelW}{stride}+1$，而输入通道$k$，这里就是输入图片的通道数了。当然更方便的是查看日志文件了。

实际上CNN训练的时候通常是使用批量梯度下降法，也就是每次送入一批样本(batch)，所以计算量应该再乘上批量大小。

通过上述参数，还可以计算出卷积层的学习参数，也就是**卷积核数目乘以卷积核的尺寸--$Params = I*J*K*L$。**

这里定义计算量-参数量之比是**CPR**=$Calculations/Params=M*N$。

因此可以得出结论：**卷积层的输出特征图尺寸越大，CPR越大，参数重复利用率越高。若输入一批大小为B的样本，则CPR值可提高B倍。**

#### 全连接层
  在CNN出现之前，最早的深度学习网络计算类型都是全连接形式的。

  全连接层的主要计算类型是**矩阵-向量乘（GEMV)。**假设输入节点组成的向量是$x$，维度是$D$,输出节点组成的向量是$y$,维度是$V$,则全连接层计算可以表示为$y=Wx$。

  其中$W$是$V*D$的权值矩阵。

  全连接层的参数量为$Params=V*D$,其单个样本前向传播的计算量也是$Calculations(MAC)=V*D$，也就是$CPR=Calculations/Params=1$。也就是其权值利用率很低。

  可以将一批大小为$B$的样本$x_i$逐列拼接成矩阵$X$，一次性通过全连接层，得到一批输出向量构成的矩阵$Y$，相应地前面的矩阵-向量乘运算升为**矩阵-矩阵乘计算（GEMM)：$Y=WX$**。

  这样全连接层前向计算量提高了$B$倍，CPR相应提高了$B$倍，权重矩阵在多个样本之间实现了共享，可提高计算速度。

  比较卷积层和全连接层，卷积层在输出特征图维度实现了**权值共享**，这是降低参数量的重要举措，同时，卷积层**局部连接**特性（相比全连接）也大幅减少了参数量。**因此卷积层参数量占比小，但计算量占比大，而全连接层是参数量占比大，计算量占比小。所以在进行计算加速优化时，重点放在卷积层；在进行参数优化、权值剪裁时，重点放在全连接层。**

  


[1]: http://7xrluf.com1.z0.glb.clouddn.com/caffe1.png
[2]: http://7xrluf.com1.z0.glb.clouddn.com/caffe2.png