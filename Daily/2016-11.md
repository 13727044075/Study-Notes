#### 10月总结
##### 研究生工作
  10月份首先是6号投稿到华工学报，在满怀希望中，到月底的时候得知初审不过，被拒了，果然还是太水了，老师说是要再找下有没有其他期刊会收录这种发布数据库类型的论文，只是感觉希望不大。除此之外，主要就是看了之前7月份老师发的一个CVPR2016的几篇精细分类的论文，看完跟老师说，老师却说只有属性标签有用，感觉这简直就是浪费我的时间嘛，当初我就觉得只有那篇涉及到食物的精细分类的论文中的方法有用，其他几篇看了介绍都是说基于部件的，但之前老师又说不能只局限在食物分类，花了一周时间，结果就是得到这样一句话。最后一个星期，根据网上教程修改了Caffe源码，可以接受多个标签，但是多标签训练等价于多任务，训练的loss都是分开计算的，得到的准确率也是分开计算，我希望是结合在一起的，还是有一定工作量要做。
  
##### 编程学习
  这个月基本每天都有在牛客网刷至少10道C++编程题，现在可以做两星难度，偶尔做下三星难度，数据结构的练习题也是有偶尔做下。《大话数据结构》也看完树的部分了，并做了点笔记，剩下后面3章内容。算法题，在LeetCode上也做了有快20道题目，之前做了六七道链表相关的算法题，最近则是开始做跟树有关的题目。暂时是做同一个类型系列的算法题。
  
##### 机器学习进展
  Caffe源码阅读是看完了。《机器学习》看了决策树一章，然后也看了几篇机器学习，反向传播，神经网络，CNN的技术文章。
  

---
#### 11月计划

##### 研究生工作
  12月份就要开题了，所以这个月需要好好花时间在科研上，首先是希望能尽快实现属性标签的方法，能够找到突破口，有创新的方向。此外，希望可以找到一个新的期刊投稿，并且能通过初审，可以被收录吧。
  
##### 编程学习
  首先编程练习，每天做个10道左右。然后《大话数据结构》希望能在10号前完成，然后开始看《剑指Offer》，算法题要能平均一天一道左右。
  
  然后就是开始要每天看至少半个小时的《现代操作系统》，将比较重要的几章看完，然后看《TCP/IP 协议1》，希望这个月可以看完这两本书吧，然后再做做笔记。
  
  机器学习部分是打算停止了，其实到现在为止，看的内容就比较少，可能会有空的时候看看SVM等算法吧，首先还是要看看科研的进度来说。


---
##### 11.5 总结
  最近一周，简单汇报了基本没什么进度的工作，然后由于感冒，还有最近也开始玩一个手游《王者荣耀》，导致这一周的效率是非常低。
  
  暂时科研进展不大，要实现论文的BGL方法，虽然说是只需要修改最后一个全连接层和loss层，但是目前还没有看明白，所以暂时真的不知道如何修改，真的需要好好静下心再看下这篇论文中方法的介绍，以及实验部分的细节介绍。
  
  《大话数据结构》目前是看到第七章的最短路径部分，还有3个小节部分，看来10号前完成这本书有很大难度了，希望是15号前可以看完吧，然后需要做做笔记，其实做笔记也是需要不少时间，不过现在突然不看机器学习的内容了，还是有点不太适应，以前还会想着，现在就少一件事情了，只需要考虑科研和编程方面了。
  
  科研接下来一周肯定要有更大的进展才行，目标是除了实现BGL这篇论文的方法，还实现triplet loss的方法。
  
  编程方面，暂时继续看《大话数据结构》，然后刷题也要继续保持，无论是编程的练习还是刷算法题目，然后是要抽时间看《现代操作系统》。
  
  最后就是由于生病了，已经好久没有跑步了，锻炼方面也就周四晚上去打羽毛球了。等感冒好了，再继续跑步，继续增加力量训练，锻炼身体。
  

---
##### 11.8 总结
  这两天开始仔细看BGL的论文，昨天是将之前修改的多标签方法的caffe代码上传到GitHub上，现在是修改了`caffe.proto`增加了 BGL 的loss层，然后新建了一个头文件，将需要的一些函数，比如构造函数，层的配置函数，前向传播和反向传播函数都写好了，不过现在需要考虑究竟是只当做一个loss层，还是要结合全连接层的作用，BGL的方法是对增加的标签也要有权值的，也就是这些权值也是需要反向传播进行训练的，总感觉还是很不通啊，还没能完全搞明白，现在连前向计算的都还是没有百分百搞懂，更不用说反向传播了。但还是必须修改代码，反复尝试吧，注定会有一段比较痛苦的修改代码经历了。
  
  《大话数据结构》就快看完第七章了，不过总结的进度还是比较慢，需要快点赶上。然后编程练习和数据练习还是继续做。算法题也是有在做，但是题目也开始变难了，打算做够10道树相关的题目就换成图相关的。
  
  锻炼方面，上周日也就是前天是做了哑铃的手臂训练+腹肌训练+HIIT训练，昨天则是恢复跑步训练，快两周没跑，所以只是跑了3.5公里，刷新了下最快的配速，达到5分57秒。今晚，应该是可以继续跑步的。

---
##### 11.23 总结
  有两周没有总结一下了。
  
  科研方面，BGL的方法写了CPU方法，但是只是实现其中一个部分，但是还是出现内存问题，它的loss是由三部分组成的，现在感觉还是很有问题，又在尝试实现一下GPU方法，毕竟跑CPU的速度太慢了，还是实现GPU来试试，不过GPU的代码也写得好艰难，现在连一部分内容也没有完成甚至已经开始出现要放弃这个念头了。
  
  centerLoss的方法是加入了Caffe了，也跑了实验，直接按照论文的网络结构似乎不能成功训练我的数据库，重新修改了网络结构，全连接层分两路，一路是softmax的，一路是center的，但是softmax的输入却使用两路最后的FC层的连接后的输出，直接训练的结果是有接近1个百分点的提高，但是微调话，反而下降了3个百分点，然后尝试在food-101上实验，直接训练的话，似乎没有效果，至少跑了应该有1W次吧，loss一直不下降，微调的话，下降了八个百分点。跟老板汇报了情况，老板反而认为这个方法应该更有普遍性，可能是我没有调好参数，要求我不能只专注于自己的shu，要考虑整个精细分类的，设计的自己的算法也是要具有通用性。但是现在还没有比较好的想法。
  
  最近也看到一个方法，largeMarginLoss，也是在loss上做文章，目的是增大类间的距离，它是修改了最后一个FC层，使得分类的难度增大，从而提高网络的区分能力。不过现在网上找到的代码还缺少反向，真是越来越考验自己的写代码能力了。加油吧。。
  
  编程方面，20号周日的时候完成《大话数据结构》，并总结了排序算法。当然，图部分和查找算法还没有总结，这个有空进行总结。然后这两天开始看《剑指offer》，看到第二章，每天花个1个半小时来看吧，这本书也是介绍了不同算法题，所以算法练习可以暂时暂停，等完成这本书的阅读后再进行。
  
  《现代操作系统》看到第三章了，要保证每天看10到15页的内容。
  
  关于科研方面的想法，首先其实也是想向centerLoss那篇论文一样将最后或者倒数第二个FC层的特征可视化，看看自己的数据库得到的是怎样的，然后考虑怎么设计算法，同样其实也是要往增大类间的距离和减小类间距离的方向吧，看能不能设计出自己的这种算法。然后，之前也有考虑过训练过程中丢弃一些训练很好的特征图，重点训练效果不好的，这个要看看dropout层的设计了；其实还是希望只用类别标签就可以了，如果是多标签的话，则要考虑最后的loss怎么综合利用多标签了，除了目前看的属性标签的联合概率，看能否有其他实现方法，可以促进提高。
  

---
##### 11.30 总结
  月末总结了。
  
  科研方面，进度不太理想，`LargeMarginLoss`方法的反向传播代码是自己的，前向还有其他函数则是参考网上一篇博客给出的，但是训练得到的loss却是太大而溢出了，还是需要调试，看看究竟是哪段代码出问题了。
  
  现在有个想法，或者是一直以来的想法，就是在dropout层做点文章，之前听到一个用这个层来丢弃训练较好的featuremap，重点训练较差的样本，现在打算结合最近看的loss函数，如`centerLoss`，不过它们的实现都没有使用这个网络层，是因为会有所影响，我在想，如果丢弃的时候只是丢弃部分较好的，保留剩下的较好的部分却可以赋予一个较高的权值，然后计算类中心的时候能够往这些权值较大的样本靠近，应该是可以尝试的，不过要实现有点困难，何时进行抛弃，抛弃的标准，这个保留部分的权值如果进行传递到最后的centerLoss层里面，都是需要考虑的。
  
  