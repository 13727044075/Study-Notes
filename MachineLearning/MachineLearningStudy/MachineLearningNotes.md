### 机器学习算法知识点

* 序列标注模型有**HMM、MEMM、CRF**。

* 能够用于指导特征选择的指标有**信息增益、信息增益率、基尼系数和信息熵**

* 用户使用**稀疏特征**进行训练时，对于**离散特征缺省值**的处理方法是**将缺省值付给一个全新值来标记**。

* **使用L2正则化可以得到平滑的权值**。

* 核回归中，最影响回归的过拟合性和欠拟合之间平衡的参数为核函数的宽度。

* 主要用来解无约束优化问题的优化算法有：**随机梯度下降， LBFGS， 共轭梯度法，拟牛顿法**

* 机器学习时间序列模型有：**移动平均法(MA)、自回归模型(AR)、自回归滑动平均模型(ARMA)、GARCH模型和指数平滑法。**具体参考[机器学习：时间序列模型](http://blog.csdn.net/ztf312/article/details/50890267)。

* 伪逆法：径向基（RBF）神经网络的训练算法，径向基解决的就是**线性不可分**的情况。

  感知器算法：**线性分类模型**。

  H-K算法：在最小均方误差准则下求得权矢量，二次准则解决**非线性问题**。

  势函数法：**势函数非线性**。

* 常见的**判别**模型有：**支持向量机、神经网络、线性判别分析、线性回归、逻辑回归、Boosting、条件随机场、k近邻、决策树、最大熵模型、区分度训练**。

* **生成**模型常见的主要有：**高斯模型、朴素贝叶斯、混合多项式、混合高斯模型和其他类型的混合模型、隐马尔可夫模型、LDA（Latent Dirichlet Allocation）、受限玻尔兹曼机(RBM)**。

* 位势函数法的积累势函数K(x)的作用相当于Bayes判决中的**后验概率或者类概率密度与先验概率的乘积**。

* 统计模式分类问题中，当**先验概率未知**时，可以使用**最小最大损失准则和N-P判决**。

* **最小最大损失准则**：考虑p(wi)变化的条件下，是风险最小

  **最小误判概率准则**， 就是判断p(w1|x)和p(w2|x)哪个大，x为特征向量，w1和w2为两分类，根据贝叶斯公式，需要用到先验知识

   **最小损失准则**，在最小误判概率准则的基础之上，还要求出p(w1|x)和p(w2|x)的期望损失，因为最小误判概率准则需要先验概率，所以最小损失准则也需要先验概率

   **N-P判决，**即限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算p(x|w1)和p(x|w2)的比值，不需要用到贝叶斯公式

* 机器学习模型 **error = bias^2 + variance**，其中**bias**反映的是**模型在样本上的输出与真实值之间的误差**，即**模型本身的精准度**，**Variance**反映的是**模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性**。更详细参考[机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？](https://www.zhihu.com/question/27068705)。

  ​

------

##### 1. 逻辑回归与线性回归

* 逻辑回归的损失函数是**log 对数损失函数**,其目标是**最大化似然函数**.

------

##### 2. k-近邻

------

##### 3. 朴素贝叶斯

* 朴素贝叶斯是**最小化后验概率**

------

##### 4. 决策树

------

##### 5. 支持向量机

* SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面,属于**结构风险最小化**

* SVM可以通过正则化系数控制模型的复杂度,避免过拟合.

* 如果**C值很大**，我们的松弛变量$ξ_i$就会很小，我们允许的误差也就很小，因此分类器会尽量正确分类样本，结果我们就会得到一个**小间距的超平面**，这样的结果可能会导致在新样本的分类性能很差，而在训练集上的分类性能很好，也就是出现**过拟合（overfitting）**现象;如果**C值很小**，我们**所允许的误差也就很大**，分类器即使要错误地分类样本，它也会找寻**大间距的超平面**，在这种情况下，如果你的训练集是线性可分的，也有可能出现错误分类的样本。

  因此，在SVM算法的训练上，我们可以通过**减小C值来避免overfitting的发生**

* SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。

------

##### 6. 提升方法(AdaBoost, 提升树Boosting Tree)

* AdaBoost算法中，所有被错分的样本的权重更新比例相同。

* Bagging与Boosting的区别：**取样方式不同**。**Bagging采用均匀取样，而Boosting根据错误率取样**。Bagging的各个预测函数**没有权重**，而Boosting是有权重的，Bagging的各个预测函数可以**并行生成**，而Boosing的各个预测函数只能**顺序生成**。

  ​

------

##### 7. EM算法

* EM算法是无监督学习方法
* **高斯混合模型（GMM）**的**极大似然估计（MLE）**只能得到**非退化（协方差矩阵正定）的局部最优解。**

------

##### 8. 隐马尔可夫模型(HMM)

* 无监督训练方法是**EM**算法

------

##### 9. 条件随机场(CRF)

* 1）CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。**特征设计灵活**（与ME一样） ————与HMM比较
  （2）同时，由于CRF**计算全局最优输出节点**的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较
  （3）CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较
  缺点：**训练代价大、复杂度高，速度慢**

* CRF 的优点：**特征灵活，可以容纳较多的上下文信息，能够做到全局最优**

  CRF 的缺点：**速度慢**

* ​

  ​

------

##### 10. 深度学习

* 主要用于无监督的深度学习网络是**Restricted Boltzmann Machines, AutoEncoder, Deep Belief Networks**

------

##### 11 降维

* 降维的算法有**PCA, Word2Vec，AutoEncoder, Latent Dirichlet Allocation，LASSO,小波分析法，线性判别法，拉普拉斯特征映射**。具体参考[数据降维方法小结](http://blog.csdn.net/yujianmin1990/article/details/48223001)。

------

##### 12 聚类

* 影响聚类算法效果的主要原因有：**特征选取、模式相似性测度和分类准则。**
* 主成分分析PCA和数据矩阵的奇异值分解SVD,**在数据均值归零后，SVD与PCA等价**，可以得到相同的特征空间。

------

#### 练习题

1. 已知两个一维模式类别的类概率密度函数为:

   ![img](https://uploadfiles.nowcoder.com/images/20160829/59_1472451822933_E39598100A5E449D6E3F3A28AB61F54B)

   先验概率P(1)=0.6,P(2)=0.4,则样本{x1=1.35,x2=1.45,x3=1.55,x4=1.65}各属于哪一类别?

解答：

> 概率问题基本上都是贝叶斯和全概率互相扯蛋,，他们之间往往可以通过条件概率建立联系。
>
> 本题中，要判断 xi 属于w1，还是w2，就是判断 p(w1 | xi)  和 p(w2 | xi)的大小关系。即在xi已经发生的情况下，xi 属于哪个类别（w1 ，w2）的可能性更大。
>
> p(w1 | xi) = p(xiw1) / p(xi) = p(xi | w1) * p(w1) / p(xi) = 0.6*(2 - xi) / p(xi)   // 因为xi都在 （1，2)范围
>
> p(w2 | xi) = p(xiw2) / p(xi) = p(xi | w2) * p(w2) / p(xi) = 0.4*(xi - 1) / p(xi)   // 因为xi都在 （1，2)范围
>
> 上面两等式相减，得：
>
> delta = p(w1 | xi) - p(w2 | xi) = (1.6 - xi) / p(xi)
>
> 所以，在上诉样本中，大于1.6的，属于w2，小于1.6的，属于w1。

2. 当X与Y相互 独立 时, Z=X+Y的密度 函数是 **X 的 密度 函数与 Y的密度 函数的卷积**

3. A为m＊n的实矩阵，那么矩阵AA'和A'A具有**相同的非零特征值。**

   ​