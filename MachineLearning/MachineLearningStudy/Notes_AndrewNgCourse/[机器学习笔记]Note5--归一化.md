# [机器学习笔记]Note5--归一化

标签（空格分隔）： 机器学习

---
[TOC]

继续是[机器学习课程](https://www.coursera.org/learn/machine-learning)的笔记，这节课会介绍归一化的内容。

### 过拟合问题
  这节课会介绍一个在机器学习过程中经常会遇到的问题--**过拟合**。通常，当我们有非常多的特征，我们可以学习得到的假设可能非常好地适应训练集，即代价函数可能几乎是0，但是可能会不能推广到新的数据，即泛化能力差，对于新的数据预测结果不理想。这就是过拟合问题。而特征非常多也是发生过拟合问题的一个原因之一。
  
  下面给出一个回归问题的例子，分别有3个模型，如下所示：
![此处输入图片的描述][1]

第一个模型是一个线性模型，低度拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过度拟合，虽然能非常好地适应我们的训练集，但在新输入变量进行预测时可能会效果不好；而中间的模型则是相对最合适的模型。

在回归问题中会有过拟合问题，同样在分类问题也是有过拟合的问题，例子如下所示：
![此处输入图片的描述][2]

同样是第一个模型是线性模型，只能低度拟合，而第三个模型是一个过渡拟合的模型，对新输入变量进行预测时效果会不好，只有中间的模型是最合适的模型。

那么，当发生过拟合的问题时，我们可以采取下面的措施来避免过拟合：
1. 丢弃一些不能帮助我们正确预测的特征：可以是手工选择保留哪些特征或者使用一些模型选择的算法来帮忙(例如PCA);
2. 归一化。保留所有的特征，但是减小参数的**大小**。

### 归一化代价函数
  在上述回归问题的例子中，对于过拟合的模型是$h_\theta(x) = \theta_0 + \theta_1x_1+\theta_2x_2^2+\theta_3x_3^3+\theta_4x_4^4$。

我们决定要减少$\theta_3和\theta_4$的大小，我们要做的是**修改代价函数**，在其中对$\theta_3和\theta_4$设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小的$\theta_3和\theta_4$。修改后的代价函数如下：
$$
min_\theta \frac{1}{2m} \sum_{i=1}^m ((h_\theta(x^{(i)}-y^{(i)})^2 + 10000\theta_3^2+100000\theta_4^2)
$$

通过这样的代价函数选择出来的$\theta_3和\theta_4$对预测结果的影响就会比之前小许多。

那么假如我们有许多的特征，我们并不知道其中哪些特征需要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。那么代价函数如下所示：
$$
J(\theta) = \frac{1}{2m} [\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum_{j=1}^n \theta_j^2)]
$$
**其中$\lambda$又称为归一化参数**。
注意：根据惯例，我们不对$\theta_0$进行惩罚。

经过归一化处理的模型与原模型的可能对比如下图所示：
![此处输入图片的描述][3]
 
 这里如果选择的归一化参数$\lambda$过大，则会把所有的参数都最小化，导致模型变成$h_\theta(x)=\theta_0$，也就是上图中红色直线所示的情况，造成低度拟合。
 
### 归一化线性回归
#### 梯度下降算法
  归一化线性回归的代价函数是：
$$
J(\theta) = \frac{1}{2m} [\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum_{j=1}^n \theta_j^2)]
$$
此时如要使用梯度下降法来令代价函数最小化，由于我们没有对$\theta_0$进行归一化，所以梯度下降法将会分两种情形:
> Repeat until convergence{
$$
 \theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_0^{(i)} \\
  \theta_j := \theta_j - \alpha [\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}+\frac{\lambda}{m}\theta_j] \\
  (for \; j=1,2,3,\ldots,n)
$$
}

其中，对第二个更新的式子进行调整可得：
$$
\theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}
$$
由该式子可以看出归一化线性回归的梯度下降算法的变化在于，**每次都在原有算法更新规则的基础上令$\theta$值减少了一个额外的值**，这是因为$1-\alpha \frac{\lambda}{m} \lt 1$，而后面第二项式子跟没有使用归一化时候是一样的。

#### 正规方程
同样，也可以使用正规方程来求解线性回归模型。这里做如下假设：
$$
X = \left[\begin{matrix} (x^{(1)})^T \\ \vdots \\ (x^{(m)})^T \end{matrix} \right] \quad y = \left[\begin{matrix} (y^{(1)}) \\ \vdots \\ (y^{(m)}) \end{matrix} \right]
$$
这里的$X$是训练集的矩阵，是一个$m*(n+1)$维矩阵，即有m个样本，每个样本有n+1个特征(实际是n个特征，但是每个样本都添加了一个$x_0=1$，而$y$则是一个$(m+1)*1$维的向量，表示的是训练样本的标签。

这里要求解归一化线性回归模型的方法如下：
$$
\theta = (X^TX+\lambda 
\begin{bmatrix}
0      & \cdots       \\
  0    & 1     & 0&\cdots &     \\
0& 0&1 & \cdots \\
 \vdots & \vdots & \ddots & \vdots \\
  \cdots    &  \cdots    & \cdots & \cdots & 1      \\
\end{bmatrix}
)^{-1}X^Ty
$$
这里的矩阵是一个$(n+1)*(n+1)$大小，并且是一个对角线上除了第一行外是1，例如当n=2时，这个矩阵就是$\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$

这个矩阵的得到是通过令$\frac{\partial}{\partial \theta_j}J(\theta) = 0$所得到的，不过视频中并没有给出详细的推导过程。

### 归一化逻辑回归
 对于逻辑回归，我们也可以得到一个归一化的代价函数表达式:
$$
J(\theta) = -\frac{1}{m} [\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)}log(1-h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2
$$

使用的梯度下降算法如下所示:
> Repeat until convergence{
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_0^{(i)} \\
  \theta_j := \theta_j - \alpha [\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}+\frac{\lambda}{m}\theta_j] \\
  (for \; j=1,2,3,\ldots,n)
$$
}

同样，看上去也是与线性回归的公式一样，但是由于$h_\theta(x) = g(\theta^TX)$,因此是与线性回归不同的。

### 小结
  本节主要是介绍了过拟合的问题，这是由于特征过多导致学习得到的模型非常拟合训练集的数据，但是对于新的输入数据却得到不理想的预测结果，解决这个问题的方法，一个是减少特征的数量，第二个就是使用归一化来减小参数的大小。
  
  因此后面分别介绍了归一化代价函数，以及线性回归和逻辑回归中梯度下降算法对于归一化代价函数的计算公式。
  
  




  [1]: http://img.blog.csdn.net/20160616203643718
  [2]: http://img.blog.csdn.net/20160616204319275
  [3]: http://img.blog.csdn.net/20160616220550902