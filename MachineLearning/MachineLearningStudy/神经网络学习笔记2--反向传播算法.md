# 神经网络学习笔记2--反向传播算法

参考文章：

1.[机器学习：一步步教你理解反向传播方法](http://yongyuan.name/blog/back-propagtion.html)

2.[A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)

3.[Backpropagation In Convolutional Neural Networks](http://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)

------

这里记录下神经网络中的反向传播算法，首先介绍普通的神经网络的反向传播实现过程，接着会介绍卷积神经网络的反向传播过程。

### 神经网络的反向传播过程

#### 概览

首先是给出一个例子，例子如下图，是一个有2个输入节点，2个隐层节点以及2个输出节点，隐层和输出节点都有一个偏置值。

![](http://7xrluf.com1.z0.glb.clouddn.com/BP1.png)

同时为了更方面说明和进行计算，这里给出一些初始的权重，偏置和输入以及期望的输出值：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP2.png)

由上图可以知道，输入时0.05和0.10，期望的输出是0.01和0.99，也就是输入的样本是(0.05,0.01)以及(0.01,0.99)。

反向传播的目标是对权重进行优化，使得神经网络能够学习到从任意的输入到输出的准确映射。

#### 前向传播

在进行反向传播前，首先需要进行前向传播，得到每个输出的实际输出值，然后才能计算输出的误差值，再进行反向传播的过程。

这里的激活函数是使用`Sigmoid`函数，其计算公式是$\sigma(x) =\frac{1}{1+e^{-x}}$。

下面是对于$h_1$全部网络输入的输入计算过程：
$$
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 *1
                = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 *1 = 0.3775
$$
然后将其输入到激活函数中，有：
$$
out_{h1} = \frac{1}{1+e^{-net_{h1}}}= \frac{1}{1+e^{-0.3775}}= 0.593269992
$$
对于$h_2$节点进行上述相同的过程，有：$out_{h2} = 0.596884378$

接着对于输出层，计算过程其实也是相同的，首先计算输出层的输入，然后再使用激活函数计算得到最终的输出值，计算过程如下所示：
$$
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1 = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967 \\
out_{o1} =  \frac{1}{1+e^{-net_{o1}}}=  \frac{1}{1+e^{- 1.105905967}}= 0.75136507 \\
out_{o2} = 0.772928465
$$

#### 计算总误差

这里采用平方和误差函数来计算输出层的误差：$E_{total} = \sum \frac{1}{2} (target - output)^2$

这里的$target$就是期望的输出，也就是$label$,而$output$就是计算的输出值，即预测值。

第一个神经元的输出误差为:$E_{o1} = \frac{1}{2}(target-output)^2 = \frac{1}{2}(0.01-0.75136507)^2=0.274811083$。

重复上述过程，可以得到第二个输出神经元的误差：$E_{o2}=0.023560026$

因此总的误差为$E_{total}=E_{o1}+E_{o2} = 0.27481103 + 0.023560026 = 0.298371109$

#### 反向传播

反向传播是从输出层一直往前传递误差，并在这个过程中更新所有的权重，使得新的权重可以让下一次前向传播后得到的输出误差变得更小。

##### 输出层的权重更新

先来考察权重$w_5$，我们想知道对于$w_5$的改变可以多大程度上影响总的误差，也就是$\frac{\partial E_{total}}{\partial w_5}$。

通过使用链式法则，可以得到$\frac{\partial E_{total}}{\partial w_5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{net_{o1}}{\partial w_5}$

为了更直观的表述上面链式法则的过程，对其进行可视化：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP3.png)

对上述链式法则得到的每一项分别进行计算。首先，整体误差关于各个神经元的输出为：
$$
E_{total} = \sum \frac{1}{2} (target - output)^2 = \frac{1}{2}(target_{o1}-output_{o1})^2 +  \frac{1}{2}(target_{o2}-output_{o2})^2 \\
\frac{\partial E_{total}}{\partial out_{o1}}  = 2 * \frac{1}{2}(target_{o1}-output_{o1})^{2-1}* (-1) + 0 \\  = -(target_{o1}-output_{o1}) \\ = -(0.01 - 0.75136507) \\ = 0.74136507
$$
而**Sigmoid**激活函数的偏导数是输出乘以1减去输出，即$\sigma(x)(1-\sigma(x))$。所以有：
$$
\frac{\partial out_{o1}}{\partial net_{o1}}=out_{o1}(1-out_{o1}) = 0.75136507 * (1 - 0.75136507) = 0.186815602
$$
然后就是$\frac{\partial net_{o1}}{\partial w_5}=out_{h1}$,这是由于$net_{o1} = net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1$,因此
$$
\frac{\partial E{total}}{\partial w5} = \frac{\partial E{total}}{\partial out{o1}} * \frac{\partial out{o1}}{\partial net{o1}} * \frac{net{o1}}{\partial w5}=0.74136507 * 0.186815602 * 0.593269992 = 0.082167041
$$
> 你也会看到用$\delta$规则表示的形式：
> $$
> \frac{\partial E_{total}}{\partial w_5} = -(target_{o1}-out_{o1}) * out_{o1}(1-out_{o1}) * out_{h1}
> $$
> 我们可以将$\frac{\partial E_{total}}{\partial out_{o1}}和\frac{\partial out_{o1}}{\partial net_o1}$写为$\frac{\partial E_{total}}{\partial net_{o1}}$,并用$\delta_{o1}$表示它，从而上述式子表示为：
> $$
> \delta_{o1} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_o1} \\
> \delta_{o1} = -(target_{o1}-out_{o1}) * out_{o1}(1-out_{o1})
> $$
> 因此有：
> $$
> \frac{\partial E_{total}}{\partial w_5} = \delta_{o1}out_{h1}
> $$
> 有一些论文中通过将负号从$delta$中提出来将其写为下面这种形式：
> $$
> \frac{\partial E_{total}}{\partial w_5} = -\delta_{o1}out_{h1}
> $$
>

为了减小误差，需要将原来的$w_5$减去目前的权重，一般会乘以一个学习率$\eta$,这里设置为0.5，则有：
$$
w_5^{+} = w_5 - \eta * \frac{\partial E_{total}}{\partial w_5}
$$
重复上面的过程，可以得到输出层其他的权重$w_6,w_7,w_8$的更新值。

注意，在继续向前推进反向传播的时候，在使用到$w_5,w_6,w_7,w_8$的时候，需要使用的是**原来的权重**，而不是更新后的权重，更新后的权重是在下一次前向传播过程中才使用到。

##### 隐层的权重更新

我们继续推进反向传播来计算$w_1、w_2、w_3和w_4$更新的权重：

同样使用链式法则，我们可以得到：
$$
\frac{\partial E{total}}{\partial w1} = \frac{\partial E{total}}{\partial out{h1}} * \frac{\partial out{h1}}{\partial net{h1}} * \frac{\partial net_{h1}}{\partial w1}
$$
同样可视化上述的链式法则：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP4.png)

对于隐层的更新，同样采用上述输出层的相似处理方式，但有所不同的是每个隐层的输出对于最终的输出都是有影响的，即$out_{h1}$对$out_{o1}和out_{o2}$都是有影响的，所以$ \frac{\partial E{total}}{\partial out{h1}} = \frac{\partial E_{o1}}{\partial out_{h1}} + \frac{\partial E_{o2}}{\partial out_{h1}} $

又由于$\frac{\partial E_{o1}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial out_{h1}}$

我们可以用前面计算的值来计算:$\frac{\partial E_o1}{\partial net_{h1}} = \frac{\partial E_{o1}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{h1}} = 0.74136507 * 0.186815602 = 0.138498562$

又因为$\frac{\partial net_{o1}}{\partial out_{h1}}=w_5=0.40$，因此将上述计算结果合起来可以得到：
$$
\frac{\partial E_{o1}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial out_{h1}} =0.138498562 * 0.4 = 0.055399425
$$
同理有$\frac{\partial E_{o2}}{\partial out_{h1}} = -0.019049119$，因此有
$$
\frac{\partial E{total}}{\partial out{h1}} = \frac{\partial E_{o1}}{\partial out_{h1}} + \frac{\partial E_{o2}}{\partial out_{h1}}=0.055399425 - 0.019049119 = 0.036350306
$$
现在计算好了$\frac{\partial E{total}}{\partial out{h1}} $,然后需要计算的就是$\frac{\partial out{h1}}{\partial net{h1}} 和 \frac{\partial net_{h1}}{\partial w1}$：
$$
\frac{\partial out{h1}}{\partial net{h1}}=out_{h1}(1-out_{h1}) = 0.59326999 * (1-0.59356999) = 0.241300709
$$

然后计算神经元$h_1$的全部输入关于权重$w_1$的偏导：
$$
net_{h1} = w_1 * i_1 + w_2 * i_2+ b_1 * 1 \\
\frac{\partial net_{h1}}{\partial w_1} = i_1 = 0.05
$$
再将上述各项求和，有：
$$
\frac{\partial E{total}}{\partial w1} = \frac{\partial E{total}}{\partial out{h1}} * \frac{\partial out{h1}}{\partial net{h1}} * \frac{\partial net_{h1}}{\partial w1} = 0.036360306 * 0.241300709 * 0.05 = 0.000438568
$$

> 你可能会看到下面的这种写法：
> $$
> \frac{\partial E_{total}}{\partial w_1} = (\sum_o \frac{\partial E_{total}}{\partial out_o} * \frac{\partial out_o}{\partial net_o} * \frac{\partial net_o}{\partial net_o}{\partial out_{h1}}) * \frac{\partial out_{h1}}{\partial net_{h1}} * \frac{\partial net_{h1}}{\partial w_1} \\
> \frac{\partial E_{total}}{\partial w_1} = (\sum_o \delta_o * w_{ho}) * out_{h1}(1-out_{h1}) * i \\
> \frac{\partial E_{total}}{\partial w_1} = \delta_{h1} i_1
> $$
>

现在就可以更新$w_1$：$w_1^+ = w_1 - \eta * \frac{\partial E_{total}}{\partial w_1} = 0.15 - 0.5 * 0.000438568 = 0.149780716$。

同理，可以得到更新后的$w_2^+,w_3^+,w_4^+$。

现在已经更新完所有的权重。最初的网络误差是0.298371109，经过第一次反向传播后，网络误差可以降低到0.291027924。虽然看起来下降不多，但是重复这个过程10000次以后，网络的误差就会下降到0.000035085。此时，再次输入0.05和0.1，两个神经元的输出就分别是0.015912196和0.98406734，非常接近期望的输出0.01和0.99了。

#### 小结

这里总结下，对于神经网络的反向传播过程，主要是要求误差对于待更新权值的偏导数，而计算方法是通过使用链式法则，将其一步一步分离出来，从而得到最终的计算结果。

假设$n$层的第$i$个神经元$a_i^{(n)}$,现在要更新它的第$j$个权值$w_{ij}^{(n)}$,误差值是$E$，神经元的输入是$input(a_i^{(n)})$,输出是$output(a_i^{(n)})$，那么通过链式法则有
$$
\frac{\partial E}{\partial w_{ij}^{(n)} }= \frac{\partial E}{\partial output(a_i^{ (n) } ) } * \frac{\partial output(a_i^{(n)} ) } {\partial input(a_i^{(n)}) } * \frac{\partial input(a_i^{(n)})}{\partial w_{ij}^{(n) }}
$$

### 卷积神经网络的反向传播过程

#### 简介

卷积神经网络(Convolutional Neural Networks,CNNs)是一个受到生物启发的**MLP**的一个改进版，它是模仿生物视觉皮层的基本结构。与**MLP**不用的是，在**CNNs**中的神经元会**共享权重**，这个改变可以不需要想过去一样为了减少内存的使用而减少**MLP**中的可训练权值的数量。下图是一个**CNN**的网络结构示例：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP5.png)

由上图可以看到，对于输入数据，首先有一个卷积层，该层的神经元是对输入的数据进行卷积操作，这里会使用带有权重的卷积滤波器。然后就是进入`pooling`层，这里会进行一个`pooling`操作，这是一种非线性的下采样操作，主要目的是为了减少参数的数量和网络的计算量，一般采用`Max-pooling`操作。

在经过多个卷积层和`pooling`层后，会得到很多尺寸变得更小的`feature map`以及提取到更多复杂的特征，接着就是几个全连接层，也就相当于一个**MLP**，最后就是进行预测，输出结果。

在卷积层和`pooling`层之间还会有一个网络层，现在一般是`ReLU`层，使用的是一个非饱和的激活函数$f(x) = max(0,x)$。

网络的最后一个全连接层是一个输出层，也是用于计算整个网络的`loss`值，也就是误差值。

##### 互相关函数

给定一个图片$I$,和一个核大小为$k \times k$的滤波器$F$，通过互相关函数得到的输出图片为$C$，其计算过程如下所示：
$$
\begin{align}
C &= I \otimes F \\
C(x,y) &= \sum_{a = 0}^{k - 1} \sum_{b = 0}^{k - 1} I(x+a, y+b)F(a,b) \tag {1}
\end{align}
$$

##### 卷积运算

给定一个图片$I$,和一个核大小为$k \times k$的滤波器$F$，通过卷积运算得到的输出图片为$C$，其计算过程如下所示：
$$
\begin{align}
C &= I \ast F \\
C(x,y) &= \sum_{a = 0}^{k - 1} \sum_{b = 0}^{k - 1} I(x-a, y-b)F(a,b) \tag {2}
\end{align}
$$
对比卷积的公式和互相关函数的公式，两者是非常相似的，除了卷积运算中，它的核会进行水平上和垂直方向上的翻转。

另外，上述两个公式中，对于$C(x,y)$的图像的像素坐标点$(x,y)$的取值范围是$0 \le x \le k-1, 0 \le y \le k-1$。

下面先定义一些符号。

* $l$表示第$l$层，其取值范围是$[1,L]$,即$L$表示最后一层；
* $w^l_{x,y}$表示连接$l$和$l+1$层的权重向量；
* $o^l_{x,y}$是第$l$层的输出向量：$o^l_{x,y} = w^l_{x,y} * a^l_{x,y} = \sum_{x^`}\sum_{y^`} w^l_{x^`,y^`}a^l_{x-x^`,y-y^`}$，这里的$*$表示卷积运算。
* $a^l$是一个第$l$层(必须是隐层)的激活函数输出向量：$a^l_{x,y} = f(o^{l-1}_{x,y})​$
* $f(x)$是激活函数，这里使用一个**ReLU**函数作为激活函数。
* $A^L$表示最后一层$L$的所有输入的矩阵表示，其中$a^L_{x,y} = f(o^{L-1}_{x,y})$ 
* $P$是训练集矩阵，$T$是标签矩阵

#### 前向传播

卷积运算中，卷积核会翻转$180^\circ $，并在输入的特征图(`feature map`)上根据设定的步进值像一个滑动窗口一样滑动。在每个像素点上，卷积核和特征图上对应重叠部分的区域会进行计算然后求和，得到的结果就是当前像素点上的数值。

上述过程会使用不同的卷积核进行重复计算，然后得到设定好的输出特征图数量。下面会展示卷积层中的权重共享这一概念的示意图：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP6.png)

卷积层的每个节点有一个大小为$3\times 3$的感受野，即每个节点只跟输入层的3个相邻的神经元相连，这是采用了稀疏连接的想法。上图中不同颜色的连线展示了卷积核的权重是如何在神经元之间进行共享的，同样颜色的权重是相等的。

为了实现卷积运算，卷积核会如下图所示进行翻转：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP7.png)

卷积运算的过程如下所示：
$$
\begin{align}
O(x,y) &= w_{x,y}^l \ast a_{x,y}^l + b_{x,y}^l \tag {3} \\
O(x,y) &= \sum_{x^{\prime}} \sum_{y^{\prime}} w_{x^{\prime},y^{\prime}}^l a_{x-x^{\prime},y-y^{\prime}}^l + b_{x,y}^l \tag {4} \\
&= \sum_{x^{\prime}} \sum_{y^{\prime}} w_{x^{\prime},y^{\prime}}^l f(o_{x-x^{\prime},y-y^{\prime}}^{l-1}) + b_{x,y}^l \tag {5}
\end{align}
$$
即如下图展示：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP8.png)

对于整个训练集$P$的预测，网络最终的输出值$a_p^L$以及相应的期望输出值$t_p$，这里使用平方和误差函数来计算误差：
$$
\begin{align}
E &= \frac{1}{2} \sum_{p =1}^P \left(t_p - a_p^L \right)^2 \tag {6}
\end{align}
$$

计算完误差后，为了减小误差，使得实际输出越来越接近标签值$T$，需要采用反向传播算法，对权值进行更新。

#### 反向传播

反向传播会进行权值的更新，方法是需要计算误差对每个权值的偏导数，采用链式法则，如下所示：
$$
\begin{align}
\frac{\partial E}{\partial w_{x,y}^l} &= \sum_{x^{\prime}} \sum_{y^{\prime}} \frac{\partial E}{\partial o_{x^{\prime},y^{\prime}}^{l}} \frac{\partial o_{x^{\prime},y^{\prime}}^{l}}{\partial w_{x,y}^l} \\
&= \sum_{x^{\prime}} \sum_{y^{\prime}} \delta^{l}_{x^{\prime},y^{\prime}} \frac{\partial o_{x^{\prime},y^{\prime}}^{l}}{\partial w_{x,y}^l} \tag {7}
\end{align}
$$
上式中，$o_{x^{\prime},y^{\prime}}^{l}=w_{x^{\prime},y^{\prime}}^{l}a_{x^{\prime},y^{\prime}}^{l} + b^l$，因此有
$$
\begin{align}
\frac{\partial o_{x^{\prime},y^{\prime}}^{l}}{\partial w_{x,y}^l} &= \frac{\partial}{\partial w_{x,y}^l}\left( \sum_{x^{\prime \prime}} \sum_{y^{\prime \prime}} w_{x^{\prime \prime},y^{\prime \prime}}^{l}a_{x^{\prime} - x^{\prime \prime},y^{\prime} - y^{\prime \prime}}^{l} + b^l \right) \\
&= \frac{\partial}{\partial w_{x,y}^l}\left( \sum_{x^{\prime \prime}} \sum_{y^{\prime \prime}} w_{x^{\prime \prime},y^{\prime \prime}}^{l}f\left(o_{x^{\prime} - x^{\prime \prime},y^{\prime} - y^{\prime \prime}}^{l-1}\right) + b^l \right)  \tag {8}
\end{align}
$$
继续展开上式，有：
$$
\begin{align}
\frac{\partial o_{x^{\prime},y^{\prime}}^{l}}{\partial w_{x,y}^l} &= \frac{\partial}{\partial w_{x,y}^l}\left( w_{0,0}^{l} f\left(o_{ x^{\prime} - 0, y^{\prime} - 0}^{l-1}\right) + \dots + w_{x,y}^{l} f\left(o_{ x^{\prime} - x, y^{\prime} - y}^{l-1}\right) + \dots + b^l\right) \\
&= \frac{\partial}{\partial w_{x,y}^l}\left( w_{x,y}^{l} f\left(o_{ x^{\prime} - x, y^{\prime} - y}^{l-1}\right) \right) \\
&= f\left( o_{x^{\prime} - x,y^{\prime} - y}^{l-1} \right) \tag {9}
\end{align}
$$
将等式(9)代入等式(7)，有如下结果：
$$
\begin{align}
\frac{\partial E}{\partial w_{x,y}^l} &= \sum_{x^{\prime}} \sum_{y^{\prime}} \delta^{l}_{x^{\prime},y^{\prime}} f\left( o_{x^{\prime} - x,y^{\prime} - y}^{l-1} \right) \tag {10} \\
&= \delta^{l}_{x,y} \ast f\left(o_{- x,- y}^{l-1} \right) \tag {11} \\
&= \delta^{l}_{x,y} \ast f\left( \text{rot}_{180^\circ} \left( o_{ x, y}^{l-1} \right) \right) \tag {12}
\end{align}
$$
等式(12)中可以看到对卷积核的翻转导致了从$f(o^{l-1}_{-x,-y})$到$f(rot_{180^\circ}(o_{x,y}^{l-1}))$的改变，下图展示了在反向传播中生成的梯度$\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22}$:

![](http://7xrluf.com1.z0.glb.clouddn.com/BP9.png)

在反向传播阶段，卷积核需要再次进行$180^\circ$的翻转从而进行卷积运算，重构输入的特征图，如下图所示：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP10.png)

这个重构过程如下图所示：

![](http://7xrluf.com1.z0.glb.clouddn.com/BP11.png)

在重构阶段，使用的是梯度$\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22}$,它们的计算公式是：
$$
\begin{align}
\delta^{l}_{x,y} &= \frac{\partial E}{\partial o_{x,y}^{l}} \tag {13}
\end{align}
$$
通过链式法则可以有如下结果：
$$
\begin{align}
\frac{\partial E}{\partial o_{x,y}^{l}} &= \sum_{x^{\prime}} \sum_{y^{\prime}} \frac{\partial E}{\partial o_{x^{\prime},y^{\prime}}^{l+1}}\frac{\partial o_{x',y'}^{l+1}}{\partial o_{x,y}^l} \\
&= \sum_{x^{\prime}} \sum_{y^{\prime}} \delta^{l+1}_{x^{\prime},y^{\prime}} \frac{\partial o_{x',y'}^{l+1}}{\partial o_{x,y}^l} \tag{14} \\
\end{align}
$$
等式(14)中，因为$o_{x^{\prime},y^{\prime}}^{l+1} = w_{x^{\prime},y^{\prime}}^{l+1}a_{x^{\prime},y^{\prime}}^{l+1} + b^{l+1}$，因此可以有：
$$
\begin{align}
\frac{\partial o_{x',y'}^{l+1}}{\partial o_{x,y}^l} &= \frac{\partial}{\partial o_{x,y}^l}\left( \sum_{x^{\prime \prime}} \sum_{y^{\prime \prime}} w_{x^{\prime \prime},y^{\prime \prime}}^{l+1}a_{x^{\prime} - x^{\prime \prime},y^{\prime} - y^{\prime \prime}}^{l+1} + b^{l+1} \right) \\
&= \frac{\partial}{\partial o_{x,y}^l}\left( \sum_{x^{\prime \prime}} \sum_{y^{\prime \prime}} w_{x^{\prime \prime},y^{\prime \prime}}^{l+1}f\left(o_{x^{\prime} - x^{\prime \prime},y^{\prime} - y^{\prime \prime}}^{l}\right) + b^{l+1} \right) \tag {15}
\end{align}
$$
继续展开上式，有：
$$
\begin{align}
\frac{\partial o_{x^{\prime},y^{\prime}}^{l+1}}{\partial o_{x,y}^l} &= \frac{\partial}{\partial o_{x,y}^l}\left( w_{0,0}^{l+1} f\left(o_{ x^{\prime} - 0, y^{\prime} - 0}^{l}\right) + \dots + w_{x^{\prime} - x,y^{\prime} - y}^{l+1} f\left(o_{x,y}^{l}\right) + \dots + b^{l+1}\right) \\
&= \frac{\partial}{\partial o_{x,y}^l}\left( w_{x^{\prime} - x,y^{\prime} - y}^{l+1} f\left(o_{x,y}^{l} \right) \right) \\
&= w_{x^{\prime} - x,y^{\prime} - y}^{l+1} \frac{\partial}{\partial o_{x,y}^l} \left( f\left(o_{x,y}^{l} \right) \right) \\
&= w_{x^{\prime} - x,y^{\prime} - y}^{l+1} f'\left(o_{x,y}^{l}\right) \tag {16}
\end{align}
$$
代入等式(14),得到：
$$
\begin{align}
\frac{\partial E}{\partial o_{x,y}^{l}} &= \sum_{x^{\prime}} \sum_{y^{\prime}} \delta^{l+1}_{x^{\prime},y^{\prime}} w_{x^{\prime} - x,y^{\prime} - y}^{l+1} f'\left(o_{x,y}^{l}\right) \tag{17} \\
\end{align}
$$
等式(17)中是实现了一个互相关函数，可以通过翻转卷积核来得到一个卷积运算的形式：
$$
\begin{align}
\frac{\partial E}{\partial o_{x,y}^{l}} &= \sum_{x^{\prime}} \sum_{y^{\prime}} \delta^{l+1}_{x^{\prime},y^{\prime}} w_{x^{\prime} - x,y^{\prime} - y}^{l+1} f'\left(o_{x,y}^{l}\right) \\
&= \delta^{l+1}_{x,y} \ast w_{-x,-y}^{l+1} f'\left(o_{x,y}^{l}\right) \\
&= \delta^{l+1}_{x,y} \ast \text{rot}_{180^\circ} \left( w_{x, y}^{l+1} \right) f'\left(o_{x,y}^{l}\right) \tag{18}
\end{align}
$$

#### Pooling层

Pooling层的作用主要将特征图的尺寸减小，从而减小参数的数量以及减少网络的计算量，并且可以避免过拟合。在这一层并没有进行权值的训练和学习。

比较常用的`Pooling`函数有`max-pooling`，`average pooling`甚至`L2-norm pooling`。在这一层，前向传播会导致一个$N\times N$大小的在执行`pooling`操作的`block`变成一个单独的值，一个来自`winning unit`的数值。

因此，反向传播会计算由这个`winning unit`得到的误差，其做法如下所示：

* **Max-pooling**:误差只会分配到`winning unit`，因为其他单元并没有任何共享，因此会赋值为0；
* **Average pooling**:误差会乘以$\frac{1}{N\times N}$，然后所有之前进行这个`pooling`操作的`block`中的所有单元都分配到这个数值。

#### 小结

**CNN**相比普通的**MLP**，多了一层卷积层和`pooling`层，不过只有卷积层才有对权值进行训练，而`pooling`层主要是为了减少特征图尺寸，减少参数和计算量而设计的。而多了卷积层，就相当于在神经网络中，正常是直接使用权值乘以输入值，即如$w*input+b$，现在这里就是权值与输入值进行卷积运算，然后再作为激活函数的输入值。反向传播还是需要计算偏导数，还是使用链式法则。