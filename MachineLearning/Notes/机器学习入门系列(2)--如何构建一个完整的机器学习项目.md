
本文会介绍下通常一个机器学习项目的整个过程，并结合一个小项目来辅助说明每个过程。

这里先给出一个完整的机器学习项目过程的主要步骤，如下所示：

1.	 项目概述。
2.	 获取数据。
3.	 发现并可视化数据，发现规律
4.	 为机器学习算法准备数据。
5.	 选择模型，进行训练。
6.	 微调模型。
7.	 给出解决方案。
8.	 部署、监控、维护系统

---

### 1. 项目概览

#### 1.1 划定问题

当我们开始一个机器学习项目的时候，需要先了解两个问题：

1. **商业目标是什么**？公司希望利用算法或者模型收获什么，这决定需要采用什么算法和评估的性能指标？
2. **当前的解决方案效果如何？**

通过上述两个问题，我们就可以开始设计系统，也就是解决方案。

但首先，有些问题也需要了解清楚：

- **监督还是无监督，或者是强化学习？**
- **是分类，回归，还是其他类型问题？**
- **采用批量学习还是需要线上学习。**



#### 1.2 选择性能指标

选择性能指标，通常对于模型，首先就是指模型的准确率，而在机器学习中，算法的准确率是需要通过减少损失来提高的，这就需要选择一个合适的损失函数来训练模型。

一般，从学习任务类型可以将损失函数划分为两大类--回归损失和分类损失，分别对应回归问题和分类问题。

##### 回归损失

###### 均方误差 / 平方误差 / L2 误差

均方误差(MSE)度量的是**预测值和真实值之间差的平方的均值**，它**只考虑误差的平均大小，不考虑其方向**。但经过平方后，对于偏离真实值较多的预测值会受到更严重的惩罚，并且 MSE 的数学特性很好，也就是特别易于求导，所以**计算梯度也会变得更加容易**。

MSE 的数学公式如下：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/mse.png)

代码实现如下：

```python
def rmse(predictions, targets):
    # 真实值和预测值的误差
    differences = predictions - targets
    differences_squared = differences ** 2
    mean_of_differences_squared = differences_squared.mean()
    # 取平方根
    rmse_val = np.sqrt(mean_of_differences_squared)
    return rmse_val
```

当然上述代码实现的是**均方根误差（RMSE)**，一个简单的测试例子如下：

```python
y_hat = np.array([0.000, 0.166, 0.333])
y_true = np.array([0.000, 0.254, 0.998])

print("d is: " + str(["%.8f" % elem for elem in y_hat]))
print("p is: " + str(["%.8f" % elem for elem in y_true]))
rmse_val = rmse(y_hat, y_true)
print("rms error is: " + str(rmse_val))
```

输出结果为：

```shell
d is: ['0.00000000', '0.16600000', '0.33300000']
p is: ['0.00000000', '0.25400000', '0.99800000']
rms error is: 0.387284994115
```

###### 平方绝对误差 / L1 误差

平均绝对误差（MAE）度量的是预测值和实际观测值之间绝对差之和的平均值。和 MSE 一样，这种度量方法也是在不考虑方向的情况下衡量误差大小。但和 MSE 的不同之处在于，MAE 需要像**线性规划这样更复杂的工具来计算梯度**。此外，MAE **对异常值更加稳健，因为它不使用平方。**

数学公式如下：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/mae.png)



MAE 的代码实现也不难，如下所示：

```python
def mae(predictions, targets):
    differences = predictions - targets
    absolute_differences = np.absolute(differences)
    mean_absolute_differences = absolute_differences.mean()
    return mean_absolute_differences
```

测试样例可以直接用刚才的 MSE 的测试代码，输出结果如下：

```
d is: ['0.00000000', '0.16600000', '0.33300000']
p is: ['0.00000000', '0.25400000', '0.99800000']
mae error is: 0.251
```

###### 平均偏差误差(mean bias error)

这个损失函数应用得比较少，在机器学习领域太不常见了，我也是第一次看到这个损失函数。它和 MAE 很相似，唯一区别就是**它没有用绝对值**。因此，需要注意的是，**正负误差可以互相抵消**。尽管在实际应用中没那么准确，但**它可以确定模型是存在正偏差还是负偏差。**

数学公式如下：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/mbe.png)

代码的实现，其实只需要在 MAE 的基础上删除加入绝对值的代码，如下所示：

```python
def mbe(predictions, targets):
    differences = predictions - targets
    mean_absolute_differences = differences.mean()
    return mean_absolute_differences
```

还是利用刚刚的测试样例，结果如下：

```
d is: ['0.00000000', '0.16600000', '0.33300000']
p is: ['0.00000000', '0.25400000', '0.99800000']
mbe error is: -0.251
```

可以看到我们给的简单测试样例，存在一个负偏差。

##### 分类误差

###### Hinge Loss / 多分类 SVM 误差

 hinge loss 常用于最大间隔分类（maximum-margin classification），**它是在一定的安全间隔内（通常是 1），正确类别的分数应高于所有错误类别的分数之和**。最常用的就是支持向量机（SVM）。**尽管不可微，但它是一个凸函数**，可以采用机器学习领域中常用的凸优化器。

其数学公式如下：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/svm_loss.png)

公式中 `sj` 表示的是预测值，而 `s_yi` 是真实值，也可以说是正确预测的数值，而 1 表示的就是间隔 `margin`，这里我们希望通过真实值和预测值之间的差来表示两种预测结果之间的相似关系，而 `margin` 是人为设置的一个安全系数，我们是**希望正确分类的得分要高于错误预测的得分，并且是高于一个 `margin` 的数值**，也就是`s_yi`越高越好，`s_j` 越低越好。这样计算得到的 Loss 会趋向于 0.

用一个简单的例子说明，假设现在有如下三个训练样本，我们需要预测三个类别，下面表格中的数值就是经过算法得到的每个类别的数值：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/svm_loss_example.png)

每一列就每张图片每个类别的数值，我们也可以知道每一列的真实值分别是狗、猫、马。简单的代码实现如下：

```python
def hinge_loss(predictions, label):
    '''
    hinge_loss = max(0, s_j - s_yi +1)
    :param predictions:
    :param label:
    :return:
    '''
    result = 0.0
    pred_value = predictions[label]
    for i, val in enumerate(predictions):
        if i == label:
            continue
        tmp = val - pred_value + 1
        result += max(0, tmp)
    return result
```

测试例子如下：

```python
image1 = np.array([-0.39, 1.49, 4.21])
image2 = np.array([-4.61, 3.28, 1.46])
image3 = np.array([1.03, -2.37, -2.27])
result1 = hinge_loss(image1, 0)
result2 = hinge_loss(image2, 1)
result3 = hinge_loss(image3, 2)
print('image1,hinge loss={}'.format(result1))
print('image2,hinge loss={}'.format(result2))
print('image3,hinge loss={}'.format(result3))

# 输出结果
# image1,hinge loss=8.48
# image2,hinge loss=0.0
# image3,hinge loss=5.199999999999999

```

这个计算过程更加形象的说明：

```python
## 1st training example
max(0, (1.49) - (-0.39) + 1) + max(0, (4.21) - (-0.39) + 1)
max(0, 2.88) + max(0, 5.6)
2.88 + 5.6
8.48 (High loss as very wrong prediction)
## 2nd training example
max(0, (-4.61) - (3.28)+ 1) + max(0, (1.46) - (3.28)+ 1)
max(0, -6.89) + max(0, -0.82)
0 + 0
0 (Zero loss as correct prediction)
## 3rd training example
max(0, (1.03) - (-2.27)+ 1) + max(0, (-2.37) - (-2.27)+ 1)
max(0, 4.3) + max(0, 0.9)
4.3 + 0.9
5.2 (High loss as very wrong prediction)
```



通过计算，hinge loss 数值越高，就代表预测越不准确。

###### 交叉熵损失 / 负对数似然

交叉熵损失（cross entroy loss）是分类算法最常用的损失函数。

数学公式：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/cross_entroy_loss.png)

根据公式，如果实际标签`y_i`是 1， 那么公式只有前半部分；如果是 0， 则只有后半部分。简单说，**交叉熵是将对真实类别预测的概率的对数相乘**，并且，**它会重重惩罚那些置信度很高但预测错误的数值。**

代码实现如下：

```python
def cross_entropy(predictions, targets, epsilon=1e-10):
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5))) / N
    return ce_loss
```

测试样例如下：

```python
predictions = np.array([[0.25, 0.25, 0.25, 0.25],
                            [0.01, 0.01, 0.01, 0.96]])
targets = np.array([[0, 0, 0, 1],
                    [0, 0, 0, 1]])
cross_entropy_loss = cross_entropy(predictions, targets)
print("Cross entropy loss is: " + str(cross_entropy_loss))

# 输出结果
# Cross entropy loss is: 0.713532969914
```

#### 1.3 核实假设

核实假设其实也可以说是确定你设计的系统的输入输出，我们的机器学习项目是需要商用的话，肯定就不只是一个算法模型，通常还会有前端展示页面效果，后端的服务等等，你需要和前后端的负责人进行沟通，核实接口的问题。

比如，《hands-on-ml-with-sklearn-and-tf》书中给出的例子是设计一个预测房价的系统，其输出是房价的数值，但是如果前端需要展示的是类别，即房价是便宜、中等还是昂贵，那么我们的系统输出的房价就没有意义了，这时候我们要解决的就是分类问题，而不是回归问题。

因此，当你在做一个机器学习项目的时候，你需要和有工作交接的同事保持良好的沟通，随时进行交流，确认接口的问题。

### 2. 获取数据

#### 2.1 常用数据集

在我们学习机器学习的时候，最好使用真实数据，而不是人工数据集。而且，幸运的是，现在有非常多的开源数据集，并且涵盖了多个领域，这里介绍几个常用的可以查找数据集的网站

1. [Kaggle 数据集](https://www.kaggle.com/datasets):每个数据集都是一个小型社区，用户可以在其中讨论数据、查找公共代码或在内核中创建自己的项目。包含各式各样的真实数据集。
2. [Amazon 数据集](https://registry.opendata.aws/)：该数据源包含多个不同领域的数据集，如：公共交通、生态资源、卫星图像等。网页中也有一个搜索框来帮助用户寻找想要的数据集，还有所有数据集的描述和使用示例，这些数据集信息丰富且易于使用！
3. [UCI机器学习资源库](https://archive.ics.uci.edu/ml/datasets.html)：来自加州大学信息与计算机科学学院的大型资源库，包含100多个数据集。用户可以找到单变量和多变量时间序列数据集，分类、回归或推荐系统的数据集。
4. [谷歌数据集搜索引擎](https://toolbox.google.com/datasetsearch)：这是一个可以按名称搜索数据集的工具箱。
5. [微软数据集](https://msropendata.com/)：2018年7月，微软与外部研究社区共同宣布推出“Microsoft Research Open Data”。它在云中包含一个数据存储库，用于促进全球研究社区之间的协作。它提供了一系列用于已发表研究的、经过处理的数据集。
6. [Awesome Public Datasets Collection](https://github.com/awesomedata/awesome-public-datasets)：Github 上的一个按“主题”组织的数据集，比如生物学、经济学、教育学等。大多数数据集都是免费的，但是在使用任何数据集之前，用户需要检查一下许可要求。
7. [计算机视觉数据集](https://www.visualdata.io/)：Visual Data包含一些可以用来构建计算机视觉(CV)模型的大型数据集。用户可以通过特定的CV主题查找特定的数据集，如语义分割、图像标题、图像生成，甚至可以通过解决方案(自动驾驶汽车数据集)查找特定的数据集。

常用的部分图像数据集：

1. **[Mnist](http://yann.lecun.com/exdb/mnist/)**: 手写数字数据集，包含 60000 张训练集和 10000 张测试集。（但该数据集通常只是作为简单 demo 使用，如果要验证算法模型的性能，最好在更大数据集上进行测试，实验结果才有足够的可信度）
2. **[Cifar](https://www.cs.toronto.edu/~kriz/cifar.html)**：分为 Cifar10 和 Cifar100。前者包含 60000 张图片，总共10个类别，每类 6000 张图片。后者是 100 个类别，每个类别 600 张图片。类别包括猫狗鸟等动物、飞机汽车船等交通工具。
3. **[Imagenet](http://www.image-net.org/about-overview)**：应该是目前最大的开源图像数据集，包含 1500 万张图片，2.2 万个类别。
4. **[LFW](http://vis-www.cs.umass.edu/lfw/)**：人脸数据集，包含13000+张图片和1680个不同的人。
5. **[CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)**：人脸数据集，包含大约 20w 张图片，总共 10177个不同的人，以及每张图片都有 5 个位置标注点，40 个属性信息


#### 2.2 准备开发环境

在找到数据集，并下载后，我们就需要开始为算法做好准备，这里首先需要确定我们需要采用的编程语言和相应的框架。

现在机器学习，一般都是采用 Python 语言，因为它简单易学，对程序员非常友好，而且也有相应很多应用于机器学习和深度学习方面的框架，比如 `scikit-learn`，`opencv`，深度学习方面的`TensorFlow, Pytorch, Keras`等。

而为了方便可视化数据，查看代码运行效果，通常会选择采用 `Jupyter` 这个模块。其他必要的 Python 模块有：

- Numpy：一个运行速度非常快的数学库，主要用于数组计算，支持大量的维度数据和矩阵运算
- Pandas：快速处理数据和分析数据
- Matplotlib: 绘图，可视化数据

此外，python 比较不友好的问题就是 2.7 版本和 3.+ 版本的不兼容问题，所以我们需要有一个包管理工具，可以单独配置不同的开发环境，这里推荐使用的是 `Anaconda`。

这些模块的安装，网上都有非常多详细的教程，这里就不花时间介绍了。

#### 2.3 创建测试集

在下载数据后，首先要做的是创建测试集，这是在分析数据前先排除测试集的数据，不会引入测试数据的规律，从而影响算法的选择，保证采用测试集进行测试的结果是客观可信的，而不会出现**数据透视偏差**的问题。

**数据透视偏差**：即由于选择模型时候参考了测试集的规律，导致在测试集上准确率很好，但实际应用的时候，系统表现很糟糕的情况。

一般我们会按照 8:2 的比例划分训练集和测试集，可以采用如下代码，随机划分出测试集：

```python
import numpy as np

def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = split_train_test(housing, 0.2)
```

当然，这个方法存在一个缺陷：**每次运行会得到不同的测试集！**

解决的办法有以下几种：

1. 第一次运行该函数后就保存测试集，随后载入测试集；
2. 调用函数`np.random.permutation() `前，设置随机数生成器的种子，比如`np.random.seed(42)`，以产生相同的洗牌指数(shuffled indices).
3. 上述两个方法对于数据集不变的情况是有效的，但更新数据集后，都会失效。第三个解决方法就是根据每个实例的 `ID`来判断其是否应该放入测试集，比如，对于图片数据集，就可以根据图片的名字（保证更新训练集不会更新图片名字）来确定其属于训练集还是测试集。

划分数据集也可以采用`Scikit-Learn`库的一些函数，最简单也是最常用的就是 `train_test_split`函数，它和上述`split_train_test`函数作用相似，但增加了更多的功能：

- `random_state`参数可以实现设置随机生成器种子的作用；
- 可以将种子传递给多个行数相同的数据集，可以在相同的索引上分割数据集。

简单使用例子如下：

```python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```

这里需要注意的是，我们采用的都是随机采样方法，对于大数据集，这方法通常可行。

但对于不大的数据集，这会出现**采样偏差**的风险。简单说，就是样本代表性不够，可能随机选择的都是同种类型的数据。

比如，当一个调查公司想要对 1000 个人进行调查，需要保证这 1000 个人对人群整体有代表性，例如，美国人口有 51.3% 是女性，48.7% 是男性。那么，在美国做这个调查，就需要保证样本也是这个比例，即选择 513 名女性，487 名男性。

这种采样称为**分层采样**：将人群分层均匀的子分组，称为分层，从每个分层去取合适数量的实例，以保证测试集对总人数有代表性。

所以上述调查公司的例子，就是先将人群按照性别划分两个子分组，然后分别再按照如年龄、职业等标准继续划分子分组。

分层采样的操作可以使用`Scikit-Learn`的`StratifiedShuffleSplit`函数，指定数据中指定的类别，代码例子如下：

```python
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
```

这里是给定一个房子信息数据`housing`，然后指定收入分类`housing["income_cat"]`，保证采样的数据集中包含根据收入分类的比例。

### 3. 特征工程

获取数据并分好测试集后，并不能直接就选择算法，训练模型，我们需要为算法准备数据，或者说就是做特征工程。

何为特征工程呢？顾名思义，就是**对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。**

本质上讲，**特征工程是一个表示和展现数据的过程**；实际工作中，特征工程的目的是**去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。**

特征工程的重要性有以下几点：

1. **特征越好，灵活性越强**。好的特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易和维护。
2. **特征越好，构建的模型越简单**。好的特征可以在参数不是最优的情况，依然得到很好的性能，减少调参的工作量和时间，也就可以大大降低模型复杂度。
3. **特征越好，模型的性能越出色**。特征工程的目的本来就是为了提升模型的性能。

特征工程，我觉得就是包括数据预处理、特征提取、特征选择、特征构建几个步骤。接下来会依次介绍这几步内容。

#### 3.1 数据预处理

首先需要对数据进行预处理，一般常用的两种数据类型：

1. **结构化数据**。结构化数据可以看作是关系型数据库的一张表，每列都有清晰的定义，包含了数值型和类别型两种基本类型；每一行数据表示一个样本的信息。
2. **非结构化数据**。主要是文本、图像、音频和视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每个数据的大小互不相同。

这里主要介绍结构化数据和图像数据两种数据的数据预处理方法。

##### 3.1.1 处理缺失值

数据的缺失主要包括**记录的缺失和记录中某个字段信息的缺失**，两者都会造成分析结果的不准确。

###### **缺失值产生的原因**

- 信息暂时无法获取，或者获取信息的代价太大。
- 信息被遗漏，人为的输入遗漏或者数据采集设备的遗漏。
- 属性不存在，在某些情况下，缺失值并不意味着数据有错误，对一些对象来说某些属性值是不存在的，如未婚者的配偶姓名、儿童的固定收入等。

###### **缺失值的影响**

- 数据挖掘建模将丢失大量的有用信息。
- 数据挖掘模型所表现出的不确定性更加显著，模型中蕴含的规律更难把握。
- 包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。

###### **缺失值的处理方法**

1. **直接使用含有缺失值的特征**：当仅有少量样本缺失该特征的时候可以尝试使用；
2. **删除含有缺失值的特征**：这个方法一般适用于大多数样本都缺少该特征，且仅包含少量有效值是有效的；
3. **插值补全缺失值**

最常使用的还是第三种插值补全缺失值的做法，这种做法又可以有多种补全方法。

1.  **均值/中位数/众数补全**

如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；

如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。

2.  **同类均值/中位数/众数补全**

对样本进行分类后，根据同类其他样本该属性的均值补全缺失值，当然同第一种方法类似，如果均值不可行，可以尝试众数或者中位数等统计数据来补全。

3.  **固定值补全**

利用固定的数值补全缺失的属性值。

4.  **建模预测**

利用机器学习方法，将缺失属性作为预测目标进行预测，具体为将样本根据是否缺少该属性分为训练集和测试集，然后采用如回归、决策树等机器学习算法训练模型，再利用训练得到的模型预测测试集中样本的该属性的数值。

这个方法**根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的**；一般的情况是介于两者之间。

5.  **高维映射**

将属性映射到高维空间，采用**独热码编码（one-hot）技术**。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。

这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是**完整保留了原始数据的全部信息、不用考虑缺失值**；缺点是**计算量大大提升，且只有在样本量非常大的时候效果才好**。

6.  **多重插补**

多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。

7.  **压缩感知和矩阵补全**

压缩感知通过**利用信号本身所具有的稀疏性，从部分观测样本中回复原信号**。压缩感知分为感知测量和重构恢复两个阶段。

- 感知测量：此阶段对原始信号进行处理以获得稀疏样本表示。常用的手段是傅里叶变换、小波变换、字典学习、稀疏编码等

- 重构恢复：此阶段基于稀疏性从少量观测中恢复原信号。这是**压缩感知的核心**

矩阵补全可以查看知乎上的问题：[矩阵补全（matrix completion）的经典算法有哪些？目前比较流行的算法是什么？](https://www.zhihu.com/question/47716840)

8. **手动补全**

除了手动补全方法，其他插值补全方法只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。但这种方法需要对问题领域有很高的认识和理解，要求比较高，如果缺失数据较多，会比较费时费力。

9. **最近邻补全**

寻找与该样本最接近的样本，使用其该属性数值来补全。

##### 3.1.2 图片数据扩充

对于图片数据，最常遇到的问题就是训练数据不足的问题。

一个模型所能获取的信息一般来源于两个方面，一个是训练数据包含的信息；另一个就是模型的形成过程中（包括构造、学习、推理等），人们提供的先验信息。

而如果训练数据不足，那么模型可以获取的信息就比较少，需要提供更多的先验信息保证模型的效果。先验信息一般作用来两个方面，一是**模型**，如采用特定的内在结构（比如深度学习的不同网络结构）、条件假设或添加其他约束条件（深度学习中体现在损失函数加入不同正则项）；第二就是**数据**，即根据先验知识来调整、变换或者拓展训练数据，让其展现出更多的、更有用的信息。

对于图像数据，如果训练数据不足，导致的后果就是**模型过拟合问题**，即模型在训练样本上的效果不错，但在测试集上的泛化效果很糟糕。过拟合的解决方法可以分为两类：

1. **基于模型的方法**：主要是采用降低过拟合风险的措施，如简化模型（从卷积神经网络变成逻辑回归算法）、添加约束项以缩小假设空间（如 L1、L2等正则化方法）、集成学习、Dropout方法（深度学习常用方法）等；
2. **基于数据的方法**：主要就是**数据扩充**(Data Augmentation)，即根据一些先验知识，在保持特点信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果。具体做法有多种，在保持图像类别不变的前提下，可以对每张图片做如下变换处理。
   - 一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等，这些变换对应着同一个目标在不同角度的观察结果；
   - 对图像中的元素添加噪声扰动，如椒盐噪声、高斯白噪声等；
   - 颜色变换。比如在图像的 RGB 颜色空间进行主成分分析，得到 3 个主成分的特征向量`p1,p2,p3`以及对应的特征值`λ1,λ2,λ3`，然后在每个像素的 RGB 值上添加增量`[p1,p2,p3]*[a1λ1,a2λ2,a3λ3]`，其中`a1,a2,a3`都是均值为 0， 方差较小的高斯分布随机数；
   - 改变图像的亮度、清晰度、对比度、锐度等。

上述数据扩充方法是在图像空间进行变换的，也可以选择先对图像进行特征提取，然后在图像的特征空间进行变换，利用一些通用的数据扩充或者上采样方法，例如 SMOTE(Synthetic Minority Over-sampling Technique)。

此外，最近几年一直比较热门的 GAN，生成对抗网络，它的其中一个应用就是生成图片数据，也可以应用于数据扩充。

最后，还有一种方法可以不需要扩充数据，利用迁移学习的做法，也是如今非常常用的一个方法，微调（Finetuning），即借用在大数据集（如 ImageNet）上预训练好的模型，然后在自己的小数据集上进行微调，这是一种简单的迁移学习，同时也可以快速训练一个效果不错的针对目标类别的新模型。

##### 3.1.3 处理异常值

异常值分析是**检验数据是否有录入错误以及含有不合常理的数据**。忽视异常值的存在是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响。

**异常值是指样本中的个别值，其数值明显偏离其余的观测值**。异常值也称为离群点，异常值分析也称为离群点分析。

###### **异常值检测**

1. **简单统计**：比如利用`pandas`库的`describe()`方法观察数据的统计性描述，或者简单使用散点图也能观察到异常值的存在，如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/anomaly%20detection_scatter_plot.png)

2. **3∂原则**: 这个原则有个条件：**数据需要服从正态分布**。在 3∂ 原则下，异常值如超过 3 倍标准差，那么可以将其视为异常值。正负3∂ 的概率是 99.7%，那么距离平均值 3∂ 之外的值出现的概率为`P(|x-u| > 3∂) <= 0.003`，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/anomaly%20detection_3.png)

3. **箱型图**

这种方法是利用箱型图的**四分位距（IQR）对异常值进行检测，也叫Tukey‘s test**。箱型图的定义如下：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/tukey_test.png)



四分位距(IQR)就是上四分位与下四分位的差值。而我们通过IQR的1.5倍为标准，规定：超过**上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离**的点为异常值。下面是Python中的代码实现，主要使用了`numpy`的`percentile`方法。

```python
Percentile = np.percentile(df['length'],[0,25,50,75,100])
IQR = Percentile[3] - Percentile[1]
UpLimit = Percentile[3]+ageIQR*1.5
DownLimit = Percentile[1]-ageIQR*1.5
```

也可以使用`seaborn`的可视化方法`boxplot`来实现：

```python
f,ax=plt.subplots(figsize=(10,8))
sns.boxplot(y='length',data=df,ax=ax)
plt.show()
```

如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/anomaly%20detection_tukey.png)

上面三种方法是比较简单的异常值检测方法，接下来是一些较复杂的异常值检测方法，因此这里简单介绍下这些方法的基本概念。

4. **基于模型预测**

顾名思义，该方法会构建一个**概率分布模型**，并计算对象符合该模型的概率，将低概率的对象视为异常点。

如果模型是簇的组合，则异常点是不在任何簇的对象；如果模型是回归，异常点是远离预测值的对象(就是第一个方法的图示例子)。

**优缺点**：

- 有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；
- 对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。

5. **基于近邻度的离群点检测**

 **一个对象的离群点得分由到它的 k-最近邻（KNN）的距离给定**。

>  这里需要注意 **k** 值的取值会影响离群点得分，如果 k 太小，则少量的邻近离群点可能会导致较低的离群点得分；如果 k 太大，则点数少于 k 的簇中所有的对象可能都成了离群点。为了增强鲁棒性，可以采用 k 个最近邻的平均距离。

**优缺点**：

- 简单;
- 基于邻近度的方法需要 `O(m2)` 时间，大数据集不适用；
- k 值的取值导致该方法对参数的选择也是敏感的；
- 不能处理具有**不同密度区域**的数据集，因为它使用全局阈值，不能考虑这种密度的变化。

6. **基于密度的离群点检测**

**一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。**

**另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。**

**优缺点：**

- 给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；
- 与基于距离的方法一样，这些方法必然具有 `O(m2)` 的时间复杂度。对于低维数据使用特定的数据结构可以达到 `O(mlogm)` ；
- 参数选择是困难的。虽然 `LOF` 算法通过观察不同的 k 值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。

7. **基于聚类的离群点检测**

**一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。**

离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：**结构是否有效**。这也是 `k-means` 算法的缺点，**对离群点敏感**。

为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。

**优缺点：**

- 基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；

- 簇的定义通常是离群点的补集，因此可能同时发现簇和离群点；

- 产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；

- 聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。

8. **专门的离群点检测**

除了以上提及的方法，还有两个专门用于检测异常点的方法比较常用：`One Class SVM` 和 `Isolation Forest`



###### **异常值处理**

- **删除含有异常值的记录**：直接将含有异常值的记录删除；
- **视为缺失值**：将异常值视为缺失值，利用缺失值处理的方法进行处理；
- **平均值修正**：可用前后两个观测值的平均值修正该异常值；
- **不处理**：直接在具有异常值的数据集上进行数据挖掘；

将含有异常值的记录直接删除的方法简单易行，但缺点也很明显，在观测值很少的情况下，**这种删除会造成样本量不足，可能会改变变量的原有分布，从而造成分析结果的不准确**。视为缺失值处理的好处是**可以利用现有变量的信息，对异常值（缺失值）进行填补。**

在很多情况下，要先分析异常值出现的可能原因，在判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。



##### 3.1.4 处理类别不平衡问题

什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。

比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。

那么如何解决类别不平衡问题呢？

这里介绍八大解决办法。

1. **扩充数据集**

首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。

如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。

2. **尝试其他评价指标**

一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。

其他可以推荐的评价指标有以下几种

- 混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量；
- 精确度(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，P=TP / (TP+FP)
- 召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，R = TP / (TP+FN)
- F1 得分(F1 Score)：精确度和召回率的加权平均，F1=2PR / (P+R)
- Kappa (Cohen kappa)
- ROC 曲线(ROC Curves):常被用于评价一个二值分类器的优劣，而且对于正负样本分布变化的时候，ROC 曲线可以保持不变，即不受类别不平衡的影响。

其中 TP、FP、TN、FN 分别表示正确预测的正类、错误预测的正类、预测正确的负类以及错误预测的负类。图例如下：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/fpr-and-tpr.png)

3. **对数据集进行重采样**

可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。

- 对小类的数据样本进行采样来增加小类的数据样本个数，即**过采样**（over-sampling ，采样的个数大于该类样本的个数）。

- 对大类的数据样本进行采样来减少该类数据样本的个数，即**欠采样**（under-sampling，采样的次数少于该类样本的个素）。

采样算法往往很容易实现，并且其运行速度快，并且效果也不错。 一些经验法则：

- 考虑对大类下的样本（超过 1 万、十万甚至更多）进行欠采样，即删除部分样本；
- 考虑对小类下的样本（不足 1万甚至更少）进行过采样，即添加部分样本的副本；
- 考虑尝试**随机采样与非随机采样**两种采样方法；
- 考虑对各类别尝试不同的采样比例，比一定是1:1，有时候1:1反而不好，因为与现实情况相差甚远；
- 考虑同时使用过采样与欠采样。

4. **尝试人工生成数据样本**

一种简单的人工样本数据产生的方法便是，**对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样**。

你可以使用**基于经验对属性值进行随机采样**而构造新的人工样本，或者使用类似**朴素贝叶斯方法**假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。 

有一个系统的构造人工数据样本的方法 **SMOTE**(Synthetic Minority Over-sampling Technique)。SMOTE 是一种**过采样算法**，它**构造新的小类样本**而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。

它基于**距离度量**选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本，然后对选择的那个样本的**一个属性增加噪声**，每次处理一个属性。这样就构造了更多的新生数据。

python 实现的 SMOTE 算法代码地址如下，它提供了多种不同实现版本，以及多个重采样算法。

https://github.com/scikit-learn-contrib/imbalanced-learn

5. **尝试不同分类算法**

强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。

**决策树往往在类别不均衡数据上表现不错**。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。

6. **尝试对模型进行惩罚**

你可以使用相同的分类算法，但使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以**对分类器的小类样本数据增加权值，降低大类样本的权值**（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。

一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 penalized-SVM 和 penalized-LDA 算法。 

如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用**惩罚模型来解决不平衡性**问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。

7. **尝试一个新的角度理解问题**

从一个新的角度来理解问题，比如我们可以将小类的样本作为异常点，那么问题就变成异常点检测与变化趋势检测问题。

- 异常点检测：即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。 
- 变化趋势检测：类似于异常点检测，不同在于其通过**检测不寻常的变化趋势**来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 

将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。

8. **尝试创新**

仔细对问题进行分析和挖掘，是否可以将问题划分为多个更小的问题，可以尝试如下方法：

- 将你的大类压缩成小类；
- 使用 One Class 分类器（将小类作为异常点）；
- 使用集成方式，训练多个分类器，然后联合这些分类器进行分类；

对于类别不平衡问题，还是需要具体问题具体分析，如果有先验知识可以快速挑选合适的方法来解决，否则最好就是逐一测试每一种方法，然后挑选最好的算法。最重要的还是多做项目，多积累经验，这样遇到一个新的问题，也可以快速找到合适的解决方法。



#### 3.2 特征缩放

特征缩放主要分为两种方法，归一化和正则化。

##### 3.2.1 归一化

1. **归一化(Normalization)，也称为标准化**，这里不仅仅是对特征，实际上对于原始数据也可以进行归一化处理，它是将特征（或者数据）都**缩放到一个指定的大致相同的数值区间内**。
2. **归一化的两个原因**：

- 某些算法要求样本数据或特征的数值**具有零均值和单位方差**；
- 为了消除样本数据或者特征之间的**量纲影响，即消除数量级的影响**。如下图所示是包含两个属性的目标函数的等高线
  - **数量级的差异将导致量级较大的属性占据主导地位**。从下图左看到量级较大的属性会让椭圆的等高线压缩为直线，使得目标函数仅依赖于该属性。
  - **数量级的差异会导致迭代收敛速度减慢**。原始的特征进行梯度下降时，每一步梯度的方向会偏离最小值（等高线中心点）的方向，**迭代次数较多，且学习率必须非常小**，否则非常容易引起**宽幅震荡**。但经过标准化后，每一步梯度的方向都几乎指向最小值（等高线中心点）的方向，**迭代次数较少**。
  - **所有依赖于样本距离的算法对于数据的数量级都非常敏感**。比如 KNN 算法需要计算距离当前样本最近的 k 个样本，当属性的量级不同，选择的最近的 k 个样本也会不同。

![图来自《百面机器学习》](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/normalization_example.png)

3. 常用的两种归一化方法：

- **线性函数归一化(Min-Max Scaling)**。它对原始数据进行线性变换，使得结果映射到`[0,1]`的范围，实现对原始数据的等比缩放，公式如下：

$$
X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
$$

其中 X 是原始数据，$X_{max}, X_{min}$分别表示数据最大值和最小值。

- **零均值归一化(Z-Score Normalization)**。它会将原始数据映射到均值为 0，标准差为 1 的分布上。假设原始特征的均值是$\mu$、方差是$\sigma$，则公式如下：

$$
z = \frac{x-\mu}{\sigma}
$$

4. 如果数据集分为训练集、验证集、测试集，那么**三个数据集都采用相同的归一化参数，数值都是通过训练集计算得到**，即上述两种方法中分别需要的数据最大值、最小值，方差和均值都是通过训练集计算得到（这个做法类似于深度学习中批归一化，BN的实现做法）。
5. 归一化不是万能的，实际应用中，**通过梯度下降法求解的模型是需要归一化的，这包括线性回归、逻辑回归、支持向量机、神经网络等模型**。但**决策树模型不需要**，以 C4.5 算法为例，决策树在分裂结点时候主要依据数据集 D 关于特征 x 的信息增益比，而信息增益比和特征是否经过归一化是无关的，归一化不会改变样本在特征 x 上的信息增益。

##### 3.2.2 正则化

1. 正则化是**将样本或者特征的某个范数（如 L1、L2 范数）缩放到单位 1**。

假设数据集为：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171702256.png)

对样本首先计算 Lp 范数，得到：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171715565.png)

正则化后的结果是：每个属性值除以其 Lp 范数

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171731785.png)

2. 正则化的过程是针对**单个样本**的，对每个样本将它缩放到单位范数。

   归一化是针对**单个属性**的，需要用到所有样本在该属性上的值。

3. 通常如果使用**二次型（如点积）或者其他核方法计算两个样本之间的相似性**时，该方法会很有用。

#### 3.3 特征编码

##### 3.3.1 序号编码(Ordinal Encoding)

**定义**：序号编码一般用于**处理类别间具有大小关系**的数据。

比如成绩，可以分为高、中、低三个档次，并且存在“高>中>低”的大小关系，那么序号编码可以对这三个档次进行如下编码：高表示为 3，中表示为 2，低表示为 1，这样转换后依然保留了大小关系。

##### 3.3.2 独热编码(One-hot Encoding)

**定义**：独热编码通常用于处理类别间不具有大小关系的特征。

独热编码是采用 **N** 位状态位来对 **N** 个可能的取值进行编码。比如血型，一共有 4 个取值（A、B、AB 以及 O 型），那么独热编码会将血型转换为一个 4 维稀疏向量，分别表示上述四种血型为：

- A型：(1,0,0,0)
- B型：(0,1,0,0)
- AB型：(0,0,1,0)
- O型：(0,0,0,1)

独热编码的优点有以下几个：

- 能够处理**非数值属性**。比如血型、性别等
- 一定程度上扩充了特征。
- 编码后的向量是稀疏向量，只有一位是 1，其他都是 0，可以利用向量的稀疏来**节省存储空间**。
- **能够处理缺失值**。当所有位都是 0，表示发生了缺失。此时可以采用处理缺失值提到的**高维映射**方法，用第 **N+1** 位来表示缺失值。

当然，独热编码也存在一些缺点：

1.高维度特征会带来以下几个方面问题：

- KNN 算法中，**高维空间下两点之间的距离很难得到有效的衡量**；
- 逻辑回归模型中，参数的数量会随着维度的增高而增加，导致**模型复杂，出现过拟合问题**；
- 通常只有部分维度是对分类、预测有帮助，需要**借助特征选择来降低维度**。

2.决策树模型不推荐对离散特征进行独热编码，有以下两个主要原因：

- **产生样本切分不平衡问题，此时切分增益会非常小**。

  比如对血型做独热编码操作，那么对每个特征`是否 A 型、是否 B 型、是否 AB 型、是否 O 型`，会有少量样本是 1 ，大量样本是 0。

  这种划分的增益非常小，因为拆分之后：

	- 较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略。

	- 较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。

- **影响决策树的学习**。

    决策树依赖的是**数据的统计信息**。而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上，统计信息是不准确的，学习效果变差。

    本质是因为**独热编码之后的特征的表达能力较差**。该特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败。最终该特征得到的重要性会比实际值低。

##### 3.3.3 二进制编码(Binary Encoding)

二进制编码主要分为两步：

1. 先采用序号编码给每个类别赋予一个类别 ID；
2. 接着将类别 ID 对应的二进制编码作为结果。

继续以血型为例子，如下表所示：

| 血型 | 类别 ID | 二进制表示 | 独热编码 |
| :--: | :-----: | :--------: | :------: |
|  A   |    1    |   0 0 1    | 1 0 0 0  |
|  B   |    2    |   0 1 0    | 0 1 0 0  |
|  AB  |    3    |   0 1 1    | 0 0 1 0  |
|  O   |    4    |   1 0 0    | 0 0 0 1  |

从上表可以知道，二进制编码本质上是利用**二进制对类别 ID 进行哈希映射，最终得到 0/1 特征向量，并且特征维度小于独热编码，更加节省存储空间**。

##### 3.3.4 二元化

**定义**：特征二元化就是将数值型的属性转换为布尔型的属性。通常用于假设属性取值分布是伯努利分布的情形。

特征二元化的算法比较简单。对属性 `j` 指定一个阈值 `m`。

- 如果样本在属性 `j` 上的值大于等于 `m`, 则二元化后为 1；
- 如果样本在属性 `j` 上的值小于 `m`，则二元化为 0

根据上述定义，`m` 是一个关键的超参数，它的取值需要结合模型和具体的任务来选择。

##### 3.3.5 离散化

**定义**：顾名思义，离散化就是将连续的数值属性转换为离散的数值属性。

那么什么时候需要采用特征离散化呢？

这背后就是需要采用“**海量离散特征+简单模型**”，还是“**少量连续特征+复杂模型**”的做法了。

- 对于线性模型，通常使用“海量离散特征+简单模型”。
  - 优点：模型简单
  - 缺点：特征工程比较困难，但一旦有成功的经验就可以推广，并且可以很多人并行研究。
- 对于非线性模型（比如深度学习），通常使用“少量连续特征+复杂模型”。
  - 优点：不需要复杂的特征工程
  - 缺点：模型复杂

**分桶**

1.离散化的常用方法是**分桶**：

- 将所有样本在连续的数值属性 `j` 的取值从小到大排列 ${a_0, a_1, ..., a_N}$ 。
- 然后从小到大依次选择分桶边界$b_1, b_2, ..., b_M$  。其中：
  - `M` 为分桶的数量，它是一个超参数，需要人工指定。
  - 每个桶的大小$b_{k+1}-b_k$  也是一个超参数，需要人工指定。
- 给定属性 `j` 的取值$a_i$，对其进行分桶：
  - 如果$a_i < b_1$，则分桶编号是 0。分桶后的属性的取值为 0；
  - 如果$b_k \le a_i \le b_{k+1}$，则分桶编号是 `k`。分桶后的属性取值是 `k`；
  - 如果 $a_i \ge b_M$, 则分桶编号是 `M`。分桶后的属性取值是 `M`。

2.分桶的数量和边界通常需要人工指定。一般有两种方法：

- **根据业务领域的经验来指定**。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：
    - 收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。
    - 年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。
    - 年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。
    - 年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。
    - 年收入超过 260 万元，则为分桶 4 。
- **根据模型指定**。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。

3.选择分桶大小时，有一些经验指导：

- **分桶大小必须足够小**，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。

  即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。

- **分桶大小必须足够大，使每个桶内都有足够的样本**。

  如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。

- 每个桶内的样本尽量**分布均匀**。

**特性**

1.在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是**将连续特征离散化为一系列 0/1 的离散特征**。

其优势有：

- 离散化之后得到的稀疏向量，**内积乘法运算速度更快，计算结果方便存储**。

- 离散化之后的特征对于**异常数据具有很强的鲁棒性**。

  如：销售额作为特征，当销售额在 `[30,100)` 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。

- 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于**引入了非线性，提升模型的表达能力，增强拟合能力**。

  假设某个连续特征 `j`  ，它离散化为 `M` 个 0/1 特征 $j_1, j_2, ..., j_M$  。则：$w_j * x_j -> w_{j1} * x_{j1}^` + w_{j2} * x_{j2}^` + ...+w_{jM} * x_{jM}^` $。其中 $x_{j1}^`，x_{j2}^`，..., x_{jM}^`$ 是离散化之后的新的特征，它们的取值空间都是  {0, 1}。

  上式右侧是一个分段线性映射，其表达能力更强。

- **离散化之后可以进行特征交叉**。假设有连续特征`j` ，离散化为 `N` 个 0/1 特征；连续特征 `k`，离散化为 `M` 个 0/1 特征，则分别进行离散化之后引入了 `N+M` 个特征。

  假设离散化时，并不是独立进行离散化，而是特征 `j,k`  联合进行离散化，则可以得到  `N*M` 个组合特征。**这会进一步引入非线性，提高模型表达能力**。

- **离散化之后，模型会更稳定**。

  如对销售额进行离散化，`[30,100)` 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。

  但是**处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理**。

2.**特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险**。

能够对抗过拟合的原因：**经过特征离散化之后，模型不再拟合特征的具体值，而是拟合特征的某个概念**。因此能够**对抗数据的扰动，更具有鲁棒性**。

另外它使得模型要**拟合的值大幅度降低，也降低了模型的复杂度**。

#### 3.4 特征选择

**定义**：从给定的特征集合中选出相关特征子集的过程称为特征选择(feature selection)。

1.对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说很关键，但有些属性意义就不大。

- 对当前学习任务有用的属性或者特征，称为**相关特征**(relevant feature)；
- 对当前学习任务没用的属性或者特征，称为**无关特征**(irrelevant feature)。

2.特征选择可能会降低模型的预测能力，因为被剔除的特征中可能包含了有效的信息，抛弃这部分信息一定程度上会降低模型的性能。但这也是计算复杂度和模型性能之间的取舍：

- 如果保留尽可能多的特征，模型的性能会提升，但同时模型就变复杂，计算复杂度也同样提升；
- 如果剔除尽可能多的特征，模型的性能会有所下降，但模型就变简单，也就降低计算复杂度。

3.常见的特征选择分为三类方法：

- 过滤式(filter)
- 包裹式(wrapper)
- 嵌入式(embedding)

##### 3.4.1 特征选择原理

1.采用特征选择的原因：

- **维数灾难问题**。因为属性或者特征过多造成的问题，如果可以选择重要的特征，使得仅需要一部分特征就可以构建模型，可以大大减轻维数灾难问题，从这个意义上讲，特征选择和降维技术有相似的动机，事实上它们也是处理高维数据的两大主流技术。
- **去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度**。

2.特征选择最重要的是**确保不丢失重要的特征**，否则就会因为缺少重要的信息而无法得到一个性能很好的模型。

- 给定数据集，学习任务不同，相关的特征很可能也不相同，因此特征选择中的**不相关特征指的是与当前学习任务无关的特征**。
- 有一类特征称作**冗余特征(redundant feature)**，它们所包含的信息可以从其他特征中推演出来。
  - 冗余特征通常都不起作用，去除它们可以减轻模型训练的负担；
  - 但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则它是有益的，可以降低学习任务的难度。

3.在没有任何先验知识，即领域知识的前提下，要想从初始特征集合中选择一个包含所有重要信息的特征子集，唯一做法就是遍历所有可能的特征组合。

但这种做法并不实际，也不可行，因为会遭遇组合爆炸，特征数量稍多就无法进行。

一个可选的方案是：

- 产生一个候选子集，评价出它的好坏。
- 基于评价结果产生下一个候选子集，再评价其好坏。
- 这个过程持续进行下去，直至无法找到更好的后续子集为止。

这里有两个问题：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

###### 3.4.1.1 子集搜索

1.**子集搜索**方法步骤如下：

- 给定特征集合$A={A_1,A_2,...,A_d}$  ，首先将每个特征看作一个候选子集（即每个子集中只有一个元素），然后对这 $d$ 个候选子集进行评价。

  假设 $A_2$ 最优，于是将  $A_2$  作为第一轮的选定子集。

- 然后在上一轮的选定子集中加入一个特征，构成了包含两个特征的候选子集。

  假定 $A_2$,A_5   最优，且优于  $A_2$  ，于是将  $A_2,A_5$  作为第二轮的选定子集。

- ....

- 假定在第 `k+1` 轮时，**本轮的最优的特征子集不如上一轮的最优的特征子集**，则停止生成候选子集，并将上一轮选定的特征子集作为特征选择的结果。

2.这种逐渐增加相关特征的策略称作**前向 `forward`搜索**

类似地，如果从完整的特征集合开始，每次尝试去掉一个无关特征，这种逐渐减小特征的策略称作**后向`backward`搜索**

3.也可以将前向和后向搜索结合起来，每一轮逐渐增加选定的相关特征（这些特征在后续迭代中确定不会被去除），同时减少无关特征，这样的策略被称作是**双向`bidirectional`搜索**。

4该策略是贪心的，因为它们仅仅考虑了使本轮选定集最优。但是除非进行穷举搜索，否则这样的问题无法避免。

###### 3.4.1.2 子集评价

1.子集评价的做法如下：

给定数据集 D，假设所有属性均为离散型。对属性子集 A，假定根据其取值将 D 分成了 V 个子集：${D_1, D_2, \cdots,  D_V}$

可以计算属性子集 A 的信息增益：
$$
g(D, A) = H(D) - H(D|A)=H(D)-\sum^V_{v=1}\frac{|D_v|}{|D|}H(D_v)
$$
其中，$|•|$表示集合大小，$H(•)$表示熵。

**信息增益越大，表明特征子集 A 包含的有助于分类的信息越多**。所以对于每个候选特征子集，可以基于训练集 D 来计算其信息增益作为评价准则。

2.更一般地，特征子集 A 实际上确定了对数据集 D 的一个划分规则。

- 每个划分区域对应着 A 上的一个取值，而样本标记信息 y 则对应着 D 的真实划分。
- 通过估算这两种划分之间的差异，就能对 A 进行评价：与 y 对应的划分的差异越小，则说明 A 越好。
- **信息熵仅仅是判断这个差异的一种方法，其他能判断这两个划分差异的机制都能够用于特征子集的评价**。

3.**将特征子集搜索机制与子集评价机制结合就能得到特征选择方法**。

- 事实上，决策树可以用于特征选择，所有树结点的划分属性所组成的集合就是选择出来的特征子集。
- 其他特征选择方法本质上都是显式或者隐式地结合了某些子集搜索机制和子集评价机制。

4.常见的特征选择方法分为以下三种，主要区别在于特征选择部分是否使用后续的学习器。

- **过滤式**(filter)：先对数据集进行特征选择，其过程与后续学习器无关，即设计一些统计量来过滤特征，并不考虑后续学习器问题
- **包裹式**(wrapper)：实际上就是一个分类器，它是将后续的学习器的性能作为特征子集的评价标准。
- **嵌入式**(embedding)：实际上是学习器自主选择特征。

5.最简单的特征选择方法是：**去掉取值变化小的特征**。

假如某特征只有 0 和 1 的两种取值，并且所有输入样本中，95% 的样本的该特征取值都是 1 ，那就可以认为该特征作用不大。

当然，该方法的一个前提是，特征值都是**离散型**才使用该方法；如果是连续型，需要离散化后再使用，并且实际上一般不会出现 95% 以上都取某个值的特征的存在。

所以，这个方法简单，但不太好用，可以作为特征选择的一个预处理，先去掉变化小的特征，然后再开始选择上述三种类型的特征选择方法。

##### 3.4.2 过滤式选择

该方法**先对数据集进行特征选择，然后再训练学习器**。特征选择过程与后续学习器无关。

也就是先采用特征选择对初始特征进行过滤，然后用过滤后的特征训练模型。

- 优点是**计算时间上比较高效，而且对过拟合问题有较高的鲁棒性**；
- 缺点是**倾向于选择冗余特征**，即没有考虑到特征之间的相关性。

###### 3.4.2.1 Relief 方法

1.`Relief:Relevant Features`是一种著名的过滤式特征选择方法。该方法设计了一个**相关统计量来度量特征的重要性**。

- 该统计量是一个向量，其中每个分量都对应于一个初始特征。特征子集的重要性则是由**该子集中每个特征所对应的相关统计量分量之和来决定的**。

- 最终只需要指定一个阈值 $\gamma$，然后**选择比 $\gamma$ 大的相关统计量分量所对应的特征**即可。

  也可以**指定特征个数 m** ，然后选择相关统计量分量最大的 m 个特征。

2.给定训练集$D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in{0, 1}$。对于每个样本$\vec{x_i}$：

- `Relief` 先在$\vec{x_i}$同类样本中寻找其最近邻 $\vec{x_{nm_i}}$，称作猜中近邻 `near-hit` ；
- 然后从$\vec{x_i}$ 的异类样本寻找最近邻$\vec{x_{nm_i}}$，称作猜错近邻`near-miss`。
- 然后相关统计量对应于属性`j`的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j}, x_{nh_i,j})^2 + diff(x_{i,j},x_{nm_i,j})^2)
$$

其中$diff(x_{a,j},x_{b,j})$是两个样本在属性 `j` 上的差异值，其结果取决于该属性是离散的还是连续的：

- 如果是离散的，则有

$$
    diff(x{a,j},x{b,j})=
    \begin{cases}
    0,\quad if\; x_{a,j}=x_{b,j}\\
    1, \quad else
    \end{cases}
$$

- 如果是连续的，则有

$$
diff(x{a,j},x{b,j})=|x_{a,j} - x_{b,j}|
$$

​     注意，此时需要对$x_{a,j},x_{b,j}$进行标准化到`[0,1]`区间。

3.根据 3 的公式可以知道，如果

- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离小于 $\vec{x_i}$ 与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是有益的，于是增大属性 `j` 所对应的统计量分量。
- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离大于$\vec{x_i}$  与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是起负作用的，于是减小属性 `j` 所对应的统计量分量。
- 最后对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量。**分量值越大，则对应属性的分类能力越强**。

4.`Relief` 是为二分类问题设计的，其拓展变体 `Relief-F` 可以处理多分类问题。

假定数据集 D 中的样本类别为：${c_1, c_2, \cdots, c_K}$  。对于样本 $\vec{x_i}$ ，假设 $\breve{y_i}=c_k$ 。

- `Relief-F` 先在类别 $c_k$ 的样本中寻找 $\vec{x_i}$ 的最近邻 $\vec{x_{nh_i}}$ 作为猜中近邻。
- 然后在 $c_k$  之外的每个类别中分别找到一个 $\vec{x_i}$的最近邻 $\vec{x_{nm_i^l}}, l=1,2,\cdots,K; l \neq k$$   作为猜错近邻。
- 于是相关统计量对应于属性  j 的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j},x_{nh_{i,j}})^2+\sum_{l\neq k}(p_l\times diff(x_{i,j},x_{nm_{i,j}^l})^2))
$$

​     其中 $p_l$ 作为第 $l$ 类的样本在数据集 D 中所占的比例。

###### 3.4.2.2 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用 sklearn 的 feature_selection 库的 VarianceThreshold 类来选择特征的代码如下：

```python
from sklearn.feature_selection import VarianceThreshold

#方差选择法，返回值为特征选择后的数据
#参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(data)
```

###### 3.4.2.3 相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。用 feature_selection 库的 SelectKBest 类结合相关系数来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

#选择 K 个最好的特征，返回选择特征后的数据
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



###### 3.4.2.4 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有 N 种取值，因变量有 M 种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距，构建统计量：
$$
X^2 = \sum\frac{(A-E)^2}{E}
$$
不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用 feature_selection 库的 SelectKBest 类结合卡方检验来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
```

###### 3.4.2.5 互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：
$$
I(X;Y)=\sum_{x\in X}\sum_{y\in Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$
为了处理定量数据，**最大信息系数法**被提出，使用 feature_selection 库的 SelectKBest 类结合最大信息系数法来选择特征的代码如下

```python
from sklearn.feature_selection import SelectKBest
 from minepy import MINE
 
 #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
 def mic(x, y):
     m = MINE()
     m.compute_score(x, y)
     return (m.mic(), 0.5)

#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



##### 3.4.3 包裹式选择

1.相比于过滤式特征选择不考虑后续学习器，包裹式特征选择**直接把最终将要使用的学习器的性能作为特征子集的评价原则**。其目的就是为给定学习器选择最有利于其性能、量身定做的特征子集。

- 优点是直接针对特定学习器进行优化，考虑到特征之间的关联性，因此**通常包裹式特征选择比过滤式特征选择能训练得到一个更好性能的学习器**，
- 缺点是由于特征选择过程需要多次训练学习器，故计算**开销要比过滤式特征选择要大得多**。

2.`LVW:Las Vegas Wrapper`是一个典型的包裹式特征选择方法。它是` Las Vegas method`   框架下使用随机策略来进行子集搜索，并以最终分类器的误差作为特征子集的评价标准。

3. `LVW` 算法：

- 输入：数据集 D，特征集 A，学习器 estimator, 迭代停止条件 T

- 输出：最优特征子集 $A^*$

- 算法步骤：

  - 初始化：令候选的最优特征子集$\breve{A^*}=A$, 然后学习器 estimator 在特征子集 $\breve{A^*}$上使用交叉验证法进行学习，通过学习结果评估学习器的误差 $err^*$。
  - 迭代，停止条件为迭代次数达到 T。迭代过程是：
    - 随机产生特征子集 $A^\prime$
    - 学习器在特征子集 $A^\prime$ 上使用交叉验证法进行学习，通过学习评估学习器的误差 $err^\prime$
    - 如果 $err^\prime$ 小于 $err^*$，或者$err^\prime = err^*$，但是特征子集 $A^\prime$ 的特征数量少于候选的最优特征子集$\breve{A^*}$，则令$A^\prime$ 为候选的最优特征子集，即 $\breve{A^*} = A^\prime; err^* = err^\prime$

  - 最终有 $A^* = \breve{A^*}$

4.由于 `LVW` 算法中**每次特征子集评价都需要训练学习器，计算开销很大**，因此算法设计了停止条件控制参数 $T$。

但是如果初始特征数量很多、$T$ 设置较大、以及每一轮训练的时间较长， 则很可能算法运行很长时间都不会停止。即：**如果有运行时间限制，则有可能给不出解**。

5.**递归特征消除法**：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用 feature_selection 库的 RFE 类来选择特征的代码如下：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```



##### 3.4.4 嵌入式选择

1.在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。

嵌入式特征选择是将特征选择与学习器训练过程融为一体，两者在同一个优化过程中完成的。**即学习器训练过程中自动进行了特征选择**。

常用的方法包括：

- 利用正则化，如`L_1, L_2` 范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；
- 使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。

2.以线性回归模型为例。给定数据集 $D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in R$。以平方差为损失函数，则优化目标是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2$

- 如果使用 $L_2$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||^2_2 , \lambda > 0$，此时称作**岭回归**(ridge regression)。 
- 如果使用 $L_1$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||_1 , \lambda > 0$，此时称作**LASSO(Least Absolute Shrinkage and Selection Operator)回归**。 

3.引入 $L_1$ 范数除了**降低过拟合风险**之外，还有一个好处：它求得的 $\vec{w}$ 会有较多的分量为零。即：**它更容易获得稀疏解**。

于是基于 $L_1$ 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。

4.常见的嵌入式选择模型：

- 在 `Lasso` 中，$\lambda$ 参数控制了稀疏性：
  - 如果 $\lambda$ 越小，则稀疏性越小，被选择的特征越多；
  - 相反，$\lambda$ 越大，则稀疏性越大，被选择的特征越少；
- 在 `SVM` 和 逻辑回归中，参数 `C` 控制了稀疏性：
  - 如果 `C` 越小，则稀疏性越大，被选择的特征越少；
  - 如果 `C` 越大， 则稀疏性越小，被选择的特征越多。



#### 3.5 特征提取

特征提取一般是在特征选择之前，它提取的对象是**原始数据**，目的就是自动地构建新的特征，**将原始数据转换为一组具有明显物理意义（比如 Gabor、几何特征、纹理特征）或者统计意义的特征**。

一般常用的方法包括降维（PCA、ICA、LDA等）、图像方面的SIFT、Gabor、HOG等、文本方面的词袋模型、词嵌入模型等，这里简单介绍这几种方法的一些基本概念。

##### 3.5.1 降维

1.**PCA**(Principal Component Analysis，主成分分析)

PCA 是降维最经典的方法，它旨在是找到**数据中的主成分，并利用这些主成分来表征原始数据，从而达到降维的目的**。

PCA 的思想是通过坐标轴转换，寻找数据分布的最优子空间。

比如，在三维空间中有一系列数据点，它们分布在过原点的平面上，如果采用自然坐标系的 x，y，z 三个轴表示数据，需要三个维度，但实际上这些数据点都在同一个二维平面上，如果我们可以通过坐标轴转换使得数据所在平面和 x，y 平面重合，我们就可以通过新的 x'、y' 轴来表示原始数据，并且没有任何损失，这就完成了降维的目的，而且这两个新的轴就是我们需要找的主成分。

因此，PCA 的解法一般分为以下几个步骤：

1. 对样本数据进行中心化处理；
2. 求样本协方差矩阵；
3. 对协方差矩阵进行特征值分解，将特征值从大到小排列；
4. 取特征值前 n 个最大的对应的特征向量 `W1, W2, ..., Wn` ，这样将原来 m 维的样本降低到 n 维。

通过 PCA ，就可以将方差较小的特征给抛弃，这里，特征向量可以理解为坐标转换中新坐标轴的方向，特征值表示在对应特征向量上的方差，**特征值越大，方差越大，信息量也就越大**。这也是为什么选择前 n 个最大的特征值对应的特征向量，因为这些特征包含更多重要的信息。

**PCA 是一种线性降维方法，这也是它的一个局限性**。不过也有很多解决方法，比如采用核映射对 PCA 进行拓展得到核主成分分析(KPCA)，或者是采用流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些 PCA 效果不好的复杂数据集进行非线性降维操作。

2.**LDA**(Linear Discriminant Analysis，线性判别分析)

LDA 是一种有监督学习算法，相比较 PCA，它考虑到数据的类别信息，而 PCA 没有考虑，只是将数据映射到方差比较大的方向上而已。

因为考虑数据类别信息，所以 LDA 的目的不仅仅是降维，还需要找到一个投影方向，使得投影后的样本尽可能按照原始类别分开，即寻找一个**可以最大化类间距离以及最小化类内距离**的方向。

LDA 的主要步骤如下：

1. 分别计算每个类别 i 的原始中心点：$m_i = \frac{1}{n_i}\sum{x}$
2. 类别 i 投影后的中心点为: $m_i = w^Tm_i$
3. 衡量类别 i 投影后，类间的分散程度，用方差来表示：$s_i = \sum{(y-m_i)^2}$
4. 使用下面的式子表示 LDA 投影到 w 后的损失函数，最大化 J(w) 可以求出最优的 w , $J(w) = \frac{|m_1-m_2|^2}{s_1^2 + s_2^2}$，具体的解法参考[机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)。

LDA 的优点如下：

- 相比较 PCA，LDA 更加擅长处理带有类别信息的数据；
- 线性模型对噪声的鲁棒性比较好，LDA 是一种有效的降维方法。

相应的，也有如下缺点：

- LDA 对**数据的分布做出了很强的假设**，比如每个类别数据都是高斯分布、各个类的协方差相等。这些假设在实际中不一定完全满足。
- **LDA 模型简单，表达能力有一定局限性**。但这可以通过引入**核函数**拓展 LDA 来处理分布比较复杂的数据。

3.**ICA**(Independent Component Analysis，独立成分分析)

PCA特征转换降维，提取的是**不相关**的部分，ICA独立成分分析，获得的是**相互独立**的属性。ICA算法本质寻找一个线性变换 `z = Wx`，使得 z 的**各个特征分量之间的独立性最大**。

通常先采用 PCA 对数据进行降维，然后再用 ICA 来从多个维度分离出有用数据。PCA 是 ICA 的数据预处理方法。

具体可以查看知乎上的这个问题和回答 [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)。

##### 3.5.2 图像特征提取

图像的特征提取，在深度学习火起来之前，是有很多传统的特征提取方法，比较常见的包括以下几种。

1.**SIFT** 特征

SIFT 是图像特征提取中非常广泛应用的特征。它包含以下几种优点：

- 具有旋转、尺度、平移、视角及亮度不变性，有利于对目标特征信息进行有效表达；
- SIFT 特征对参数调整鲁棒性好，可以根据场景需要调整适宜的特征点数量进行特征描述，以便进行特征分析。

SIFT 对图像局部特征点的提取主要包括四个步骤：

1. 疑似特征点检测
2. 去除伪特征点
3. 特征点梯度与方向匹配
4. 特征描述向量的生成

SIFT 的缺点是不借助硬件加速或者专门的图像处理器很难实现。

2.**SURF** 特征

SURF 特征是对 SIFT 算法的改进，降低了时间复杂度，并且提高了鲁棒性。

它主要是简化了 SIFT 的一些运算，如将 SIFT 中的高斯二阶微分的模型进行了简化，使得卷积平滑操作仅需要转换成加减运算。并且最终生成的特征向量维度从 128 维减少为 64 维。

3.**HOG** 特征

方向梯度直方图(HOG)特征是 2005 年针对行人检测问题提出的直方图特征，它通过计算和统计图像局部区域的梯度方向直方图来实现特征描述。

HOG 特征提取步骤如下：

1. **归一化处理**。先将图像转为灰度图像，再利用伽马校正实现。这一步骤是为了提高图像特征描述对光照及环境变化的鲁棒性，降低图像局部的阴影、局部曝光过多和纹理失真，尽可能抵制噪声干扰；
2. **计算图像梯度**；
3. **统计梯度方向**；
4. **特征向量归一化**；为克服光照不均匀变化及前景与背景的对比差异，需要对块内的特征向量进行归一化处理。
5. **生成特征向量**。



4.**LBP** 特征

局部二值模式（LBP）是一种描述图像局部纹理的特征算子，它具有旋转不变性和灰度不变性的优点。

LBP 特征描述的是一种灰度范围内的图像处理操作技术，针对的是**输入为 8 位或者 16 位的灰度图像**。

LBP 特征通过对**窗口中心点与邻域点的关系进行比较**，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了**复杂场景下（光照变换）特征**描述问题。

根据窗口领域的不同分为两种，经典 LBP 和圆形 LBP。前者的窗口是 3×3 的正方形窗口，后者将窗口从正方形拓展为任意圆形领域。

更详细的可以参考这篇文章--[图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)

当然上述特征都是比较传统的图像特征提取方法了，现在图像基本都直接利用 CNN（卷积神经网络）来进行特征提取以及分类。

##### 3.5.3 文本特征提取

1.**词袋模型**

最基础的文本表示模型是词袋模型。

具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量的每一个维度代表一个单词，而该维度的权重反映了该单词在原来文章中的重要程度。

通常采用 **TF-IDF** 计算权重，公式为 `TF-IDF(t, d) = TF(t,d) × IDF(t)`

其中 TF(t, d) 表示单词 t 在文档 d 中出现的频率，IDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，其表示为：
$$
IDF(t)=log\frac{文章总数}{包含单词 t 的文章总数+1}
$$
直观的解释就是，如果这个单词在多篇文章都出现过，那么它很可能是比较通用的词汇，对于区分文章的贡献比较小，自然其权重也就比较小，即 IDF(t) 会比较小。

2.**N-gram 模型**

词袋模型是以单词为单位进行划分，但有时候进行单词级别划分并不是很好的做法，毕竟有的单词组合起来才是其要表达的含义，比如说 `natural language processing(自然语言处理)`、`computer vision(计算机视觉)` 等。

因此可以将连续出现的 n 个词 (n <= N) 组成的词组(N-gram)作为一个单独的特征放到向量表示中，构成了 N-gram 模型。

另外，同一个词可能会有多种词性变化，但却具有相同含义，所以实际应用中还会对单词进行**词干抽取**(Word Stemming)处理，即将不同词性的单词统一为同一词干的形式。

3.**词嵌入模型**

词嵌入是一类将词向量化的模型的统称，核心思想是将**每个词都映射成低维空间（通常 K=50~300 维）上的一个稠密向量（Dense Vector）**。

常用的词嵌入模型是 **Word2Vec**。它是一种底层的神经网络模型，有两种网络结构，分别是 CBOW(Continues Bag of Words) 和 Skip-gram。

CBOW 是根据**上下文出现的词语预测当前词**的生成概率；Skip-gram 是根据**当前词来预测上下文中各个词的生成概率**。

词嵌入模型是将每个词都映射成一个 K 维的向量，如果一篇文档有 N 个单词，那么每篇文档就可以用一个 N×K 的矩阵进行表示，但这种表示过于底层。实际应用中，如果直接将该矩阵作为原文本的特征表示输入到模型中训练，通常很难得到满意的结果，一般还需要对该矩阵进行处理，提取和构造更高层的特征。

深度学习模型的出现正好提供了一种自动进行特征工程的方法，它的每个隐含层都相当于不同抽象层次的特征。卷积神经网络（CNN)和循环神经网络（RNN)在文本表示中都取得了很好的效果，这是因为它们可以很好地对文本进行建模，抽取出一些高层的语义特征。

##### 3.5.4 特征提取和特征选择的区别

特征提取与特征选择都是为了从原始特征中找出最有效的特征。

它们之间的区别是特征提取强调通过**特征转换**的方式得到一组具有明显物理或统计意义的特征；

而特征选择是从特征集合中挑选一组具有明显物理或统计意义的**特征子集**。

两者都能**帮助减少特征的维度、数据冗余**，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。

#### 3.6 特征构建

特征构建是指**从原始数据中人工的构建新的特征**。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。

特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用**混合属性或者组合属性**来创建新的特征，或是**分解或切分原有的特征**来创建新的特征。

特征构建非常需要相关的领域知识或者丰富的实践经验才能很好构建出更好的有用的新特征，相比于特征提取，特征提取是通过一些现成的特征提取方法来将原始数据进行特征转换，而特征构建就需要我们自己人为的手工构建特征，比如组合两个特征，或者分解一个特征为多个新的特征。



#### 3.7 多类分类问题

------

### 参考

- 《hands-on-ml-with-sklearn-and-tf》第二节
- https://www.jiqizhixin.com/articles/091202
- https://blog.csdn.net/fendegao/article/details/79968994
- https://blog.csdn.net/xg123321123/article/details/80781611
- https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378b
- 《百面机器学习》第一章 特征工程
- [机器学习之特征工程](https://blog.csdn.net/dream_angel_z/article/details/49388733#commentBox)
- [[数据预处理（方法总结）]](https://www.cnblogs.com/sherial/archive/2018/03/07/8522405.html)
- [Python数据分析（三）——数据预处理](https://gofisher.github.io/2018/06/22/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/)
- [Python数据分析（二）——数据探索](https://gofisher.github.io/2018/06/20/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/)
- [【Python数据分析基础】: 异常值检测和处理](https://juejin.im/post/5b6a44f55188251aa8294b8c)
- http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/8_feature_selection.html
- [Python机器学习-数据预处理技术（标准化处理、归一化、二值化、独热编码、标记编码）](https://blog.csdn.net/weixin_38168620/article/details/79233086)
- [[Scikit-learn介绍几种常用的特征选择方法](http://dataunion.org/14072.html)](http://dataunion.org/14072.html)
- [博客园--机器学习之特征工程](https://www.cnblogs.com/wxquare/p/5484636.html)
- [机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)
- [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)
- [图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)
- [Gabor特征提取](https://blog.csdn.net/xidianzhimeng/article/details/19493019)