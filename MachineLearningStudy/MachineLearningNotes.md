### 机器学习算法知识点

* 序列标注模型有**HMM、MEMM、CRF**。
* 能够用于指导特征选择的指标有**信息增益、信息增益率、基尼系数和信息熵**
* 用户使用**稀疏特征**进行训练时，对于**离散特征缺省值**的处理方法是**将缺省值付给一个全新值来标记**。
* **使用L2正则化可以得到平滑的权值**。
* 核回归中，最影响回归的过拟合性和欠拟合之间平衡的参数为核函数的宽度。
* 主要用来解无约束优化问题的优化算法有：**随机梯度下降， LBFGS， 共轭梯度法，拟牛顿法**
* 机器学习时间序列模型有：**移动平均法(MA)、自回归模型(AR)、自回归滑动平均模型(ARMA)、GARCH模型和指数平滑法。**具体参考[机器学习：时间序列模型](http://blog.csdn.net/ztf312/article/details/50890267)。

------

##### 1. 逻辑回归与线性回归

* 逻辑回归的损失函数是**log 对数损失函数**,其目标是**最大化似然函数**.

------

##### 2. k-近邻

------

##### 3. 朴素贝叶斯

* 朴素贝叶斯是**最小化后验概率**

------

##### 4. 决策树

------

##### 5. 支持向量机

* SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面,属于**结构风险最小化**

* SVM可以通过正则化系数控制模型的复杂度,避免过拟合.

* 如果**C值很大**，我们的松弛变量$ξ_i$就会很小，我们允许的误差也就很小，因此分类器会尽量正确分类样本，结果我们就会得到一个**小间距的超平面**，这样的结果可能会导致在新样本的分类性能很差，而在训练集上的分类性能很好，也就是出现**过拟合（overfitting）**现象;如果**C值很小**，我们**所允许的误差也就很大**，分类器即使要错误地分类样本，它也会找寻**大间距的超平面**，在这种情况下，如果你的训练集是线性可分的，也有可能出现错误分类的样本。

  因此，在SVM算法的训练上，我们可以通过**减小C值来避免overfitting的发生**

* ​

------

##### 6. 提升方法(AdaBoost, 提升树Boosting Tree)

* AdaBoost算法中，所有被错分的样本的权重更新比例相同。

------

##### 7. EM算法

------

##### 8. 隐马尔可夫模型(HMM)

* 无监督训练方法是**EM**算法

------

##### 9. 条件随机场(CRF)

------

##### 10. 深度学习

* 主要用于无监督的深度学习网络是**Restricted Boltzmann Machines, AutoEncoder, Deep Belief Networks**

------

##### 11 降维

* 降维的算法有**PCA, Word2Vec，AutoEncoder, Latent Dirichlet Allocation，LASSO,小波分析法，线性判别法，拉普拉斯特征映射**。具体参考[数据降维方法小结](http://blog.csdn.net/yujianmin1990/article/details/48223001)。

------

##### 12 聚类

* 影响聚类算法效果的主要原因有：**特征选取、模式相似性测度和分类准则。**

------

#### 练习题

1. 已知两个一维模式类别的类概率密度函数为:

   ![img](https://uploadfiles.nowcoder.com/images/20160829/59_1472451822933_E39598100A5E449D6E3F3A28AB61F54B)

   先验概率P(1)=0.6,P(2)=0.4,则样本{x1=1.35,x2=1.45,x3=1.55,x4=1.65}各属于哪一类别?

解答：

> 概率问题基本上都是贝叶斯和全概率互相扯蛋,，他们之间往往可以通过条件概率建立联系。
>
> 本题中，要判断 xi 属于w1，还是w2，就是判断 p(w1 | xi)  和 p(w2 | xi)的大小关系。即在xi已经发生的情况下，xi 属于哪个类别（w1 ，w2）的可能性更大。
>
> p(w1 | xi) = p(xiw1) / p(xi) = p(xi | w1) * p(w1) / p(xi) = 0.6*(2 - xi) / p(xi)   // 因为xi都在 （1，2)范围
>
> p(w2 | xi) = p(xiw2) / p(xi) = p(xi | w2) * p(w2) / p(xi) = 0.4*(xi - 1) / p(xi)   // 因为xi都在 （1，2)范围
>
> 上面两等式相减，得：
>
> delta = p(w1 | xi) - p(w2 | xi) = (1.6 - xi) / p(xi)
>
> 所以，在上诉样本中，大于1.6的，属于w2，小于1.6的，属于w1。

