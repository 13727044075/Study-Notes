# [机器学习笔记]Note10--支持向量机(SVM)

标签（空格分隔）： 机器学习

---
[TOC]

继续是[机器学习课程](https://www.coursera.org/learn/machine-learning)的笔记，这节课的内容是介绍支持向量机（SVM）的内容。SVM是一个非常强大且流行的算法，在一些情况下，面对一些复杂的非线性问题可以提供比逻辑回归或神经网络更加简洁更加有效的结果。

### 优化目标
  首先是以逻辑回归为例展开讨论，逻辑回归的模型是$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$,这里分$y=1$和$y=0$两种情况讨论：

* $y=1$时，希望假设的$h_\theta(x) \approx 1$,也就是令$z=\theta^Tx \gg 0$
*  $y=0$时，希望假设的$h_\theta(x) \approx 0$,也就是令$z=\theta^Tx \ll 0$

从代价函数来看，逻辑回归的代价函数如下所示：
$$
J(\theta) = -\frac{1}{m} [\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)}log(1-h_\theta(x^{(i)}))]
$$
对于任意训练集中的一个实例，对总的代价的影响为
$$
-(ylogh_\theta(x)+(1-y)log(1-h_\theta(x))) \\
= -ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})
$$

为了让每个实例造成的代价都尽可能地小，这里也说分$y=1$和$y=0$两种情况讨论，如下图所示,令$z=\theta^Tx$，最佳的情况是代价为0，但由下图中曲线可以看出，代价始终会存在，且不为0。

![此处输入图片的描述][1]

而在SVM中，我们将曲线的代价函数转变成由两条线段构成的折线，如下图所示：

* $y=1$时，希望构建的新代价函数如$cost_1(z)$所示，当$z \ge 1$时，$cost_1(z)=0$
*  $y=0$时，希望构建的新代价函数如$cost_0(z)$所示，当$z \le -1$时，$cost_0(z)=0$

![此处输入图片的描述][2]

用两个新构建的代价函数替换原来逻辑回归的代价函数，得到：
$$
\frac{1}{m} [\sum_{i=1}^my^{(i)}cost_1(z)+(1-y^{(i)})cost_0(z)]+\frac{\lambda}{2m}\sum_{j=1}^{n} \theta_j^2
$$

这里对上面的代价函数作如下调整：

1. 因为$\frac{1}{m}$实际上不影响最优化的结构，将其去掉
2. 因为归一化参数$\lambda$控制的是归一化这一项在整个代价函数中占的比例，对于SVM，我们想要控制的新构建的代价函数部分，因此我们去掉$\lambda$的同时给第一项乘以一个常数C，相当于我们将整个代价函数除以了$\lambda$，且$C=\frac{1}{\lambda}$

我们依旧是希望能找出使得该代价函数最小的参数。注意，**调整后的代价函数是一个凸函数**，而非之前逻辑回归那样的非凸函数，这意味着，求解的过程中，**不会陷入局部最小值而错过全局最小值的情况**，修改后的代价函数如下：
$$
min_\theta C \sum_{i=1}^m [y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n} \theta_j^2
$$

最后给出SVM的假设为：
$$
h_\theta(x) = 
\begin{cases}
1\quad if\ \theta^Tx \ge 0 \\
0\quad if\ \theta^Tx \lt 0
\end{cases}
$$
注意，这里给出的假设在预测时是以z与0的大小关系作为依据的，但是，在**训练函数时，我们是以正负1为依据的**，这是SVM与逻辑回归的一个关键区别，且导致了后面将介绍的支持向量机的特性。

### 支持向量机的判定边界
  SVM有的时候也被称为**最大间隔分类器(Large Margin Classifer)**，其原因是：**SVM可以尝试发现一个与样本数据集之间有着最大间隔的判定边界。**
  
  上一小节得到了SVM的代价函数表达式，并且其跟逻辑回归的区别是新构建了两个函数$cost_1(z)和cost_0(z)$，其图像分别如下所示
  
  ![此处输入图片的描述][3]
  
  由此可以得知,为了最小化代价函数：

* $y=1$时，希望$z \ge 1$时，即$\theta^Tx \ge 1$
* $y=0$时，希望$z \le -1$时，即$\theta^Tx \le -1$

下图是一个可以用直线来区分的分类问题实例，图中绿色和红色的两条线代表作两条逻辑回归的判定边界，而黑色的线代表的则是SVM的判定边界，从图上可以看出黑色的线似乎是更加合理，蓝色的两条线代表的是SVM的判定边界与样本数据之间的间隔，从这幅图也可以看出SVM被称为最大间隔分类器的原因。

![此处输入图片的描述][4]

接下来是考虑下在SVM中**归一化常数C**的作用。

假设选择的C是一个非常大的值，那么在最小化代价函数的过程中，我们希望找出在$y=1$和$y=0$两种情况下都使得代价函数中第一项尽量为0的参数，如果我们找到这样的参数，则我们的最小化问题便转变成：
$$
min \frac{1}{2}\sum_{j=1}^n \theta_j^2\quad s.t 
\begin{cases}
\theta^Tx^{(i)} \ge 1\ \quad if\ y^{(i)}=1 \\
\theta^Tx^{(i)} \le -1\ \ if\ y^{(i)}=0 
\end{cases}
$$
在这种情况下，我们得出的SVM判定边界就是上面黑线那样，具有尝试使得判定边界与样本数据间间隔最大的特性。

但是，使得判断边界与样本数据之间间隔最大并总是好事，假设，我们的数据集如下图所示：

![此处输入图片的描述][5]

也就是在数据集中间有一个较为明显的异常值，即图中左侧在圆圈处下方的红色叉数据，此时如果选择较大的常数C，会得到图中红色的直线，这样并非很合理，但如果选择较小的C，可能会得到图中黑色直线所示的判定边界。也就是说，**C值越小，SVM对异常值越不敏感**。

回顾$C=\frac{1}{\lambda}$,有：

* **C较大时，相当于$\lambda$较小，可能会导致过拟合，高偏倚**
* **C较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差**

### 核函数
  接下来是介绍如何修改SVM算法来解决一些非线性分类的问题。
  
  首先是回顾之前讨论的一个使用高次的多项式模型来解决的一个无法用直线进行分类的问题，其样本集如下图所示：
  
  ![此处输入图片的描述][6]

为了获得上图所示的判定边界，使用的模型可能是$\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots$的形式。

这里使用一系列新的特征$f$来替换模型中的每一项，如令：$f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,f_5=x_2^2,\ldots$,那么得到新的模型形式为：$h_\theta(x)=\theta_0+\theta_1 f_1+\theta_2 f_2+\ldots+\theta_n f_n$。但，除了对原有的特征进行组合外，能否有更好的办法来构造新的特征$f_1,f_2,f_3$。

#### 使用核函数计算出新的特征
  给定一个训练实例x，可以利用x的各个特征与预先选定的**地标(landmarks)**$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$。如下图所示
  
  ![此处输入图片的描述][7]
  
  这里可以使用如**相似度函数**来计算新的特征，如令：$f_1 = similarity(x,l^{(1)})=exp(-\frac{||x-l^{(1)}||^2}{2\sigma^2})$,其中$||x-l^{(1)}||^2=\sum_{j=1}^n(x_j-l_j^{(1)})^2$，表示实例x中所有的特征与地标$l^{(1)}$之间的距离的和。
  
  这里的相似度函数$similarity(x,l^{(1)})$就是核函数，实质上这就是一个**高斯核函数**，注意，这个函数与正态分布没有什么实际上的关系，只是看上去像而已。
  
  这些地标的作用是什么呢？如果一个训练实例x与地标L之间的距离近似于0，则新特征f近似于$e^{-0}=1$;而如果x与L之间距离非常远，则f近似于0。
  
  这里假设训练实例含有两个特征$[x_1,x_2]$，给定地标$l^{(1)}$与不同的$\sigma$，如下图所示:
  
  ![此处输入图片的描述][8]
  
 上图中水平面的坐标为$x_1,x_2$，垂直坐标代表f。可以看出只有当x与$l^{(1)}$ 重合时，f才具有最大值，随着x的改变，f值改变的速率受到$\sigma^2$的控制。
 
 下图是另外一个例子，图中有3个地标，模型$h_\theta(x)=\theta_0+\theta_1f_1+\theta_2 f_2+\theta_1 f_3$,令$\theta_0=-0.5,\theta_1=\theta_2=1,\theta_3=0$，当实例处于红色的点位置时，因为离$l^{(1)}$更近，但是离$l^{(2)}和l^{(3)}$较远,则有$f_1 \approx 1, f_2 = f_3 \approx 0$,因此$h_\theta(x) \ge 1$，预测$y=1$。同理可以求出对于离$l^{(2)}$较近的绿色点，也预测$y=1$，但对于青蓝色的点，距离三个点都比较远，所以预测$y=0$，从而可以得到图中红色的封闭曲线的范围内都是预测$y=1$，而曲线外部就是预测$y=0$。
 
 因此，在预测的时候，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征$f_1,f_2,f_3$。
 
#### 选择地标
  一般是根据训练集的数量来选择地标的数量，即**如果训练集有m个实例，则选择m个地标，并且令$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},\ldots,l^{(m)}=x^{(m)}$**。这样做的好处是：得到的新特征是建立在原有特征与训练集中所有其他特征之间的距离的基础之上的，即：
  
  ![此处输入图片的描述][9]

下面将核函数应用到SVM到，修改SVM的假设为：
**给定x，计算新特征f，当$\theta^Tf \ge 0$时，预测$y=1$,否则反之。**

相应地修改代价函数为：$min_\theta C \sum_{i=1}^m [y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^{n=m} \theta_j^2$

而在具体实施过程中，对最后一项需要做点调整，计算$\sum_{j=1}^{n=m} \theta_j^2=\theta^T\theta$时，**使用$\theta^TM\theta$代替$\theta^T\theta$，**其中$M$是根据选择的核函数而不同的一个矩阵，这样做的原因是为了简化运算。

理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用$M$来简化计算的方法不适合逻辑回归，计算会非常耗费时间。

这里不介绍最小化SVM的代价函数的方法，可以使用现有的软件包（如liblinear，libsvm等）。在使用这些软件包最小化我们的代价函数的时候，通常需要编写核函数，并且如果我们使用高斯核函数，在使用前进行特征缩放是非常必要的。

另外，SVM也可以不用核函数，不使用核函数又称为**线性核函数**，当不采用非常复杂的函数，或者训练集特征非常多而实例非常少的时候，可以使用这种不带核函数的SVM。

下面是SVM的两个参数$C和\sigma$的影响：

* C较大时，相当于$\lambda$较小，可能会导致过拟合，高偏倚
* C较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差
* $\sigma$较大时，特征f会比较平滑，导致低拟合，高偏差
* $\sigma$较小时，特征f会相对没有那么平滑，导致过拟合，高偏倚

在高斯核函数外，还有其他一些核函数，如：

* 多项式核函数(Polynomial Kernel)
* 字符串核函数(String Kernel)
* 卡方核函数(chi-square kernel)
* 直方图交集核函数(histogram intersection kernel)
* etc...

这些核函数的模板也都是根据训练集和地标之间的距离来构建新的特征，这些核函数需要满足Mercer's定理，才能被SVM的优化软件正确处理。

#### 多类分类
  在之前的[逻辑回归][10]这节课中，曾接受一对多的方法来解决一个多类分类问题。即如果一共有k个类，则需要使用k个模型，以及k个参数向量$\theta$。
  
  同样地，我们也可以训练k个SVM来解决多类分类问题。但是现在大多数SVM软件包都有内置的多类分类功能，可以直接使用。
  
### 逻辑回归和SVM
  对于逻辑回归和SVM两种算法，应该如何根据不同问题来选择，下面给出选择的准则：
  
* **如果相较于m而言，n要大很多，即训练集数据量不够支持训练一个复杂的非线性模型，则选择使用逻辑回归模型或者不带核函数的SVM**
* **如果n较小，而且m大小中等，例如n在1-1000之间，而m在10-10000之间，可以使用带高斯核函数的SVM**
* **如果n较小，而m较大，例如n在1-1000之间，而m大于50000，则使用SVM会比较慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或者不带核函数的SVM**

值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能会非常慢，选择SVM的原因**主要在于它的代价函数是凸函数，不存在局部最小值**。

### 小结
  本节课主要介绍一个非常流行的算法--支持向量机(SVM)，不过要真正弄懂这个算法的原理，单纯看完这节课其实感觉还是不太够的，还需要更深入了解，并且要顺便用代码实现下，通过实践来促进理解。
  
  


  [1]: http://img.blog.csdn.net/20160714202714026
  [2]: http://img.blog.csdn.net/20160714203214361
  [3]: http://img.blog.csdn.net/20160716203716429
  [4]: http://img.blog.csdn.net/20160716204329338
  [5]: http://img.blog.csdn.net/20160716205051185
  [6]: http://img.blog.csdn.net/20160720192613835
  [7]: http://img.blog.csdn.net/20160720193451174
  [8]: http://img.blog.csdn.net/20160720194427124
  [9]: http://img.blog.csdn.net/20160720201702875
  [10]: http://blog.csdn.net/lc013/article/details/51622656#t10