# [机器学习笔记]Note14--推荐系统

标签（空格分隔）： 机器学习

---
[TOC]

继续是[机器学习课程](https://www.coursera.org/learn/machine-learning)的笔记，本节课将介绍推荐系统的内容。

### 问题形式化
  推荐系统是机器学习的一个非常重要的应用，在很多音乐、购物等网站都有推荐系统，如豆瓣，淘宝，网易云音乐等都是有使用到推荐系统的，因此推荐系统的应用范围非常广泛。
  
  我们从一个例子开始定义推荐系统的问题。
  
  假设我们是一个电影供应商，我们有5部电影和4个用户，我们要求用户为电影评分。
  
  ![此处输入图片的描述][1]
  
  由上图可以知道，前3部电影是爱情片，后两部是动作片，用户Alice和Bob似乎更倾向于爱情片，而其他两位用户Carol和Dave似乎更倾向于动作片。并且没有一个用户给所有的电影都打过分，我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并依此作为推荐的依据。
  
  下面引入一些标记：

* $n_u$代表用户的数量
* $n_m$代表电影的数量
* $r(i,j)=1$表示用户i给电影j评过分
* $y^{(i,j)}$代表用户i给电影j的评分，在上图中，其评分范围是0~5分
* $m_j$代表用户j评过分的电影的总数

### 基于内容的推荐系统
  在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据就是有关这些东西的特征。
  
  在我们的例子中，我们可以假设每部电影都有两个特征，如$x_1$代表电影的浪漫程度，$x_2$代表电影的动作程度。
  
  ![此处输入图片的描述][2]
  
  如上图所示，每部电影都有一个特征向量，如$x^{(1)}=[0.9 0]$是第一部电影的特征向量。
  
  下面我们可以基于这些特征来构建一个推荐系统算法。
  
  假设我们使用线性回归模型，我们可以针对每个用户都训练一个线性回归模型，如$\theta^{(1)}$是第一个用户的模型的参数。
  
  于是，我们有：
* $\theta^{(j)}$是用户j的参数向量
* $x^{(i)}$是电影i的特征向量

对于用户j和电影i，我们预测评分为：$(\theta^{(j)})^T(x^{(i)})$

对于用户j，该线性回归模型的代价函数为预测误差的平方和，加上归一化项：
$$
min_{\theta^{(j)}} \frac{1}{2} \sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^n (\theta_k^{(j)})^2
$$

其中，$i:r(i,j)=1$表示我们只计算用户j评过分的电影。在一般的线性回归模型中，误差项和归一化项应该都是乘以$\frac{1}{2m}$，在这里我们将m去掉，并且不对偏倚项$\theta_0$进行归一化处理。

上面的代价函数是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：
$$
min_{\theta^{(1)},\ldots,\theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u}\sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u}\sum_{k=1}^n (\theta_k^{(j)})^2
$$

如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：
$$
\theta_k^{(j)} = \theta_k^{(j)} -\alpha \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}\ (for\ k=0) \\
\theta_k^{(j)} = \theta_k^{(j)} -\alpha (\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})\ (for\ k\neq 0)
$$

### 协同过滤算法
  接下来介绍一种可以自行学习所要使用的特征的算法--**协同过滤算法**。
  
  在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有了用户的参数，我们可以学习得出电影的特征。也就是给出参数$\theta^{(1)},\ldots,\theta^{(n_u)}$,来学习$x^{(1)},\ldots,x^{(n_m)}$,那么优化代价函数的公式如下所示:
$$
min_{x^{(1)},\ldots,x^{(n_m)}} \frac{1}{2} \sum_{j=1}^{n_m}\sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_m}\sum_{k=1}^n (x_k^{(i)})^2
$$  

但是如果我们即没有用户的参数，也没有电影的特征，这两种方法都不可行了。而协同过滤算法可以同时学习这两者。

我们的优化目标便改为同时针对$x和\theta$进行。
$$
J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)})=\frac{1}{2} \sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2} \sum_{j=1}^{n_m}\sum_{k=1}^n (x_k^{(i)})^2+\frac{\lambda}{2} \sum_{j=1}^{n_u}\sum_{k=1}^n (x_k^{(i)})^2
$$
对代价函数求偏导数的结果如下：
$$
x_k^{(j)} = x_k^{(j)} -\alpha (\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda x_k^{(i)}) \\
\theta_k^{(j)} = \theta_k^{(j)} -\alpha (\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})
$$

**注意，在协同过滤算法中，我们通常不使用偏倚项，如果需要的话，算法会自动学得。**

协同过滤算法使用步骤如下：

1. 将$x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)}$初始化为一些小的随机值
2. 使用梯度下降算法最小化代价函数
3. 在训练完算法后，我们预测$(\theta^{(j)})^T(x^{(i)})$为用户j给电影i的评分

通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。


### 低秩矩阵分解(Low Rank Matrix Factorization)
  接下来会介绍协同过滤算法的向量化实现，以及使用该算法可以做的其他事情，比如正在观看一部电影，能否推荐另一部相关的电影。
  
  首先还是利用初始给出的电影的例子，如下图所示：
  
  ![此处输入图片的描述][3]
  
  矩阵Y是一个$5\times 4$的矩阵，代表的就是左边4个用户分别给5部电影的评分，其中？号表示该用户没有看过该电影，因此没有打分。根据这个矩阵，可以根据评分公式$(\theta^{(j)})^T(x^{(i)})$，得到如下图所示的预测评分矩阵：
  
  ![此处输入图片的描述][4]
  
  上述预测评分矩阵中的位置$(i,j)$表示的就是用户j给电影i的评分，它的值就是$(\theta^{(j)})^T(x^{(i)})$。因而，这个矩阵第一行代表的就是所有用户分别给第一部电影的评分，即第i行表示所有用户给电影i的评分，而第j列表示用户j给所有电影的评分。
  
  这里可以使用向量化实现，如定义一个$X和\theta$矩阵，分别如下所示定义：
$$
X = \begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(n_m)})^T \end{bmatrix} \qquad \theta = \begin{bmatrix} (\theta^{(1)})^T \\ (\theta^{(2)})^T \\ \vdots \\ (\theta^{(n_u)})^T \end{bmatrix}
$$
然后预测评分矩阵就等于$X\theta^T$。

这里由于$X\theta^T$的低秩属性，因此协同过滤算法也被称为低秩矩阵分解。
  
  接下来就介绍如何寻找相关的电影，对于一部电影i，我们根据协同过滤算法可以学习到一个特征向量$x^{(i)}$，在这个向量中包含了$x_1,x_2,\ldots,x_n$个特征，这些特征包含了电影的重要数据，但一般很难进行数据可视化，同时也可能是人很难解释的这些特征实际上是什么，但这种特征学习方法得到的特征却是可以帮助我们进行推荐。
  
例如，如果一个用户正在观看电影$x^{(i)}$,我们可以寻找另一部电影$x^{(j)}$，依据是两部电影的特征向量之间的距离$|| x^{(i)}- x^{(j)} ||$的大小。当这个距离很小的时候，表示两部电影是非常相似的。

### 均值归一化
  最后介绍均值归一化，它有时候可以使得推荐算法运行得更好。
  
  首先看下如下面所示的用户评分数据：
  
  ![此处输入图片的描述][5]
  
  这里新增一个用户Eve，并且Eve没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？
  
  我们首先需要对结果Y矩阵进行均值归一化处理，**将每一位用户对某一部电影的评分减去所有用户对该电影评分的平均值**，如下所示：
  
![此处输入图片的描述][6]

上图中$\mu$矩阵就是一个均值矩阵，第i行表示所有用户对电影i的评分的平均值。进行均值归一化后，我们将使用最右边的新的Y矩阵来训练算法。

而**如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测值为$(\theta^{(j)})^T(x^{(i)})+\mu_i$。**

对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。

此外，如果某部电影没有评分，也可以使用这种均值归一化的方法，只是变成对每列数据进行均值归一化。

### 小结
  本节课，介绍了机器学习一个很重要的应用--推荐系统，介绍了其基本的定义以及使用的算法。


  [1]: http://img.blog.csdn.net/20160728193156461
  [2]: http://img.blog.csdn.net/20160728194116275http://img.blog.csdn.net/20160728193156461
  [3]: http://img.blog.csdn.net/20160729160411389
  [4]: http://img.blog.csdn.net/20160729160712207
  [5]: http://img.blog.csdn.net/20160729162740342
  [6]: http://img.blog.csdn.net/20160729163003311