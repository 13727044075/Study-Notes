# [机器学习笔记]Note6--神经网络：表达

标签（空格分隔）： 机器学习

---
[TOC]

继续是[机器学习课程](https://www.coursera.org/learn/machine-learning)的笔记，这节课会介绍神经网络的内容。

### 非线性假设
  在之前的课程中，我们看到使用非线性的多项式能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合($x_1x_2+x_1x_3+x_1x_4+\ldots+x_2x_3+x_2x_4+\ldots+x_99x_100$),那么我们也会有近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。
  
  当我们希望训练一个模型来识别视觉对象，比如识别一张图片上是否是一辆汽车，一种实现方法是利用许多汽车和非汽车的图片，然后利用**图片上一个个像素的值(饱和度或亮度)来作为特征**。这也是因为在计算机中，一张图片其实就一个包含所有像素值的矩阵。
  
  如果我们选择的是灰度图片，每个像素则只有一个值，而非RGB值，我们可以选择图片上两个不同位置的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车。
  
  但假设我们采用的是50*50像素的小图片，并且我们将所有的像素视为特征，那么就会有2500个特征，而如果我们进一步将两两特征组合构成一个多项式模型，则会有约$\frac{2500^2}{2}$，即接近三百万个特征。普通的逻辑回归模型，不能有效地处理那么多的特征，这个时候就需要**神经网络。**
  
### 神经网络介绍
  神经网络算法源自于对大脑的模仿。神经网络算法在八十到九十年代被广为使用过，但是之后由于其计算量大的原因逐渐减少了使用，而最近，从2006年开始，到后来2012年ImageNet比赛中CNN取得非常大的提升效果，现在神经网络变得非常流行，准确地说是深度神经网络。原因也是因为神经网络是非常依赖计算能力的，而要实现深度神经网络，也就是神经网络的层数更多，需要的计算量更大，但是现在随着计算机硬件的提高，还有就是数据量的增加，使得神经网络又开始流行起来了。
  
  神经网络算法的目的是发现一个能模型人类大脑学习能力的算法。研究表明，如果我们将视觉信号传导给大脑中负责其他感觉的大脑皮层处，则这些大脑组织将能学会如何处理视觉信号。

### 模型表达
  为了构建神经网络模型，我们会参考大脑中的神经网络。每个神经元可以被认为是一个**处理单元/神经核(processing unit/Nucleus)**,它含有许多**输入/树突(input/Dendrite)**,并且有一个**输出/轴突(output/Axon)**。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络，如下图所示：
  ![此处输入图片的描述][1]
  
  神经网络是模型就是建立在很多神经元之上的，每个神经元又是一个个学习模型。这些神经元(也叫**激活单元，activation unit**）采纳一些特征作为输入，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，**参数又可被称为权重(weight)**。
  ![此处输入图片的描述][2]

上图中输入是有4个特征，包括人工加入的$x_0=1$,以及$x_1,x_2,x_3$,其输出就是假设$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$,也就是之前逻辑回归中的假设，而黑色的直线表示的就是每种特征的权重值。此外，$x_0$也被称为**偏置单元(bias  unit)**，中间红色的圈表示的就是使用的激活函数，这里是使用S形函数，也就是$g(z) =\frac{1}{1+e^{-z}}$。

由上图还可以知道，其实**神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。**

下图是一个3层的神经网络，第一层是**输入层**，最后一层是**输出层**，中间一层是**隐藏层**。我们为每一层都增加一个**偏置单元**。
![此处输入图片的描述][3]

下面引入一些标记来帮助描述模型：

* $a_i^{(j)}$ 代表第$j$层的第$i$个激活单元
* $\theta^{(j)}$ 代表从第$j$层映射到第$j+1$层时的权重的矩阵，例如$\theta^{(1)}$代表从第一层到第二层的权重的矩阵，其尺寸为：**以第$j$层的激活单元数量为行数，第$j+1$层的激活单元数为列数的矩阵，即$S_{j+1}*(S_j+1)$。所以如上图的神经网站中$\theta^{(1)}$的尺寸是3*4。

对于上图所示的模型，激活单元和输出分别表达为：
$$
a_1^{(2)} = g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3) \\
a_2^{(2)} = g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3) \\
a_3^{(2)} = g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3) \\
h_\theta(x) = a_1^{(3)}= g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)}) \\
$$

### 正向传播
  相对于使用循环来编码，利用**向量化**的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值：
  ![此处输入图片的描述][4]

我们令$z^{(2)} = \theta^{(1)}x$,则$a^{(2)} = g(z^{(2)})$,计算后添加$a_0^{(2)}=1$,计算输出的值：
![此处输入图片的描述][5]

令$z^{(3)} = \theta^{(2)}a^{(2)}$,则$h_\theta(x)=a^{(3)} = g(z^{(3)})$.

> 前向传播是一个从输入层到隐藏层再到输出层依次计算激励，即激活函数a的过程。

### 对神经网络的理解
  本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征$x_1,x_2,\ldots,x_n$,我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。而在神经网络中，原始特征只是输入层，在上面三层的神经网络例子中，第三层也就是输出层所做出的预测是利用第二层的特征，而非输入层的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。
  
### 神经网络示例：二元逻辑运算符
  当输入特征是布尔值(0或1)时，我们可以用一个单一的激活层作为二元逻辑运算符，**为了表示不同的运算符，我们只需要选择不同的权重即可。**
  
  下图的神经元(三个权重分别为-30,20,20)可以被视为作用同于逻辑**与(AND)**:
  ![此处输入图片的描述][6]
  其表达式是$h_\theta(x)=g(-30+20x_1+20x_2)$,其可能的输出如下图所示：
  ![此处输入图片的描述][7]
  
  同理，下面的神经元，三个权重分别是-10,20,20，可以被视为作用等同于逻辑**或(OR)**
  ![此处输入图片的描述][8]

  下面的神经元，两个权重分别为-10,20，可以视作逻辑**非(NOT)**
  ![此处输入图片的描述][9]
  
  上面是一些基本的逻辑运算符，我们还可以用神经元来组成更为复杂的神经网络以实现更复杂的运算。例如实现**XNOR**，即异或，即只有输入的两个值都相同，均为1或0时，输出才是1，也就是$XNOR=(x_1\ AND\ x_2)OR\ ((NOT\ x_1)\ AND\ (NOT\ x_2))$
  
其实现如下所示：
首先分别列出三个部分的神经元的实现，
![此处输入图片的描述][10]

然后组合起来就得到最终的结果：
![此处输入图片的描述][11]

我们就实现了一个XNOR运算符功能的神经网络。

### 多类分类
  假如我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，那么在输出层我们应该有4个值，例如，第一个值为1或0用于预测是否是行人，第二个值用来判断是否为汽车。
  下面是该神经网络的可能结构示例：
  ![此处输入图片的描述][12]
  
  那么神经网络算法的输出结果是下列四种可能情形之一：
$$
\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix},
$$

### 小结
  本节课是介绍了神经网络的基础知识，包括产生的背景，模型表达以及正向传播的内容。暂时还没有涉及到更深层次的知识。



  [1]: http://img.blog.csdn.net/20160618184818760
  [2]: http://img.blog.csdn.net/20160618184846167
  [3]: http://img.blog.csdn.net/20160618191115979
  [4]: http://img.blog.csdn.net/20160618193608173
  [5]: http://img.blog.csdn.net/20160618193617845
  [6]: http://img.blog.csdn.net/20160618195541321
  [7]: http://img.blog.csdn.net/20160618195801528
  [8]: http://img.blog.csdn.net/20160618195813635
  [9]: http://img.blog.csdn.net/20160618195822103
  [10]: http://img.blog.csdn.net/20160618200935153
  [11]: http://img.blog.csdn.net/20160618201232876
  [12]: http://img.blog.csdn.net/20160618202439174