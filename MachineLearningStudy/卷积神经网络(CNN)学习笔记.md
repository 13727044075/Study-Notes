# 卷积神经网络(CNN)学习笔记  

标签（空格分隔）： 深度学习 CNN

---

记录在学习CNN过程中的一些知识点，包括参考的文章，论文或者博客等。

参考文章：
1. [An Intuitive Explanation of Convolutional Neural Networks][1]


CNN可以应用在场景分类，图像分类，现在还可以应用到自然语言处理(NLP)方面的很多问题，比如句子分类等。

**LeNet**是最早的CNN结构之一，它是由大神**Yann LeCun**所创造的，主要是用在字符分类问题。

下面是一个简单的CNN结构，图来自参考文章[1]。这个网络结构是用于一个四类分类的问题，分别是狗、猫、船和鸟，图中的输入图片是属于船一类。

![此处输入图片的描述][2]

该结构展示了四种运算，也可以说是由四种不同的层，分别是卷积层，非线性层(也就是使用了ReLU函数)，Pooling层，全连接层，下面将一一介绍这几种网络层。

------

### 卷积层
#### 卷积简介

  CNN的名字由来就是因为其使用了卷积运算的缘故。卷积的目的主要是为了提取图片的特征。卷积运算可以保持像素之间的空间关系。

  每张图片可以当做是一个包含每个像素值的矩阵，像素值的范围是0~255,0表示黑色，255是白色。下面是一个$5 \times 5$大小的矩阵例子，它的值是0或者1。

  ![此处输入图片的描述][3]

  接下来是另一个$3\times 3$矩阵：

  ![此处输入图片的描述][4]

  上述两个矩阵通过卷积，可以得到如下图右侧粉色的矩阵结果。

  ![此处输入图片的描述][5]

  黄色的矩阵在绿色的矩阵上从左到右，从上到下，每次滑动的步进值是1个像素，所以得到一个$3\times 3$的矩阵。

  在CNN中，黄色的矩阵被叫做**滤波器(filter)或者核(kernel)或者是特征提取器**，而通过卷积得到的矩阵则是称为**“特征图(Feature Map)”或者“Activation Map”**。

  另外，**使用不同的滤波器矩阵是可以得到不同的 Feature Map** ，例子如下图所示：

  ![此处输入图片的描述][6]

  上图通过滤波器矩阵，实现了不同的操作，比如边缘检测，锐化以及模糊操作等。

  在实际应用中，CNN是可以在其训练过程中学习到这些滤波器的值，不过我们需要首先指定好滤波器的大小，数量以及网络的结构。使用越多的滤波器，可以提取到更多的图像特征，网络也就能够有更好的性能。

  Feature Map的尺寸是由以下三个参数来决定的：

* **深度(Depth)**： **深度等于滤波器的数量**。
* **步进(Stride)**: 步进值是在使用滤波器在输入矩阵上滑动的时候，每次滑动的距离。步进值越大，得到的Feature Map的尺寸越小。
* **Zero-padding**: 有时候可以在输入矩阵的边界填补0，这样就可以将滤波器应用到边缘的像素点上，一个好的Zero-padding是能让我们可以控制好特征图的尺寸的。使用该方法的卷积称为**wide convolution**，没有使用的则是**narrow convolution**。



#### 卷积公式和参数量

 上一小节简单介绍了卷积的操作和其实现的效果，接下来将介绍卷积运算的公式，以及CNN中卷积层的参数数量。

 卷积是大自然中最常见的运算，一切信号观测、采集、传输和处理都可以用卷积过程实现，其用公式表达如下：
$$
\begin{align}
Y(m,n) & =X(m,n)*H(m,n) \\ 
&= \sum_{i=-\infty}^{+\infty}\sum_{j=-\infty}^{+\infty}X(i,j)H(m-i,n-j) \\ &=\sum_{i=-\infty}^{+\infty}\sum_{j=-\infty}^{+\infty}X(m-i,n-j)H(i,j)
\end{align}
$$
上述公式中$H(m,n)$表示卷积核。

在CNN中的卷积层的计算步骤与上述公式定义的二维卷积有点差异，首先是维度升至三维、四维卷积，跟二维卷积相比多了一个**“通道”(channel)**，每个通道还是按照二维卷积方式计算，而多个通道与多个卷积核分别进行二维卷积，得到多通道输出，需要“合并”为一个通道；**其次是卷积核在卷积计算时没有“翻转”，而是与输入图片做滑动窗口“相关”计算**。用公式重新表达如下：
$$
Y^l(m,n) =X^k(m,n)*H^{kl}(m,n) = \sum_{k=0}^{K-1}\sum_{i=0}^{I-1}\sum_{j=0}^{J-1}X^k(m+i,n+j)H^{kl}(i,j)
$$
这里假定卷积层有$L$个输出通道和$K$个输入通道，于是需要有$KL$个卷积核实现通道数目的转换。其中$X^k$表示第$k$个输入通道的二维特征图，$Y^l$表示第$l$个输出通道的二维特征图，$H^{kl}$表示第$k$行、第$l$列二维卷积核。假定卷积核大小是$I*J$,每个输出通道的特征图大小是$M*N$，则该层每个样本做一次前向传播时卷积层的计算量是$Calculations(MAC)=I*J*M*N*K*L$。

卷积层的学习参数，也就是**卷积核数目乘以卷积核的尺寸--$Params = I*J*K*L$。**

这里定义计算量-参数量之比是**CPR**=$Calculations/Params=M*N$。

因此可以得出结论：**卷积层的输出特征图尺寸越大，CPR越大，参数重复利用率越高。若输入一批大小为B的样本，则CPR值可提高B倍。**

------

### 非线性层(ReLU)
  非线性修正函数**ReLU(Rectified Linear Unit)**如下图所示：

  ![此处输入图片的描述][7]

这是一个对每个像素点实现点乘运算，并用0来替换负值像素点。其目的是在CNN中加入非线性，**因为使用CNN来解决的现实世界的问题都是非线性的，而卷积运算是线性运算，所以必须使用一个如ReLU的非线性函数来加入非线性的性质。**

其他非线性函数还包括**tanh**和**Sigmoid**,但是**ReLU**函数已经被证明在大部分情况下性能最好。

### Pooling层
  **空间合并（Spatial Pooling)**也可以叫做子采样或者下采样，可以在保持最重要的信息的同时降低特征图的维度。它有不同的类型，如最大化，平均，求和等等。

  对于**Max Pooling**操作，首先定义一个空间上的邻居，比如一个$2\times 2$的窗口，对该窗口内的经过ReLU的特征图提取最大的元素。除了提取最大的元素，还可以使用窗口内元素的平均值或者是求和的值。不过，**Max Pooling**的性能是最好的。例子可以如下图所示：

  ![此处输入图片的描述][8]

  上图中使用的步进值是2。

  使用Pooling的原因有如下几点：

* 可以让输入的表示，也就是特征维度变得更小和更加可控制；
* 减少网络的参数和计算量，从而控制过拟合；
* 使得网络对于细小的变化、畸变等具有不变性；
* 能实现对图片是近似于尺度不变性的，准确的词语是**等变化的(equivariant)**。

------

### 全连接层
  全连接层就是一个传统的多层感知器，它在输出层使用一个**softmax**激活函数。其主要作用就是将前面卷积层提取到的特征结合在一起然后进行分类。**Softmax**函数可以将输入是一个任意实数分数的向量变成一个值的范围是0~1的向量，但所有值的总和是1。

  在CNN出现之前，最早的深度学习网络计算类型都是全连接形式的。

  全连接层的主要计算类型是**矩阵-向量乘（GEMV)。**假设输入节点组成的向量是$x$，维度是$D$,输出节点组成的向量是$y$,维度是$V$,则全连接层计算可以表示为$y=Wx$。

  其中$W$是$V*D$的权值矩阵。

  全连接层的参数量为$Params=V*D$,其单个样本前向传播的计算量也是$Calculations(MAC)=V*D$，也就是$CPR=Calculations/Params=1$。也就是其权值利用率很低。

  可以将一批大小为$B$的样本$x_i$逐列拼接成矩阵$X$，一次性通过全连接层，得到一批输出向量构成的矩阵$Y$，相应地前面的矩阵-向量乘运算升为**矩阵-矩阵乘计算（GEMM)：$Y=WX$**。

  这样全连接层前向计算量提高了$B$倍，CPR相应提高了$B$倍，权重矩阵在多个样本之间实现了共享，可提高计算速度。

  比较卷积层和全连接层，卷积层在输出特征图维度实现了**权值共享**，这是降低参数量的重要举措，同时，卷积层**局部连接**特性（相比全连接）也大幅减少了参数量。**因此卷积层参数量占比小，但计算量占比大，而全连接层是参数量占比大，计算量占比小。所以在进行计算加速优化时，重点放在卷积层；在进行参数优化、权值剪裁时，重点放在全连接层。**

------

### 反向传播Backpropagation
  CNN的整个训练过程如下所示：

1. 首先是随机初始化所有滤波器以及其他参数和权重值；
2. 输入图片，进行前向传播，也就是经过卷积层，ReLU和pooling运算，最后到达全连接层进行分类，得到一个分类的结果，也就是输出一个包含每个类预测的概率值的向量；
3. 计算误差，也就是代价函数，这里代价函数可以有多种计算方法，比较常用的有平方和函数，即$Error = \frac{1}{2}\sum(实际值-预测值)^2$；
4. 使用反向传播来计算网络中对应各个权重的误差的梯度，一般是使用梯度下降法来更新各个滤波器的权重值，目的是为了让输出的误差，也就是代价函数的值尽可能小。
5. 重复上述第二到第四步，直到训练次数达到设定好的值。

------

### 网络结构
  这里介绍比较有名的网络结构。

1. **LeNet（1990s）**：在开头介绍了，这是最早使用的CNN网络结构之一，主要是用于字符分类；
2. **AlexNet（2012）**：这是在2012年的ImageNet视觉挑战比赛上获得第一名所使用的网络结构，这也是使得许多视觉问题取得重大突破，让CNN变得非常热门的原因。
3. **ZF Net（2013）**：这是2013年ImageNet比赛的胜者，对AlexNet的结构超参数做出了调整。
4. **GoogleNet（2014）**:2014年ImageNet比赛的胜者，其主要贡献是使用了一个**Inception Module**，可以大幅度减少网络的参数数量，其参数数量是4M，而AlexNet的则有60M。
5. **VGGNet（2014）**：是一个更深的网络，使用了16层的结构。
6. **ResNets（2015）**：目前CNN中最好性能的结构，使用了152层。



​    


[1]: https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
[2]: http://7xrluf.com1.z0.glb.clouddn.com/CNN1.png
[3]: http://7xrluf.com1.z0.glb.clouddn.com/CNN2.png
[4]: http://7xrluf.com1.z0.glb.clouddn.com/CNN3.png
[5]: http://7xrluf.com1.z0.glb.clouddn.com/CNN4.png
[6]: http://7xrluf.com1.z0.glb.clouddn.com/CNN5.png
[7]: http://7xrluf.com1.z0.glb.clouddn.com/CNN6.png
[8]: http://7xrluf.com1.z0.glb.clouddn.com/CNN7.png