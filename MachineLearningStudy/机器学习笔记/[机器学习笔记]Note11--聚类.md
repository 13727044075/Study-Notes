# [机器学习笔记]Note11--聚类

标签（空格分隔）： 机器学习

---
[TOC]

继续是[机器学习课程](https://www.coursera.org/learn/machine-learning)的笔记，本节介绍的是聚类方法，主要是K-means算法。

### 非监督学习
  首先介绍监督学习和非监督学习的区别。一个监督学习的例子如下图，给定训练集如:$\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\ldots,(x^{(m)},y^{(m)})\}$,每个训练实例都有对应的标签。

![此处输入图片的描述][1]

而对于非监督学习，其例子如下图所示，给定训练集是如：$\{ x^{(1)},x^{(2)},\ldots,x^{(m)}\}$,没有标签，一般可以使用聚类的方法将训练集的数据进行划分。

![此处输入图片的描述][2]

### K-均值算法(K-means)
  K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据集聚类成不同的组。
  
  K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为：

1. 首先选择**K**个随机的点，称其为**聚类中心**
2. 对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一个类
3. 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置
4. 重复步骤2-3，直到中心点不再变化

下面是一个聚类示例，这是迭代1次的状态，可以看出初始有3群数据，初始聚类中心有3个。如下所示

![此处输入图片的描述][3]

然后是迭代3次时候的情况，如下所示，初始聚类中心开始移动。

![此处输入图片的描述][4]

这是迭代10次后，初始聚类中心已经分别移动到3群数据的中心位置。

![此处输入图片的描述][5]

这里用$\mu_1,\mu_2,\ldots,\mu_m$来表示聚类中心，用$c^{(1)},c^{(2)},\ldots,c^{(m)}$来存储与第i个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下所示：
>  Repeat{
    for i = 1 to m
        $c^{(i)}$ := index (from 1 to K) of cluster centroid closed to $x{(i)}$
    for k = 1 to K
        $\mu_k$:= average (mean) of points assigned to cluster K
}

这个算法分为两个步骤，第一个for循环是计算数据集每个数据与聚类中心的距离，然后分别将最近的聚类中心的索引赋值到对应的$c^{(i)}$,而第二个for循环就是移动聚类中心到其类平均值处。

当然在这个过程中可能遇到有聚类中心是没有分配数据点给它的，通常的一个做法是删除这种聚类中心，或者是重新选择聚类中心，保证聚类中心数还是初始设定的K个。

K-均值算法是可以很方便将数据分为不同组的，即使是在没有非常明显区分的组群的情况下也可以。如下图是一个包含身高和体重两个特征的数据集，可以利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤的三种尺寸。

![此处输入图片的描述][6]

### 优化目标
  K-均值最小化问题，就是**最小化所有的数据点与其所关联的聚类中心之间的距离之和**，因此K-均值的代价函数（又称为**畸变函数**）为：
$$
J(c^{(1)},c^{(2)},\ldots,c^{(m)},\mu_1,\mu_2,\ldots,\mu_m)=\frac{1}{m}\sum_{i=1}^m ||x^{(i)}-\mu_{c^{(i)}}||^2
$$
其中$\mu_{c^{(i)}}$代表与$x^{(i)}$最近的聚类中心点。

所以我们的优化目标是找出是的代价函数最小的$c^{(1)},c^{(2)},\ldots,c^{(m)}和\mu_1,\mu_2,\ldots,\mu_m$:
$$
min_{c^{(1)},c^{(2)},\ldots,c^{(m)}, \mu_1,\mu_2,\ldots,\mu_m}J(c^{(1)},c^{(2)},\ldots,c^{(m)},\mu_1,\mu_2,\ldots,\mu_m)
$$

回顾上一小节中K-均值迭代算法的伪代码可知，第一个循环就是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小$\mu_i$引起的代价，因此，**迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。**

### 随机初始化
  在运行K-均值算法之前，首先需要随机初始化所有的聚类中心点，做法如下：
  
1. 首先应该选择$K \lt m$,即聚类中心点的个数要小于所有训练集实例的数量
2. 随机选择K个训练实例，然后令K个聚类中心分别于这K个训练实例相等

K-均值的一个问题在于，**它有可能会停留在一个局部最小值处，而这取决于初始化的情况。**

为了解决这个问题，通常需要**多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。**这种方法在**K较小（2-10）**的时候还是可行的，但是如果K较大，这种做法可能不会有明显地改善。

### 选择聚类数
  没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目标的聚类数。
  
  例如，在之前给出的T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸S,M,L，也可以分成5个尺寸XS,S,M,L,XL，这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上做出的。
  
### 小结
  本节课介绍了非监督学习中的聚类算法的一种最常用的K-均值算法，这种算法是比较简单，也比较常用。它的做法是先随机选择K个聚类中心，然后计算每个数据点到这K个聚类中心的距离，然后划分数据点到其最近的聚类中心，并视为一类，然后计算每类的平均值，再将聚类中心移动到这个类平均值处，然后再重复这几个步骤，直到聚类中心不再移动。
  
  该算法的不足就是聚类中心数目的选择是需要多次实验才能得到最佳的，同时对于初始的聚类中心数，一般是使用随机初始化，所以可能导致算法落入局部最优，在K较小的情况可以多次运行算法，选择代价函数最小的一个。
  
  


  [1]: http://img.blog.csdn.net/20160722150827407
  [2]: http://img.blog.csdn.net/20160722150959249
  [3]: http://img.blog.csdn.net/20160722152514195
  [4]: http://img.blog.csdn.net/20160722152624849
  [5]: http://img.blog.csdn.net/20160722152741507
  [6]: http://img.blog.csdn.net/20160722154226042