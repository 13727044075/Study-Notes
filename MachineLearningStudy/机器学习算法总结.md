### 机器学习算法总结

------

#### 1. 线性回归

##### 简述

在统计学中，**线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析**。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。

优点：结果易于理解，计算上不复杂。
缺点：对非线性数据拟合不好。
适用数据类型：数值型和标称型数据。
算法类型：回归算法

线性回归的模型函数如下：
$$
h_\theta = \theta ^T x
$$

它的损失函数如下：
$$
J(\theta) = {1\over {2m}} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$
通过训练数据集寻找参数的最优解，即求解可以得到$minJ(\theta)$的参数向量$\theta$,其中这里的参数向量也可以分为参数$w和b$,分别表示权重和偏置值。

求解最优解的方法有**最小二乘法和梯度下降法**。

##### 梯度下降法

梯度下降算法的思想如下(这里以一元线性回归为例)：

> 首先，我们有一个代价函数，假设是$J(\theta_0,\theta_1)$，我们的目标是$min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$。
> 接下来的做法是：
>
> - 首先是随机选择一个参数的组合$(\theta_0,\theta_1)$,一般是设$\theta_0 = 0,\theta_1 = 0$;
> - 然后是不断改变$(\theta_0,\theta_1)$，并计算代价函数，直到一个**局部最小值**。之所以是**局部最小值**，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是**全局最小值**，选择不同的初始参数组合，可能会找到不同的局部最小值。
>   下面给出梯度下降算法的公式：

> repeat until convergence{
$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)\quad (for\quad j=0 \quad and\quad j=1)
$$
> } 

也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到$\color{red}{局部最小值}$。其中符号`:=`是赋值符号的意思。

而应用梯度下降法到线性回归，则公式如下：
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})\ \\
 \theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^m ((h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)})
$$

公式中的$\alpha$称为**学习率(learning rate)**，它决定了我们沿着能让代价函数下降程度最大的方向向下迈进的步子有多大。

在梯度下降中，还涉及都一个参数更新的问题，即更新$(\theta_0,\theta_1)$，一般我们的做法是**同步更新。**

最后，上述梯度下降算法公式实际上是一个叫**批量梯度下降(batch gradient descent)**，即它在每次梯度下降中都是使用整个训练集的数据，所以公式中是带有$\sum_{i=1}^m$。

##### 岭回归（ridge regression）:

岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。

岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数K（1>K>0），并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。

其他回归还可以参考[这篇文章](http://blog.csdn.net/ownfed/article/details/41181665)。

##### 代码实现

Python实现的代码如下：

```python
#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import linear_model
#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays

x_train=input_variables_values_training_datasets
y_train=target_variables_values_training_datasets
x_test=input_variables_values_test_datasets

# Create linear regression object
linear = linear_model.LinearRegression()

# Train the model using the training sets and check score
linear.fit(x_train, y_train)
linear.score(x_train, y_train)

#Equation coefficient and Intercept
print('Coefficient: \n', linear.coef_)
print('Intercept: \n', linear.intercept_)

#Predict Output
predicted= linear.predict(x_test)
```

上述是使用`sklearn`包中的线性回归算法的代码例子，下面是一个实现的具体例子。

```python
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 17 10:36:06 2016

@author: cai
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
from sklearn import linear_model

# 计算损失函数
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))

# 梯度下降算法
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)

    for i in range(iters):
        error = (X * theta.T) - y

        for j in range(parameters):
            # 计算误差对权值的偏导数
            term = np.multiply(error, X[:, j])
            # 更新权值
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))

        theta = temp
        cost[i] = computeCost(X, y, theta)
    return theta, cost

dataPath = os.path.join('data', 'ex1data1.txt')
data = pd.read_csv(dataPath, header=None, names=['Population', 'Profit'])
# print(data.head())
# print(data.describe())
# data.plot(kind='scatter', x='Population', y='Profit', figsize=(12, 8))
# 在数据起始位置添加1列数值为1的数据
data.insert(0, 'Ones', 1)
print(data.shape)

cols = data.shape[1]
X = data.iloc[:, 0:cols-1]
y = data.iloc[:, cols-1:cols]

# 从数据帧转换成numpy的矩阵格式
X = np.matrix(X.values)
y = np.matrix(y.values)
# theta = np.matrix(np.array([0, 0]))
theta = np.matrix(np.zeros((1, cols-1)))
print(theta)
print(X.shape, theta.shape, y.shape)
cost = computeCost(X, y, theta)
print("cost = ", cost)

# 初始化学习率和迭代次数
alpha = 0.01
iters = 1000

# 执行梯度下降算法
g, cost = gradientDescent(X, y, theta, alpha, iters)
print(g)

# 可视化结果
x = np.linspace(data.Population.min(),data.Population.max(),100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Training Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')

fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('Iteration')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')


# 使用sklearn 包里面实现的线性回归算法
model = linear_model.LinearRegression()
model.fit(X, y)

x = np.array(X[:, 1].A1)
# 预测结果
f = model.predict(X).flatten()
# 可视化
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Training Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size(using sklearn)')
plt.show()
```

上述代码参考自[Part 1 - Simple Linear Regression](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/)。具体可以查看[我的Github](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/linearRegressionPractise.py)。

#### 2. 逻辑回归

##### 简述

Logistic回归算法基于Sigmoid函数，或者说Sigmoid就是逻辑回归函数。Sigmoid函数定义如下：
$\frac{1}{1+e^{-z}}$。函数值域范围(0,1)。

因此逻辑回归函数的表达式如下：
$$
h_\theta(x) =g(\theta^T X) = \frac{1}{1+e^{-\theta^TX}} \\
其中，g(z) = \frac{1}{1+e^{-z}}
$$
其导数形式为：
$$
g\prime (z)  =  \frac{d}{dz} \frac{1}{1+e^{-z}} \\
		 = \frac{1}{(1+e^{-z})^2} (e^{-z}) \\
		 =  \frac{1}{1+e^{-z}} (1-  \frac{1}{1+e^{-z}}) \\
		 = g(z)(1-g(z))
$$

##### 代价函数

逻辑回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：
$$
p(y | x; \theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}
$$
到整个样本的后验概率就是:
$$
L(\theta) = p(y | X;\theta) \\
	      = \prod_{i=1}^{m} p(y^{(i)} | x^{(i)};\theta)\\
	      = \prod_{i=1}^{m} (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
$$
其中，$P(y=1|x;\theta) = h_\theta(x), P(y=0|x;\theta)=1-h_\theta(x)$。

通过对数进一步简化有：$l(\theta) = logL(\theta) = \sum_{i=1}^{m}y^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))$.

而逻辑回归的代价函数就是$-l(\theta)$。也就是如下所示：
$$
J(\theta) = \frac{1}{m} [\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)}log(1-h_\theta(x^{(i)}))]
$$

同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为：

![这里写图片描述](http://img.blog.csdn.net/20170212181541232?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170212181600234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##### 总结

**优点**：

　　1、实现简单；

　　2、分类时计算量非常小，速度很快，存储资源低；

**缺点：**

　　1、容易欠拟合，一般准确度不太高

　　2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；

适用数据类型：数值型和标称型数据。
类别：分类算法。
试用场景：解决二分类问题。

##### 代码实现

首先是采用`sklearn`包中的逻辑回归算法代码：

```python
#Import Library
from sklearn.linear_model import LogisticRegression
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create logistic regression object

model = LogisticRegression()

# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)

#Equation coefficient and Intercept
print('Coefficient: \n', model.coef_)
print('Intercept: \n', model.intercept_)

#Predict Output
predicted= model.predict(x_test)
```

接下来则是应用例子，如下所示：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2016/10/19 21:35
@Author  : cai

实现多类的逻辑回归算法
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
from scipy.optimize import minimize
from scipy.io import loadmat

# 定义Sigmoid函数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 定义 cost函数
def costReg(theta, X, y, lambdas):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    h = X * theta.T
    first = np.multiply(-y, np.log(sigmoid(h)))
    second = np.multiply((1-y), np.log(1 - sigmoid(h)))
    reg = (lambdas / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))
    return np.sum(first - second) / (len(X)) + reg

# 梯度下降算法的实现, 输出梯度对权值的偏导数
def gradient(theta, X, y, lambdas):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)
    # 计算误差
    error = sigmoid(X * theta.T) - y

    grad = ((X.T * error) / len(X)).T + ((lambdas / len(X)) * theta)

    grad[0, 0] = np.sum(np.multiply(error, X[:, 0])) / len(X)

    return np.array(grad).ravel()

# 实现一对多的分类方法
def one_vs_all(X, y, num_labels, lambdas):
    rows = X.shape[0]
    params = X.shape[1]

    # 每个分类器有一个 k * (n+1)大小的权值数组
    all_theta = np.zeros((num_labels, params + 1))

    # 增加一列，这是用于偏置值
    X = np.insert(X, 0, values=np.ones(rows), axis=1)

    # 标签的索引从1开始
    for i in range(1, num_labels + 1):
        theta = np.zeros(params + 1)
        y_i = np.array([1 if label == i else 0 for label in y])
        y_i = np.reshape(y_i, (rows, 1))

        # 最小化损失函数
        fmin = minimize(fun=costReg, x0=theta, args=(X, y_i, lambdas), method='TNC', jac=gradient)
        all_theta[i-1, :] = fmin.x

    return all_theta

def predict_all(X, all_theta):
    rows = X.shape[0]
    params = X.shape[1]
    num_labels = all_theta.shape[0]

    # 增加一列，这是用于偏置值
    X = np.insert(X, 0, values=np.ones(rows), axis=1)

    X = np.matrix(X)
    all_theta = np.matrix(all_theta)

    # 对每个训练样本计算其类的概率值
    h = sigmoid(X * all_theta.T)

    # 获取最大概率值的数组索引
    h_argmax = np.argmax(h, axis=1)
    # 数组是从0开始索引，而标签值是从1开始，所以需要加1
    h_argmax = h_argmax + 1

    return h_argmax

dataPath = os.path.join('data', 'ex3data1.mat')
# 载入数据
data = loadmat(dataPath)
print(data)
print(data['X'].shape, data['y'].shape)

# print(np.unique(data['y']))
# 测试
# rows = data['X'].shape[0]
# params = data['X'].shape[1]
#
# all_theta = np.zeros((10, params + 1))
#
# X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)
#
# theta = np.zeros(params + 1)
#
# y_0 = np.array([1 if label == 0 else 0 for label in data['y']])
# y_0 = np.reshape(y_0, (rows, 1))
# print(X.shape, y_0.shape, theta.shape, all_theta.shape)

all_theta = one_vs_all(data['X'], data['y'], 10, 1)
print(all_theta)

# 计算分类准确率
y_pred = predict_all(data['X'], all_theta)
correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]
accuracy = (sum(map(int, correct)) / float(len(correct)))
print('accuracy = {0}%'.format(accuracy * 100))
```

实现代码来自[Part 4 - Multivariate Logistic Regression](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/)。具体可以查看[我的github](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/mulLogisticRegressionPractise.py)。

#### 3 决策树

##### 简介

**定义**：分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。

决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪。**

决策树学习本质上是从训练数据集中归纳出一组分类规则，也可以说是**由训练数据集估计条件概率模型**。它使用的损失函数通常是**正则化的极大似然函数**，其策略是以损失函数为目标函数的最小化。

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。

决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。

##### 特征选择

特征选择的准则通常是信息增益或者信息增益比。

首先是给出信息熵的计算公式$H(p) = -\sum_{i=1}^{n} p_i log p_i$，熵越大，随机变量的不确定性就越大。公式中$p_i$表示随机变量X属于类别$i$的概率，因此$n$表示类别的总数。

条件熵的定义为：$H(Y|X) = \sum_{i=1}^n p_iH(Y|X=x_i)$

已经有了**熵作为衡量训练样例集合纯度**的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为“**信息增益（information gain）**”。简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低(或者说，**样本按照某属性划分时造成熵减少的期望,个人结合前面理解，总结为用来衡量给定的属性区分训练样例的能力**)。更精确地讲，**一个属性A相对样例集合S的信息增益Gain(S,A)被定义为**：

![这里写图片描述](http://img.blog.csdn.net/20170213171939623?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中 Values(A)是属性A所有可能值的集合，Sv是S中属性A的值为v的子集，注意上式第一项就是原集合S的熵，第二项是用A分类S后的熵的期望值，第二项描述的期望熵就是每个子集的熵的加权和，权值为属性Sv的样例占原始样例S的比例|Sv|/|S|,所以Gain(S,A)是由于知道属性A的值而导致的期望熵减少，换句话来讲，Gain(S,A)是由于给定属性A的值而得到的关于目标函数值的信息。

信息增益的缺点是**存在偏向于选择取值较多的特征的问题**。为了解决这个问题，可以使用**信息增益比**。

因此，特征A对训练数据集D的信息增益比$g_R(D,A)$的定义如下：
$$
g_R(D, A) = \frac{g(D,A)}{H_A(D)}
$$
其中$g(D,A)$是信息增益，而$H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}$,其中$n$是特征A取值的个数。

不过对于信息增益比，其也存在**对可取值数目较少的属性有所偏好的问题**。

##### 决策树的生成

接下来会介绍决策树生成的算法，包括**ID3, C4.5**算法。

###### ID3算法

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体步骤如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170213204003966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

ID3的算法思路总结如下：

1. 首先是针对当前的集合，计算每个特征的信息增益
2. 然后选择信息增益最大的特征作为当前节点的决策决策特征
3. 根据特征不同的类别划分到不同的子节点（比如年龄特征有青年，中年，老年，则划分到3颗子树）
4. 然后继续对子节点进行递归，直到所有特征都被划分

###### C4.5算法

C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：

- 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
-  在树构造过程中进行剪枝；
-  能够完成对连续属性的离散化处理；
-  能够对不完整数据进行处理。

C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。

其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

算法的实现过程如下:

![这里写图片描述](http://img.blog.csdn.net/20170213204817800?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**实际上由于信息增益比的缺点，C4.5算法并没有直接选择信息增益比最大的候选划分属性，而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最高的。**

##### 剪枝

在生成树的过程中，如果没有剪枝的操作的话，就会长成每一个叶都是单独的一类的样子。这样对我们的训练集是完全拟合的，但是对测试集则是非常不友好的，泛化能力不行。**因此，我们要减掉一些枝叶，使得模型泛化能力更强。** 
根据剪枝所出现的时间点不同，分为预剪枝和后剪枝。**预剪枝是在决策树的生成过程中进行的；后剪枝是在决策树生成之后进行的。**

决策树的剪枝往往是通过极小化决策树整体的损失函数或代价函数来实现的。简单来说，就是对比剪枝前后整体树的损失函数或者是准确率大小来判断是否需要进行剪枝。

决策树剪枝算法有多种，具体参考[决策树剪枝算法](http://blog.csdn.net/yujianmin1990/article/details/49864813)这篇文章。

##### CART

分类回归树(Classification And Regression Tree)是一个**决策二叉树**，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：

1. **分类树：基尼指数最小化(gini_index)**
2. **回归树：平方误差最小化**

分类树的生成步骤如下所示：
![这里写图片描述](http://img.blog.csdn.net/20170213212619889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

简单总结如下：

1. 首先是根据当前特征计算他们的基尼增益
2. 选择**基尼增益最小**的特征作为划分特征
3. 从该特征中查找基尼指数最小的分类类别作为最优划分点
4. 将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于
5. 针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值

基尼指数的计算公式为$Gini(p) = 1 - \sum_{k=1}^K p_k^2$。K是类别的数目，$p_k$表示样本属于第k类的概率值。它可以用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。

给定一个数据集D，在特征A的条件下，其基尼指数定义为$Gini(D,A) = \sum_{i=1}^n \frac{|D_i|}{|D|} Gini(D_i)$。

回归树：

> 回归树是以平方误差最小化的准则划分为两块区域

1. 遍历特征计算最优的划分点s，
   使其最小化的平方误差是：$min \{min(∑_i^{R1}((y_i−c_1)^2))+min(∑_i^{R2}((y_i−c_2)^2))\}$
   计算根据s划分到左侧和右侧子树的目标值与预测值之差的平方和最小，这里的预测值是两个子树上输入$x_i$样本对应$y_i$的均值

2. 找到最小的划分特征j以及其最优的划分点s,根据特征j以及划分点s将现有的样本划分为两个区域，一个是在特征j上小于等于s，另一个在在特征j上大于s
   $$
   R1(j)= \{x|x(j)≤s\} \\
   R2(j)=\{x|x(j)>s\}
   $$

3. 进入两个子区域按上述方法继续划分，直到到达停止条件

关于CART剪枝的方法可以参考[决策树系列（五）——CART](http://www.cnblogs.com/yonghao/p/5135386.html)。

##### 停止条件

1. 直到每个叶子节点都只有一种类型的记录时停止，（这种方式很容易过拟合）
2. 另一种是当叶子节点的样本数目小于一定的阈值或者节点的信息增益小于一定的阈值时停止

##### 关于特征与目标值

1. 特征离散 目标值离散：可以使用ID3，cart
2. 特征连续 目标值离散：将连续的特征离散化 可以使用ID3，cart

##### 决策树的分类与回归

- 分类树
  输出叶子节点中所属类别最多的那一类
- 回归树
  输出叶子节点中各个样本值的平均值

##### 理想的决策树

1. 叶子节点数尽量少
2. 叶子节点的深度尽量小(太深可能会过拟合)

##### 解决决策树的过拟合

1. 剪枝
   1. 前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）
   2. 后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）
2. 交叉验证
3. 随机森林

##### 优缺点

优点：

1. 计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；

缺点：

2. 单颗决策树分类能力弱，并且对连续值变量难以处理；

3. 容易过拟合（后续出现了随机森林，减小了过拟合现象）；

##### 代码实现

使用sklearn中决策树函数的简单代码例子如下所示：

```python
#Import Library
#Import other necessary libraries like pandas, numpy...

from sklearn import tree
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create tree object 
model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  

# model = tree.DecisionTreeRegressor() for regression

# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)

#Predict Output
predicted= model.predict(x_test)
```

决策树的代码在开源库OpenCV中有实现，具体的源码分析可以参考[Opencv2.4.9源码分析——Decision Trees](http://blog.csdn.net/zhaocj/article/details/50503450)，这篇文章也比较详细总结了决策树的知识点以及对OpenCV中决策树部分的源码进行了分析。

#### 4 SVM

