### 机器学习算法总结

------

#### 1. 线性回归

##### 简述

在统计学中，**线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析**。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。

优点：结果易于理解，计算上不复杂。
缺点：对非线性数据拟合不好。
适用数据类型：数值型和标称型数据。
算法类型：回归算法

线性回归的模型函数如下：
$$
h_\theta = \theta ^T x
$$

它的损失函数如下：
$$
J(\theta) = {1\over {2m}} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$
通过训练数据集寻找参数的最优解，即求解可以得到$minJ(\theta)$的参数向量$\theta$,其中这里的参数向量也可以分为参数$w和b$,分别表示权重和偏置值。

求解最优解的方法有**最小二乘法和梯度下降法**。

##### 梯度下降法

梯度下降算法的思想如下(这里以一元线性回归为例)：

> 首先，我们有一个代价函数，假设是$J(\theta_0,\theta_1)$，我们的目标是$min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$。
> 接下来的做法是：
>
> - 首先是随机选择一个参数的组合$(\theta_0,\theta_1)$,一般是设$\theta_0 = 0,\theta_1 = 0$;
> - 然后是不断改变$(\theta_0,\theta_1)$，并计算代价函数，直到一个**局部最小值**。之所以是**局部最小值**，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是**全局最小值**，选择不同的初始参数组合，可能会找到不同的局部最小值。
>   下面给出梯度下降算法的公式：

> repeat until convergence{
$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)\quad (for\quad j=0 \quad and\quad j=1)
$$
> } 

也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到$\color{red}{局部最小值}$。其中符号`:=`是赋值符号的意思。

而应用梯度下降法到线性回归，则公式如下：
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})\ \\
 \theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^m ((h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)})
$$

公式中的$\alpha$称为**学习率(learning rate)**，它决定了我们沿着能让代价函数下降程度最大的方向向下迈进的步子有多大。

在梯度下降中，还涉及到一个参数更新的问题，即更新$(\theta_0,\theta_1)$，一般我们的做法是**同步更新。**

最后，上述梯度下降算法公式实际上是一个叫**批量梯度下降(batch gradient descent)**，即它在每次梯度下降中都是使用整个训练集的数据，所以公式中是带有$\sum_{i=1}^m$。

##### 岭回归（ridge regression）:

岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。

岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数K（1>K>0），并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。

其他回归还可以参考[这篇文章](http://blog.csdn.net/ownfed/article/details/41181665)。

##### 代码实现

Python实现的代码如下：

```python
#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import linear_model
#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays

x_train=input_variables_values_training_datasets
y_train=target_variables_values_training_datasets
x_test=input_variables_values_test_datasets

# Create linear regression object
linear = linear_model.LinearRegression()

# Train the model using the training sets and check score
linear.fit(x_train, y_train)
linear.score(x_train, y_train)

#Equation coefficient and Intercept
print('Coefficient: \n', linear.coef_)
print('Intercept: \n', linear.intercept_)

#Predict Output
predicted= linear.predict(x_test)
```

上述是使用`sklearn`包中的线性回归算法的代码例子，下面是一个实现的具体例子。

```python
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 17 10:36:06 2016

@author: cai
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
from sklearn import linear_model

# 计算损失函数
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))

# 梯度下降算法
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)

    for i in range(iters):
        error = (X * theta.T) - y

        for j in range(parameters):
            # 计算误差对权值的偏导数
            term = np.multiply(error, X[:, j])
            # 更新权值
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))

        theta = temp
        cost[i] = computeCost(X, y, theta)
    return theta, cost

dataPath = os.path.join('data', 'ex1data1.txt')
data = pd.read_csv(dataPath, header=None, names=['Population', 'Profit'])
# print(data.head())
# print(data.describe())
# data.plot(kind='scatter', x='Population', y='Profit', figsize=(12, 8))
# 在数据起始位置添加1列数值为1的数据
data.insert(0, 'Ones', 1)
print(data.shape)

cols = data.shape[1]
X = data.iloc[:, 0:cols-1]
y = data.iloc[:, cols-1:cols]

# 从数据帧转换成numpy的矩阵格式
X = np.matrix(X.values)
y = np.matrix(y.values)
# theta = np.matrix(np.array([0, 0]))
theta = np.matrix(np.zeros((1, cols-1)))
print(theta)
print(X.shape, theta.shape, y.shape)
cost = computeCost(X, y, theta)
print("cost = ", cost)

# 初始化学习率和迭代次数
alpha = 0.01
iters = 1000

# 执行梯度下降算法
g, cost = gradientDescent(X, y, theta, alpha, iters)
print(g)

# 可视化结果
x = np.linspace(data.Population.min(),data.Population.max(),100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Training Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')

fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('Iteration')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')


# 使用sklearn 包里面实现的线性回归算法
model = linear_model.LinearRegression()
model.fit(X, y)

x = np.array(X[:, 1].A1)
# 预测结果
f = model.predict(X).flatten()
# 可视化
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Training Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size(using sklearn)')
plt.show()
```

上述代码参考自[Part 1 - Simple Linear Regression](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/)。具体可以查看[我的Github](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/linearRegressionPractise.py)。

#### 2. 逻辑回归

##### 简述

Logistic回归算法基于Sigmoid函数，或者说Sigmoid就是逻辑回归函数。Sigmoid函数定义如下：
$\frac{1}{1+e^{-z}}$。函数值域范围(0,1)。

因此逻辑回归函数的表达式如下：
$$
h_\theta(x) =g(\theta^T X) = \frac{1}{1+e^{-\theta^TX}} \\
其中，g(z) = \frac{1}{1+e^{-z}}
$$
其导数形式为：
$$
g\prime (z)  =  \frac{d}{dz} \frac{1}{1+e^{-z}} \\
		 = \frac{1}{(1+e^{-z})^2} (e^{-z}) \\
		 =  \frac{1}{1+e^{-z}} (1-  \frac{1}{1+e^{-z}}) \\
		 = g(z)(1-g(z))
$$

##### 代价函数

逻辑回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：
$$
p(y | x; \theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}
$$
到整个样本的后验概率就是:
$$
L(\theta) = p(y | X;\theta) \\
	      = \prod_{i=1}^{m} p(y^{(i)} | x^{(i)};\theta)\\
	      = \prod_{i=1}^{m} (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
$$
其中，$P(y=1|x;\theta) = h_\theta(x), P(y=0|x;\theta)=1-h_\theta(x)$。

通过对数进一步简化有：$l(\theta) = logL(\theta) = \sum_{i=1}^{m}y^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))$.

而逻辑回归的代价函数就是$-l(\theta)$。也就是如下所示：
$$
J(\theta) = -\frac{1}{m} [\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
$$

同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为：

![这里写图片描述](http://img.blog.csdn.net/20170212181541232?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



![这里写图片描述](http://img.blog.csdn.net/20170212181600234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##### 推导

具体参见博客[http://blog.csdn.net/ice_martin/article/details/61966790](http://blog.csdn.net/ice_martin/article/details/61966790)

![](https://uploadfiles.nowcoder.com/images/20170907/470415_1504777142123_4A47A0DB6E60853DEDFCFDF08A5CA249)



##### 逻辑回归与SVM

###### 相同点

1. 都是分类算法
2. 都是监督学习算法
3. 都是判别模型
4. 都能通过核函数方法针对非线性情况分类
5. 目标都是找一个分类超平面
6. 都能减少离群点的影响

###### 不同点

1. 损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss
2. 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。
3. 逻辑回归对概率建模，svm对分类超平面建模
4. 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有
5. 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响
6. 逻辑回归是统计方法，svm是几何方法
7. 线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响

##### 优缺点

###### 优点

1. 实现简单，广泛的应用于工业问题上；
2. 分类时计算量非常小，速度很快，存储资源低；
3. 便于观测样本概率分数
4. 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

###### 缺点

1. 容易**欠拟合，一般准确度不太高**
2. 只能处理**两分类**问题（在此基础上衍生出来的softmax可以用于多分类），且必须**线性可分**；
3. **特征空间很大**时，逻辑回归的性能不是很好；
4. 不能很好地处理**大量多类特征或变量**
5. 对于非线性特征，需要进行转换。

适用数据类型：数值型和标称型数据。
类别：分类算法。
试用场景：解决二分类问题。

##### 代码实现

首先是采用`sklearn`包中的逻辑回归算法代码：

```python
#Import Library
from sklearn.linear_model import LogisticRegression
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create logistic regression object

model = LogisticRegression()

# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)

#Equation coefficient and Intercept
print('Coefficient: \n', model.coef_)
print('Intercept: \n', model.intercept_)

#Predict Output
predicted= model.predict(x_test)
```

接下来则是应用例子，如下所示：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2016/10/19 21:35
@Author  : cai

实现多类的逻辑回归算法
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
from scipy.optimize import minimize
from scipy.io import loadmat

# 定义Sigmoid函数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 定义 cost函数
def costReg(theta, X, y, lambdas):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    h = X * theta.T
    first = np.multiply(-y, np.log(sigmoid(h)))
    second = np.multiply((1-y), np.log(1 - sigmoid(h)))
    reg = (lambdas / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))
    return np.sum(first - second) / (len(X)) + reg

# 梯度下降算法的实现, 输出梯度对权值的偏导数
def gradient(theta, X, y, lambdas):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)
    # 计算误差
    error = sigmoid(X * theta.T) - y

    grad = ((X.T * error) / len(X)).T + ((lambdas / len(X)) * theta)

    grad[0, 0] = np.sum(np.multiply(error, X[:, 0])) / len(X)

    return np.array(grad).ravel()

# 实现一对多的分类方法
def one_vs_all(X, y, num_labels, lambdas):
    rows = X.shape[0]
    params = X.shape[1]

    # 每个分类器有一个 k * (n+1)大小的权值数组
    all_theta = np.zeros((num_labels, params + 1))

    # 增加一列，这是用于偏置值
    X = np.insert(X, 0, values=np.ones(rows), axis=1)

    # 标签的索引从1开始
    for i in range(1, num_labels + 1):
        theta = np.zeros(params + 1)
        y_i = np.array([1 if label == i else 0 for label in y])
        y_i = np.reshape(y_i, (rows, 1))

        # 最小化损失函数
        fmin = minimize(fun=costReg, x0=theta, args=(X, y_i, lambdas), method='TNC', jac=gradient)
        all_theta[i-1, :] = fmin.x

    return all_theta

def predict_all(X, all_theta):
    rows = X.shape[0]
    params = X.shape[1]
    num_labels = all_theta.shape[0]

    # 增加一列，这是用于偏置值
    X = np.insert(X, 0, values=np.ones(rows), axis=1)

    X = np.matrix(X)
    all_theta = np.matrix(all_theta)

    # 对每个训练样本计算其类的概率值
    h = sigmoid(X * all_theta.T)

    # 获取最大概率值的数组索引
    h_argmax = np.argmax(h, axis=1)
    # 数组是从0开始索引，而标签值是从1开始，所以需要加1
    h_argmax = h_argmax + 1

    return h_argmax

dataPath = os.path.join('data', 'ex3data1.mat')
# 载入数据
data = loadmat(dataPath)
print(data)
print(data['X'].shape, data['y'].shape)

# print(np.unique(data['y']))
# 测试
# rows = data['X'].shape[0]
# params = data['X'].shape[1]
#
# all_theta = np.zeros((10, params + 1))
#
# X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)
#
# theta = np.zeros(params + 1)
#
# y_0 = np.array([1 if label == 0 else 0 for label in data['y']])
# y_0 = np.reshape(y_0, (rows, 1))
# print(X.shape, y_0.shape, theta.shape, all_theta.shape)

all_theta = one_vs_all(data['X'], data['y'], 10, 1)
print(all_theta)

# 计算分类准确率
y_pred = predict_all(data['X'], all_theta)
correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]
accuracy = (sum(map(int, correct)) / float(len(correct)))
print('accuracy = {0}%'.format(accuracy * 100))
```

实现代码来自[Part 4 - Multivariate Logistic Regression](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/)。具体可以查看[我的github](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/mulLogisticRegressionPractise.py)。

此外，C++代码参考[c++实现logistic回归代码](http://blog.csdn.net/u014403897/article/details/45871939)。

#### 3 决策树

##### 简介

**定义**：分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。

决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪。**

决策树学习本质上是从训练数据集中归纳出一组分类规则，也可以说是**由训练数据集估计条件概率模型**。它使用的损失函数通常是**正则化的极大似然函数**，其策略是以损失函数为目标函数的最小化。

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。

**决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。**

##### 特征选择

特征选择的准则通常是信息增益或者信息增益比。

首先是给出信息熵的计算公式$H(p) = -\sum_{i=1}^{n} p_i log p_i$，熵越大，随机变量的不确定性就越大。公式中$p_i$表示随机变量X属于类别$i$的概率，因此$n$表示类别的总数。

条件熵的定义为：$H(Y|X) = \sum_{i=1}^n p_iH(Y|X=x_i)$

已经有了**熵作为衡量训练样例集合纯度**的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为“**信息增益（information gain）**”。简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低(或者说，**样本按照某属性划分时造成熵减少的期望,个人结合前面理解，总结为用来衡量给定的属性区分训练样例的能力**)。更精确地讲，**一个属性A相对样例集合S的信息增益Gain(S,A)被定义为**：

![这里写图片描述](http://img.blog.csdn.net/20170213171939623?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中 Values(A)是属性A所有可能值的集合，Sv是S中属性A的值为v的子集，注意上式第一项就是原集合S的熵，第二项是用A分类S后的熵的期望值，第二项描述的期望熵就是每个子集的熵的加权和，权值为属性Sv的样例占原始样例S的比例|Sv|/|S|,所以Gain(S,A)是由于知道属性A的值而导致的期望熵减少，换句话来讲，Gain(S,A)是由于给定属性A的值而得到的关于目标函数值的信息。

信息增益的缺点是**存在偏向于选择取值较多的特征的问题**。为了解决这个问题，可以使用**信息增益比**。

因此，特征A对训练数据集D的信息增益比$g_R(D,A)$的定义如下：
$$
g_R(D, A) = \frac{g(D,A)}{H_A(D)}
$$
其中$g(D,A)$是信息增益，而$H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}$,其中$n$是特征A取值的个数。

不过对于信息增益比，其也存在**对可取值数目较少的属性有所偏好的问题**。

##### 决策树的生成

接下来会介绍决策树生成的算法，包括**ID3, C4.5**算法。

###### ID3算法

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体步骤如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170213204003966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

ID3的算法思路总结如下：

1. 首先是针对当前的集合，计算每个特征的信息增益
2. 然后选择信息增益最大的特征作为当前节点的决策决策特征
3. 根据特征不同的类别划分到不同的子节点（比如年龄特征有青年，中年，老年，则划分到3颗子树）
4. 然后继续对子节点进行递归，直到所有特征都被划分

**ID3的缺点是**

1）容易造成过度拟合（over fitting）； 
2）只能处理标称型数据（离散型）； 
3）信息增益的计算依赖于特征数目较多的特征，而属性取值最多的属性并不一定最优； 
4）抗噪性差，训练例子中正例和反例的比例较难控制

###### C4.5算法

C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：

- 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
- 在树构造过程中进行剪枝；
- 能够完成对连续属性的离散化处理；
- 能够对不完整数据进行处理。

C4.5算法有如下优点：**产生的分类规则易于理解，准确率较高**。

其缺点是：

1. **算法低效**，在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效 
2. **内存受限，**只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。 

算法的实现过程如下:

![这里写图片描述](http://img.blog.csdn.net/20170213204817800?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**实际上由于信息增益比的缺点，C4.5算法并没有直接选择信息增益比最大的候选划分属性，而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最高的。**

**无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只适用于小数据。当属性取值很多时最好选择C4.5算法，ID3得出的效果会非常差。**

##### 剪枝

在生成树的过程中，如果没有剪枝的操作的话，就会长成每一个叶都是单独的一类的样子。这样对我们的训练集是完全拟合的，但是对测试集则是非常不友好的，泛化能力不行。**因此，我们要减掉一些枝叶，使得模型泛化能力更强。** 
根据剪枝所出现的时间点不同，分为预剪枝和后剪枝。**预剪枝是在决策树的生成过程中进行的；后剪枝是在决策树生成之后进行的。**

决策树的剪枝往往是通过极小化决策树整体的损失函数或代价函数来实现的。简单来说，就是对比剪枝前后整体树的损失函数或者是准确率大小来判断是否需要进行剪枝。

决策树剪枝算法有多种，具体参考[决策树剪枝算法](http://blog.csdn.net/yujianmin1990/article/details/49864813)这篇文章。

##### CART

分类回归树(Classification And Regression Tree)是一个**决策二叉树**，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：

1. **分类树：基尼指数最小化(gini_index)**
2. **回归树：平方误差最小化**

分类树的生成步骤如下所示：
![这里写图片描述](http://img.blog.csdn.net/20170213212619889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

简单总结如下：

1. 首先是根据当前特征计算他们的基尼增益
2. 选择**基尼增益最小**的特征作为划分特征
3. 从该特征中查找基尼指数最小的分类类别作为最优划分点
4. 将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于
5. 针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值

基尼指数的计算公式为$Gini(p) = 1 - \sum_{k=1}^K p_k^2$。K是类别的数目，$p_k$表示样本属于第k类的概率值。它可以用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。

给定一个数据集D，在特征A的条件下，其基尼指数定义为$Gini(D,A) = \sum_{i=1}^n \frac{|D_i|}{|D|} Gini(D_i)$。

回归树：

> 回归树是以平方误差最小化的准则划分为两块区域

1. 遍历特征计算最优的划分点$s$，
   使其最小化的平方误差是：$min \{min(∑_i^{R1}((y_i−c_1)^2))+min(∑_i^{R2}((y_i−c_2)^2))\}$
   计算根据s划分到左侧和右侧子树的目标值与预测值之差的平方和最小，这里的预测值是两个子树上输入$x_i$样本对应$y_i$的均值

2. 找到最小的划分特征j以及其最优的划分点$s$,根据特征$j$以及划分点$s$将现有的样本划分为两个区域，一个是在特征$j$上小于等于$s$，另一个在特征$j$上大于$s$
   $$
   R1(j)= \{x|x(j)≤s\} \\
   R2(j)=\{x|x(j)>s\}  \\
   c_m = \frac{1}{N_m} \sum_{x_i \in R_m(j, s)} y_i, m = 1,2, \quad x\in R_m
   $$

3. 进入两个子区域按上述方法继续划分，直到到达停止条件

回归树的缺点：

- **不如线性回归普遍；**
- **要求大量训练数据；**
- **难以确定某个特征的整体影响；**
- **比线性回归模型难解释**

关于CART剪枝的方法可以参考[决策树系列（五）——CART](http://www.cnblogs.com/yonghao/p/5135386.html)。

##### 停止条件

1. 直到每个叶子节点都只有一种类型的记录时停止，（这种方式很容易过拟合）
2. 另一种是当叶子节点的样本数目小于一定的阈值或者节点的信息增益小于一定的阈值时停止

##### 关于特征与目标值

1. 特征离散 目标值离散：可以使用ID3，cart
2. 特征连续 目标值离散：将连续的特征离散化 可以使用ID3，cart

##### 连续值属性的处理

​	**C4.5既可以处理离散型属性，也可以处理连续性属性。**在选择某节点上的分枝属性时，对于离散型描述属性，C4.5的处理方法与ID3相同。对于连续分布的特征，其处理方法是：

　　**先把连续属性转换为离散属性再进行处理。**虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$<=v_j$的分到左子树，$>v_j$的分到右子树。计算这N-1种情况下最大的信息增益率。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。**经证明，在决定连续特征的分界点时采用增益这个指标**（因为若采用增益率，splittedinfo影响分裂点信息度量准确性，若某分界点恰好将连续特征分成数目相等的两部分时其抑制作用最大），**而选择属性的时候才使用增益率这个指标能选择出最佳分类特征。**

　　在C4.5中，对连续属性的处理如下：

　　　　1、对特征的取值进行升序排序

　　　　2、两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算**每个可能的分裂点的信息增益**（InforGain）。**优化算法就是只计算分类属性发生改变的那些特征取值。**

　　　　3、选择修正后**信息增益(InforGain)最大的分裂点**作为该特征的最佳分裂点

　　　　4、计算**最佳分裂点的信息增益率（Gain Ratio）作为特征的Gain Ratio**。注意，此处需对最佳分裂点的信息增益进行修正：减去log2(N-1)/|D|（N是连续特征的取值个数，D是训练数据数目，此修正的原因在于：**当离散属性和连续属性并存时，C4.5算法倾向于选择连续特征做最佳树分裂点**）

##### 决策树的分类与回归

- 分类树
  输出叶子节点中所属类别最多的那一类
- 回归树
  输出叶子节点中各个样本值的平均值

##### 理想的决策树

1. 叶子节点数尽量少
2. 叶子节点的深度尽量小(太深可能会过拟合)

##### 过拟合原因

采用上面算法生成的决策树在事件中往往会导致过滤拟合。也就是该决策树对训练数据可以得到很低的错误率，但是运用到测试数据上却得到非常高的错误率。过渡拟合的原因有以下几点：

- **噪音数据**：训练数据中存在噪音数据，决策树的某些节点有噪音数据作为分割标准，导致决策树无法代表真实数据。
- **缺少代表性数据**：训练数据没有包含所有具有代表性的数据，导致某一类数据无法很好的匹配，这一点可以通过观察混淆矩阵（Confusion Matrix）分析得出。
- **多重比较（Mulitple Comparition）**：举个列子，股票分析师预测股票涨或跌。假设分析师都是靠随机猜测，也就是他们正确的概率是0.5。每一个人预测10次，那么预测正确的次数在8次或8次以上的概率为 [![image](http://images.cnitblog.com/blog/349490/201303/15154352-dd92afccc91e4e2e9a08578d8ba9ab04.png)](http://images.cnitblog.com/blog/349490/201303/15154352-831a596c9a3c4fcca3d6e8f863b2f91f.png)，只有5%左右，比较低。但是如果50个分析师，每个人预测10次，选择至少一个人得到8次或以上的人作为代表，那么概率为 [![image](http://images.cnitblog.com/blog/349490/201303/15154353-c827fc2a20e74c31a3f6a1f1d64a436c.png)](http://images.cnitblog.com/blog/349490/201303/15154352-be8974a2a79a4662a4579210187d31fa.png)，概率十分大，随着分析师人数的增加，概率无限接近1。但是，选出来的分析师其实是打酱油的，他对未来的预测不能做任何保证。上面这个例子就是**多重比较**。这一情况和决策树选取分割点类似，需要在每个变量的每一个值中选取一个作为分割的代表，所以选出一个噪音分割标准的概率是很大的。

##### 解决决策树的过拟合

1. 剪枝
   1. 前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）
   2. 后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）
2. 交叉验证
3. 随机森林

##### 优缺点

###### 优点

1. 计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；

2. 效率高，决策树只需要一次构建，反复使用。

3. 训练时间复杂度较低，预测的过程比较快速，每一次预测的最大计算次数不超过决策树的深度。对于N个样本，每个样本都包含M个属性，在不考虑连续属性离散化以及子树增长的代价情况下，决策树算法的平均时间复杂度仅为$O(M*N*logN)$。构建一个决策树，最坏情况下的复杂度是$O(tree  depth)$，其中树的深度一般呈对数增长。

   ​

###### 缺点

1. 单颗决策树分类能力弱，并且对连续值变量难以处理；
2. 容易过拟合（后续出现了随机森林，减小了过拟合现象）；
3. 可能或陷于局部最小值中
4. 没有在线学习

##### 代码实现

使用sklearn中决策树函数的简单代码例子如下所示：

```python
#Import Library
#Import other necessary libraries like pandas, numpy...

from sklearn import tree
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create tree object 
model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  

# model = tree.DecisionTreeRegressor() for regression

# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)

#Predict Output
predicted= model.predict(x_test)
```

决策树的代码在开源库OpenCV中有实现，具体的源码分析可以参考[Opencv2.4.9源码分析——Decision Trees](http://blog.csdn.net/zhaocj/article/details/50503450)，这篇文章也比较详细总结了决策树的知识点以及对OpenCV中决策树部分的源码进行了分析。

#### 4 随机森林

##### 简介

> **随机森林**指的是利用多棵树对样本进行训练并预测的一种分类器。它是由多棵CART(Classification And Regression Tree)构成的。对于每棵树，其**使用的训练集是从总的训练集中有放回采样出来的**，这意味着总训练集中有些样本可能多次出现在一棵树的训练集中，也可能从未出现在一棵树的训练集中。在训练每棵树的节点时，**使用的特征是从所有特征中按照一定比例随机地无放回的抽取的**，假设总的特征数是`M`,则这个比例可以是$\sqrt(M), \frac{1}{2} \sqrt(M), 2\sqrt(M)$。

##### 训练过程

随机森林的训练过程可以总结如下：

(1)给定训练集`S`，测试集`T`，特征维数`F`。确定参数：使用到的CART的数量`t`，每棵树的深度`d`，每个节点使用到的特征数量`f`，终止条件：节点上最少样本数`s`，节点上最少的信息增益`m`

对于第1-t棵树，`i=1-t`：

(2)从S中有放回的抽取大小和S一样的训练集S(i)，作为根节点的样本，从根节点开始训练

(3)如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的预测输出为当前节点样本集合中数量最多的那一类`c(j)`，概率`p`为`c(j)`占当前样本集的比例；如果是回归问题，预测输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。**如果当前节点没有达到终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的一维特征`k`及其阈值`th`，当前节点上样本第k维特征小于`th`的样本被划分到左节点，其余的被划分到右节点。**继续训练其他节点。有关分类效果的评判标准在后面会讲。

(4)重复(2)(3)直到所有节点都训练过了或者被标记为叶子节点。

(5)重复(2),(3),(4)直到所有CART都被训练过。

##### 预测过程

预测过程如下：

对于第1-t棵树，i=1-t：

(1)从当前树的根节点开始，根据当前节点的阈值th，判断是进入左节点(`<th`)还是进入右节点(`>=th`)，直到到达，某个叶子节点，并输出预测值。

(2)重复执行(1)直到所有t棵树都输出了预测值。**如果是分类问题，则输出为所有树中预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值。**



**有关分类效果的评判标准，因为使用的是CART，因此使用的也是CART的评判标准，和C3.0,C4.5都不相同。**

**对于分类问题（将某个样本划分到某一类），也就是离散变量问题，CART使用Gini值作为评判标准。定义为$Gini(p) = 1 - \sum_{k=1}^K p_k^2$，  $p_k$为当前节点上数据集中第k类样本的比例。**

**例如：分为2类，当前节点上有100个样本，属于第一类的样本有70个，属于第二类的样本有30个，则$Gini=1-0.7×07-0.3×03=0.42$，可以看出，类别分布越平均，Gini值越大，类分布越不均匀，Gini值越小。在寻找最佳的分类特征和阈值时，评判标准为：$argmax（Gini-GiniLeft-GiniRight）$，即寻找最佳的特征f和阈值th，使得当前节点的Gini值减去左子节点的Gini和右子节点的Gini值最大。**

**对于回归问题，相对更加简单，直接使用$argmax(Var-VarLeft-VarRight)$作为评判标准，即当前节点训练集的方差Var减去减去左子节点的方差VarLeft和右子节点的方差VarRight值，求其最大值**。

##### 特征重要性度量

计算某个特征X的重要性时，具体步骤如下：

1. 对每一颗决策树，选择相应的袋外数据（out of bag，OOB）计算袋外数据误差，记为errOOB1.

   所谓袋外数据是指，每次建立决策树时，通过重复抽样得到一个数据用于训练决策树，这时还有**大约1/3的数据没有被利用**，没有参与决策树的建立。这部分数据可以用于对决策树的性能进行评估，计算模型的预测错误率，称为袋外数据误差。

   **这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。**

2. 随机对袋外数据OOB所有样本的特征X加入**噪声干扰**（可以随机改变样本在特征X处的值），再次计算袋外数据误差，记为errOOB2。

3. 假设森林中有N棵树，则特征X的重要性=$∑\frac{errOOB2-errOOB1}{N}$。这个数值之所以能够说明特征的重要性是因为，**如果加入随机噪声后，袋外数据准确率大幅度下降（即errOOB2上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。**

##### 特征选择

在特征重要性的基础上，特征选择的步骤如下：

1. 计算每个特征的重要性，并按降序排序
2. 确定要剔除的比例，依据特征重要性剔除相应比例的特征，得到一个新的特征集
3. 用新的特征集重复上述过程，直到剩下m个特征（m为提前设定的值）。
4. 根据上述过程中得到的各个特征集和特征集对应的袋外误差率，选择袋外误差率最低的特征集。

##### 优缺点

###### 优点

-     在数据集上表现良好，在当前的很多数据集上，相对其他算法有着很大的优势
-     它能够处理很高维度（feature很多）的数据，并且不用做特征选择
-     在训练完后，它能够给出哪些feature比较重要
-     在创建随机森林的时候，对generlization error使用的是无偏估计
-     训练速度快，容易做成并行化方法
-     在训练过程中，能够检测到feature间的互相影响
-     实现比较简单
-     对于不平衡的数据集来说，它可以平衡误差。
-     如果有很大一部分的特征遗失，仍可以维持准确度。

###### 缺点

1. 随机森林已经被证明在某些**噪音较大**的分类或回归问题上会过拟
2. 对于有不同取值的属性的数据，**取值划分较多的属性会对随机森林产生更大的影响**，所以随机森林在这种数据上产出的属性权值是不可信的。

##### 与bagging的区别

1. Random forest是选与**输入样本的数目相同多**的样本（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比**输入样本的数目少**的样本；

2. bagging是用**全部特征**来得到分类器，而Random forest是需要从全部特征中**选取其中的一部分**来训练得到分类器； **一般Random forest效果比bagging效果好！**

   ​

##### 代码实现

简单使用sklearn中随机森林算法的例子：

```python
#Import Library
from sklearn.ensemble import RandomForestClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create Random Forest object
model= RandomForestClassifier()

# Train the model using the training sets and check score
model.fit(X, y)

#Predict Output
predicted= model.predict(x_test)
```

此外，OpenCV中也实现了随机森林算法。具体使用例子可以查看[RandomForest随机森林总结](http://www.cnblogs.com/hrlnw/p/3850459.html)。

#### 5 SVM

##### 简介

> SVM是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是**间隔最大化**，最终可转化为一个凸二次规划问题的求解。或者简单的可以理解为就是在高维空间中寻找一个合理的超平面将数据点分隔开来，其中涉及到非线性数据到高维的映射以达到数据线性可分的目的。

训练数据线性可分时，通过**硬间隔最大化**，学习一个线性分类器，即**线性可分支持向量机**，又称为硬间隔支持向量机；训练数据近似线性可分时，通过**软间隔最大化**，也学习一个线性分类器，即**线性支持向量机**，也称为软间隔支持向量机；训练数据线性不可分时，通过**使用核技巧和软间隔最大化**，学习**非线性支持向量机**。

##### 线性可分支持向量机和硬间隔最大化

接下来主要是手写的笔记，主要参考

* 《统计学习方法》
* [SVM详解(包含它的参数C为什么影响着分类器行为)-scikit-learn拟合线性和非线性的SVM](http://blog.csdn.net/xlinsist/article/details/51311755)
* [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)
* [SVM-支持向量机算法概述](http://blog.csdn.net/passball/article/details/7661887)
* [机器学习算法与Python实践之（二）支持向量机（SVM）初级](http://blog.csdn.net/zouxy09/article/details/17291543)
* [机器学习算法与Python实践之（三）支持向量机（SVM）进阶](http://blog.csdn.net/zouxy09/article/details/17291805)
* [机器学习算法与Python实践之（四）支持向量机（SVM）实现](http://blog.csdn.net/zouxy09/article/details/17292011)
* [干货|详解LinearSVM](https://mp.weixin.qq.com/s/MfYRipBX4la5jEG-ZMBhEw)

![这里写图片描述](http://img.blog.csdn.net/20170216123438940?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

关于凸函数，**凸集是指有这么一个点的集合，其中任取两个点连一条直线，这条线上的点仍然在这个集合内部，因此说“凸”是很形象的**。例如下图，对于凸函数（在数学表示上，满足约束条件是仿射函数，也就是线性的Ax+b的形式）来说，局部最优就是全局最优，但对非凸函数来说就不是了。

![这里写图片描述](http://img.blog.csdn.net/20170216133841807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



![这里写图片描述](http://img.blog.csdn.net/20170216123848270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> **支持向量**是训练数据集的样本点中与分离超平面距离最近的样本点的实例。

如下图所示：

![这里写图片描述](http://img.blog.csdn.net/20170216125100136?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

图中对类1，即Class 1的支持向量就在超平面$H_2: wx+b=-1$上，而对于类2，即Class 2正类的支持向量就在超平面$H_1: wx+b=1$上。而在这两个超平面中间的距离，即图中标着`m`的距离称为**间隔**，它依赖于分离超平面的法向量$w$，等于$\frac{2}{|| w ||}$，而两个超平面$H_1 和H_2$称为**间隔平面**。

在决定分离超平面时只有支持向量其作用，其他实例点并不起作用。如果移动支持向量将改变所求的解。**正是因为支持向量在确定分离超平面中起着决定性作用 ，所以将这种分类模型称为支持向量机。**支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。

##### 线性支持向量机和软间隔最大化

![这里写图片描述](http://img.blog.csdn.net/20170216125731017?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170216125743970?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##### 核函数

![这里写图片描述](http://img.blog.csdn.net/20170216140507376?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170216140517591?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

采用不同的核函数就相当于采用不同的相似度的衡量方法。从计算的角度，不管$Φ(x)$变换的空间维度有多高，甚至是无限维（函数就是无限维的），**这个空间的线性支持向量机的求解都可以在原空间通过核函数进行**，这样就可以避免了高维空间里的计算，而**计算核函数的复杂度和计算原始样本内积的复杂度没有实质性的增加**。

> 一般情况下RBF效果是不会差于Linear
> 但是时间上RBF会耗费更多
> 下面是吴恩达的见解：
>
> 1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
> 2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
> 3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

更多有关核函数可以参考[【模式识别】SVM核函数](http://blog.csdn.net/xiaowei_cqu/article/details/35993729)和[SVM的核函数如何选取?--知乎](https://www.zhihu.com/question/21883548)。

这里总结一下：支持向量机的基本思想可以概括为，**首先通过非线性变换将输入空间变换到一个高维的空间，然后在这个新的空间求最优分类面即最大间隔分类面，而这种非线性变换是通过定义适当的内积核函数来实现的。**SVM实际上是根据统计学习理论依照**结构风险最小化**的原则提出的，要求实现两个目的：**1）两类问题能够分开（经验风险最小）2）margin最大化（风险上界最小），既是在保证风险最小的子集中选择经验风险最小的函数。**

##### 优缺点

###### 优点

1. 使用核函数可以向高维空间进行映射
2. 使用核函数可以解决非线性的分类
3. 分类思想很简单，就是将样本与决策面的间隔最大化
4. 分类效果较好

###### 缺点

1. 对大规模数据训练比较困难
2. 无法直接支持多分类，但是可以使用间接的方法来做
3. 噪声也会影响SVM的性能，因为SVM主要是由少量的支持向量决定的。

##### 多类分类

###### 直接法

直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类。但是计算复杂度很高，实现起来较为困难。

###### 间接法

> **1 一对多方法**就是每次训练的时候设置其中某个类为一类，其余所有类为另一个类。比如有A,B,C,D四个类，第一次A是一个类，{B,C,D}是一个类，训练一个分类器，第二次B是一个类，然后A,C,D是一个类，训练一个分类器，依次类推。因此，如果总共有$n$个类，最终将训练$n$个分类器。测试的时候，将测试样本都分别送入所有分类器中，取得到最大值的类别作为其分类结果。这是因为到分类面距离越大，分类越可信。

这种方法的优点是**每个优化问题的规模比较小，而且分类速度很快**，因为分类器数目和类别数目相同；但是，有时会出现这样两种情况：对一个测试样本，每个分类器都得到它属于分类器所在类别；或者都不属于任意一个分类器的类别。**前者称为分类重叠现象，后者叫不可分类现象**。前者可以任意选择一个结果或者就按照其到每个超平面的距离来分，哪个远选哪个类别；而后者只能分给新的第$n+1$个类别了。最大的缺点还是由于将$n-1$个类别作为一个类别，其数目会数倍于只有1个类的类别，这样会人为造成**数据集偏斜**的问题。

> **2 一对一方法**是任意两个类都训练一个分类器，那么$n$个类就需要$\frac{n(n-1)}{2}$个分类器。预测的时候通过投票选择最终结果。

这个方法同样会有分类重叠的现象，但不会有不可分类现象，因为不可能所有类别的票数都是0。**这种方法会比较高效，每次训练使用的样本其实就只有两类数据，而且预测会比较稳定，但是缺点是预测时间会很久。**

> **3 层次支持向量机（H-SVMs）。**层次分类法首先将所有类别分成两个子类，再将子类进一步划分成两个次级子类，如此循环，直到得到一个单独的类别为止。

> **4 DAG-SVMS**是由Platt提出的决策导向的循环图DDAG导出的,是针对“一对一”SVMS存在误分、拒分现象提出的。

##### 序列最小最优化算法(SMO)

SMO是用于快速求解SVM的，是一种启发式算法。
基本思路如下：如果所有变量的解都满足此最优化问题的**KKT**条件，那么这个最优化问题的解就得到了。**因为KKT条件是该最优化问题的充分必要条件**。否则它选择**凸二次规划的两个变量**，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：

1. 其中一个是**严重违反KKT条件**的一个变量
2. 另一个变量由约束条件自动确定。

整个SMO算法分为两部分：求解两个变量二次规划的解析方法和选择变量的启发式方法。

![这里写图片描述](http://img.blog.csdn.net/20170216155938379?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170216155947160?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> **SMO称选择第一个变量的过程为外层循环**。外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第一个变量。具体的，检验训练样本($x_i, y_i$)是否满足KKT条件，也就是：

![这里写图片描述](http://img.blog.csdn.net/20170216160228381?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 该检验是在$ε$范围内进行的。在检验过程中，外层循环**首先遍历所有满足条件$0<α_j<C$的样本点，即在间隔边界上的支持向量点，**检验他们是否满足KKT条件，然后选择违反KKT条件最严重的$α_i$。如果这些样本点都满足KKT条件，那么遍历整个训练集，检验他们是否满足KKT条件，然后选择违反KKT条件最严重的$α_i$。

       **优先选择遍历非边界数据样本，因为非边界数据样本更有可能需要调整，边界数据样本常常不能得到进一步调整而留在边界上**。由于大部分数据样本都很明显不可能是支持向量，因此对应的$α$乘子一旦取得零值就无需再调整。遍历非边界数据样本并选出他们当中违反KKT 条件为止。**当某一次遍历发现没有非边界数据样本得到调整时，遍历所有数据样本，以检验是否整个集合都满足KKT条件**。如果整个集合的检验中又有数据样本被进一步进化，则有必要再遍历非边界数据样本。这样，**不停地在遍历所有数据样本和遍历非边界数据样本之间切换，直到整个样本集合都满足KKT条件为止**。**以上用KKT条件对数据样本所做的检验都以达到一定精度ε就可以停止为条件。如果要求十分精确的输出算法，则往往不能很快收敛。**

       对整个数据集的遍历扫描相当容易，而实现对非边界$α_i$的扫描时，首先需要将所有非边界样本的$α_i$值（也就是满足$0<α_i<C$）保存到新的一个列表中，然后再对其进行遍历。同时，该步骤跳过那些已知的不会改变的$α_i$值。

> SMO称选择第2个变量的过程为内层循环。第2个变量选择的标准是希望能使$\alpha_2$有足够大的变化。

记$g(x) = \sum_{i=1}^N \alpha_i y_iK(x_i, x) + b$, 令$E_i = g(x_i) - y_i= (\sum_{i=1}^N \alpha_i y_iK(x_i, x) + b) - y_i,\qquad i = 1, 2$。

当$i = 1, 2$时，$E_i$是函数$g(x_i)$对输入$x_i$的预测值与真实输出$y_i$之差。

对于第2个变量$\alpha_2$的选择，一个简单的做法是选择让$|E_1 - E_2|$最大的变化。为了节省计算时间，将所有$E_i$值保存在一个列表中。

如果上述方法选择的变量不能使目标函数有足够的下降，那么采用以下启发式规则继续选择第2个变量。遍历在**间隔边界上**的支持向量点，依次将其对应的向量作为$\alpha_2$试用，直到目标函数有足够的下降。若找不到合适的，则遍历训练数据集；若仍找不到合适的$\alpha_2$，则放弃第一个变量$\alpha_1$，再通过外层循环寻求另外的$\alpha_1$。

选择这两个拉格朗日乘子后，我们需要先计算这些参数的约束值。然后再求解这个约束最大化问题,下面用$\alpha_i, \alpha_j$表示选择的第1个和第2个变量。

 首先，我们需要给$α_j$找到边界$L\le α_j \le H$，以保证$α_j$满足$0\le α_j\le C$的约束。这意味着$α_j$必须落入这个盒子中。由于只有两个变量($α_i, α_j$)，约束可以用二维空间中的图形来表示，如下图：

![这里写图片描述](http://img.blog.csdn.net/20170216163029434?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  不等式约束使得($α_i, α_j$)在盒子[0, C]x[0, C]内，等式约束使得($α_i, α_j$)在平行于盒子[0, C]x[0, C]的对角线的直线上。**因此要求的是目标函数在一条平行于对角线的线段上的最优值**。这使得两个变量的最优化问题成为实质的单变量的最优化问题。由图可以得到，$α_j$的上下界可以通过下面的方法得到：

![这里写图片描述](http://img.blog.csdn.net/20170216163047736?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

   我们优化的时候，$α_j$必须要满足上面这个约束。也就是说上面是$α_j$的可行域。然后我们开始寻找$α_j$，使得目标函数最大化。通过推导得到$α_j$的更新公式如下：

![这里写图片描述](http://img.blog.csdn.net/20170216163407907?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  这里$E_k$可以看做对第$k$个样本，SVM的输出与期待输出，也就是样本标签的误差。

![这里写图片描述](http://img.blog.csdn.net/20170216163623191?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  而$η$实际上是度量两个样本$i和j$的相似性的。在计算$η$的时候，我们需要使用核函数，那么就可以用核函数来取代上面的内积。

  得到新的$α_j$后，我们需要保证它处于边界内。换句话说，如果这个优化后的值跑出了边界L和H，我们就需要简单的裁剪，将$α_j$收回这个范围：

![这里写图片描述](http://img.blog.csdn.net/20170216163645193?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  最后，得到优化的$α_j$后，我们需要用它来计算$α_i$：

![这里写图片描述](http://img.blog.csdn.net/20170216163655723?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 到这里，$α_i和α_j$的优化就完成了。

最后就是更新阈值$b$了，使得两个样本$i和j$都满足KKT条件。如果优化后$α_i$不在边界上（也就是满足$0<α_i<C$，这时候根据KKT条件，可以得到$y_ig_i(x_i)=1$，这样我们才可以计算$b$），那下面的阈值$b_1$是有效的，因为当输入$x_i$时它迫使SVM输出$y_i$。

![这里写图片描述](http://img.blog.csdn.net/20170216164702252?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

同样，如果$0 < \alpha_j < C$,那么下面的$b_2$也是有效的：

![这里写图片描述](http://img.blog.csdn.net/20170216165055215?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 如果$0 < \alpha_i < C$和$0 < \alpha_j < C$都满足，那么$b_1和b_2$都有效，而且他们是相等的。如果他们两个都处于边界上（也就是$α_i=0$或者$α_i=C$，同时$α_j=0$或者$α_j=C$），那么在$b_1$和$b_2$之间的阈值都满足KKT条件，一般我们取他们的平均值$b=\frac{b1+b2}{2}$。所以，总的来说对$b$的更新如下：

![这里写图片描述](http://img.blog.csdn.net/20170216165345667?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

每次完成两个变量的优化后，还必须更新对应的$E_i$值，并将它们保存在列表中。

##### KKT条件分析

KKT条件具体可以查看[深入理解拉格朗日乘子法（Lagrange Multiplier) 和KKT条件](http://blog.csdn.net/xianlingmao/article/details/7919597)。

SVM的KKT条件应该是

![](http://images.cnblogs.com/cnblogs_com/zgw21cn/031309_1004_SVM12.png)

即满足:

* L对各个x求导为零； 
* $h(x)=0$; 
* $∑α_ig_i(x)=0，α_i≥0$

拉格朗日乘子法(Lagrange Multiplier)和KKT(Karush-Kuhn-Tucker)条件是**求解约束优化问题**的重要方法，**在有等式约束时使用拉格朗日乘子法，在有不等约束时使用KKT条件**。前提是：只有当**目标函数为凸函数**时，使用这两种方法才保证求得的是**最优解**。

假设我们优化得到的最优解是：$α_i,β_i, ξ_i, w和b$。我们的最优解需要满足KKT条件：

![这里写图片描述](http://img.blog.csdn.net/20170216170048177?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

同时$β_i, ξ_i$都需要大于等于0，而$α_i$需要在0和C之间。那可以分三种情况讨论：

![这里写图片描述](http://img.blog.csdn.net/20170216170217710?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

因此，KKT条件变成了：

![这里写图片描述](http://img.blog.csdn.net/20170216170256658?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

第一个式子表明**如果$α_i=0$，那么该样本落在两条间隔线外**。第二个式子表明**如果$α_i=C$，那么该样本有可能落在两条间隔线内部，也有可能落在两条间隔线上面，主要看对应的松弛变量的取值是等于0还是大于0**，第三个式子表明**如果$0<α_i<C$，那么该样本一定落在分隔线上**（这点很重要，$b$就是拿这些落在分隔线上的点来求的，因为在分割线上$w^Tx+b=1$或者$w^Tx+b=-1$嘛，才是等式，在其他地方，都是不等式，求解不了$b$）。具体形象化的表示如下：

![这里写图片描述](http://img.blog.csdn.net/20170216170440432?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 通过KKT条件可知，$α_i$不等于0的都是支持向量，它有可能落在分隔线上，也有可能落在两条分隔线内部。KKT条件是非常重要的，在SMO也就是SVM的其中一个实现算法中，我们可以看到它的重要应用。

##### 代码实现

下面是线性SVM的代码实现：

```python
from sklearn import datasets
import numpy as np
from sklearn.cross_validation import train_test_split

iris = datasets.load_iris() # 由于Iris是很有名的数据集，scikit-learn已经原生自带了。
X = iris.data[:, [2, 3]]
y = iris.target # 标签已经转换成0，1，2了
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # 为了看模型在没有见过数据集上的表现，随机拿出数据集中30%的部分做测试

# 为了追求机器学习和最优化算法的最佳性能，我们将特征缩放
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train) # 估算每个特征的平均值和标准差
sc.mean_ # 查看特征的平均值，由于Iris我们只用了两个特征，所以结果是array([ 3.82857143,  1.22666667])
sc.scale_ # 查看特征的标准差，这个结果是array([ 1.79595918,  0.77769705])
X_train_std = sc.transform(X_train)
# 注意：这里我们要用同样的参数来标准化测试集，使得测试集和训练集之间有可比性
X_test_std = sc.transform(X_test)
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

# 导入SVC
from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1.0, random_state=0) # 用线性核，你也可以通过kernel参数指定其它的核。
svm.fit(X_train_std, y_train)
# 打印决策边界，这个函数是我自己写的，如果你想要的话，我发给你
plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```

接下来是使用非线性SVM的代码：

```python
svm = SVC(kernel='rbf', random_state=0, gamma=x, C=1.0) # 令gamma参数中的x分别等于0.2和100.0
svm.fit(X_train_std, y_train) # 这两个参数和上面代码中的训练集一样
plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```

SVM的知识点就总结到这里，还是参考了不少文章和看书才完成，但是需要继续通过实践才能加深对SVM的了解。











