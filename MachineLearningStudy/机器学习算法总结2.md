# 机器学习算法总结2

------

## 6. 朴素贝叶斯

参考文章：

* 《统计学习方法》

* [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)

* [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)

* [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)


### 简介

> **朴素贝叶斯**是基于贝叶斯定理与特征条件独立假设的分类方法。

**贝叶斯定理**是基于条件概率来计算的，条件概率是在已知事件B发生的前提下，求解事件A发生的概率，即$P(A|B)=\frac{P(AB)}{P(B)}$，而贝叶斯定理则可以通过$P(A|B)$来求解$P(B|A)$：
$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$
其中分母$P(A)$可以根据全概率公式分解为：$P(A)=\sum_{i=1}^n P(B_i)P(A|B_i)$

而**特征条件独立假设**是指假设各个维度的特征$x_1,x_2,...,x_n$互相独立，则条件概率可以转化为：
$$
P(x|y_{k})=P(x_{1},x_{2},...,x_{n}|y_{k})=\prod_{i=1}^{n}P(x_{i}|y_{k})
$$

朴素贝叶斯分类器可表示为：
$$
f(x)=argmax_{y_{k}} P(y_{k}|x)=argmax_{y_{k}} \frac{P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})}{\sum_{k}P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})}
$$
而由于对上述公式中分母的值都是一样的，所以可以忽略分母部分，即可以表示为：
$$
f(x)=argmax P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})
$$
这里$P(y_k)$是先验概率，而$P(y_k|x)$则是后验概率，朴素贝叶斯的目标就是最大化后验概率，这等价于期望风险最小化。

### 参数估计

#### 极大似然估计

朴素贝叶斯的学习意味着估计$P(y_k)$和$P(x_i|y_k)$,可以通过**极大似然估计**来估计相应的概率。

![这里写图片描述](http://img.blog.csdn.net/20170218105353279?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

如上图所示，分别是$P(y_k)$和$P(x_i|y_k)$的极大似然估计。

当求解完上述两个概率，就可以对测试样本使用朴素贝叶斯分类算法来预测其所属于的类别，简单总结的算法流程如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170218105758081?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 贝叶斯估计/多项式模型

用**极大似然估计可能会出现所要估计的概率值为0**的情况，这会影响到后验概率的计算，使分类产生偏差。解决这个问题的办法是使用**贝叶斯估计**，也被称为多项式模型。

当**特征是离散的时候，使用多项式模型**。多项式模型在计算先验概率$P(y_k)$和条件概率$P(x_i|y_k)$时，会做一些**平滑处理**，具体公式为：
$$
P(y_k)=\frac{N_{y_k}+α}{N+kα}
$$

> $N$是总的样本个数，$k$是总的类别个数，$N_{y_k}$是类别为$y_k$的样本个数，$α$是平滑值。

$$
P(x_i|y_k) = \frac{N_{y_k,x_i} + \alpha}{N_{y_k}+n\alpha}
$$

> $N_{y_k}$是类别为$y_k$的样本个数，$n$是特征的维数，$N_{y_k,x_i}$是类别为$y_k$的样本中，第$i$维特征的值是$x_i$的样本个数，$α$是平滑值。

当$α=1$时，称作**Laplace平滑**，当$0<α<1$时，称作**Lidstone**平滑，α=0时不做平滑。

如果不做平滑，当某一维特征的值$x_i$没在训练样本中出现过时，会导致$P(x_i|y_k)=0$，从而导致后验概率为0。加上平滑就可以克服这个问题。

#### 高斯模型

当特征是连续变量的时候，运用多项式模型会导致很多$P(x_i|y_k) = 0$（不做平滑的情况下），即使做平滑，所得到的条件概率也难以描述真实情况，所以处理连续变量，应该采用高斯模型。

高斯模型是假设每一维特征都服从高斯分布（正态分布）：
$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$
$\mu_{y_{k},i}$表示类别为$y_k$的样本中，第$i$维特征的均值；
$\sigma_{y_{k},i}^{2}$表示类别为$y_k$的样本中，第$i$维特征的方差。

#### 伯努利模型

与多项式模型一样，伯努利模型适用于**离散特征**的情况，所不同的是，**伯努利模型中每个特征的取值只能是1和0**(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).

伯努利模型中，条件概率$P(x_i|y_k)$的计算方式是：

当特征值$x_i$为1时，$P(x_i|y_k)=P(x_i=1|y_k)$；

当特征值$x_i$为0时，$P(x_i|y_k)=1−P(x_i=1|y_k)$；

### 工作流程

1. 准备阶段
   确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。
2. 训练阶段
   计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计
3. 应用阶段
   使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别

### 属性特征

1. 特征为离散值时直接统计即可（表示统计概率）
2. 特征为连续值的时候假定特征符合高斯分布，则有

$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$

### 优缺点

#### 优点

1. 对小规模的数据表现很好，适合多分类任务，适合增量式训练。

#### 缺点
2. 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。


### 代码实现

下面是使用`sklearn`的代码例子，分别实现上述三种模型,例子来自[朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)。
下面是高斯模型的实现

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> iris.feature_names  # 四个特征的名字
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
>>> iris.data
array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2],
       [ 5.4,  3.9,  1.7,  0.4],
       [ 4.6,  3.4,  1.4,  0.3],
       [ 5. ,  3.4,  1.5,  0.2],
       ......
       [ 6.5,  3. ,  5.2,  2. ],
       [ 6.2,  3.4,  5.4,  2.3],
       [ 5.9,  3. ,  5.1,  1.8]]) #类型是numpy.array
>>> iris.data.size  
600  #共600/4=150个样本
>>> iris.target_names
array(['setosa', 'versicolor', 'virginica'], 
      dtype='|S10')
>>> iris.target
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,....., 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ......, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
>>> iris.target.size
150
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(iris.data, iris.target)
>>> clf.predict(iris.data[0])
array([0])   # 预测正确
>>> clf.predict(iris.data[149])
array([2])   # 预测正确
>>> data = numpy.array([6,4,6,2])
>>> clf.predict(data)
array([2])  # 预测结果很合理
```

多项式模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```

值得注意的是，多项式模型在训练一个数据集结束后可以继续训练其他数据集而无需将两个数据集放在一起进行训练。在sklearn中，MultinomialNB()类的partial_fit()方法可以进行这种训练。这种方式特别适合于训练集大到内存无法一次性放入的情况。

在第一次调用`partial_fit()`时需要给出所有的分类标号。

```python
>>> import numpy
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB() 
>>> clf.partial_fit(numpy.array([1,1]), numpy.array(['aa']), ['aa','bb'])
GaussianNB()
>>> clf.partial_fit(numpy.array([6,1]), numpy.array(['bb']))
GaussianNB()
>>> clf.predict(numpy.array([9,1]))
array(['bb'], 
      dtype='|S2')
```

伯努利模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(2, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```



朴素贝叶斯的总结就到这里为止。

## 7. K-近邻算法(KNN)

### 简介

> k近邻（KNN)是一种基本分类与回归方法。

其思路如下：给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的$k$个训练实例，然后统计最近的$k$个训练实例中**所属类别计数最多的那个类**，就是新实例的类。其流程如下所示：

1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序；
3. 选前$k$个最小距离的样本；
4. 根据这$k$个样本的标签进行投票，得到最后的分类类别；

KNN的特殊情况是$k=1$的情况，称为最近邻算法。对输入的实例点（特征向量）$x$，最近邻法将训练数据集中与$x$最近邻点的类作为其类别。

### 三要素

1. $k$值的选择
2. 距离的度量（常见的距离度量有欧式距离，马氏距离）
3. 分类决策规则（多数表决规则）

### k值的选择

1. **$k$值越小表明模型越复杂，更加容易过拟合**
2. 但是**$k$值越大，模型越简单**，如果$k=N$的时候就表明无论什么点都是训练集中类别最多的那个类

> 所以一般$k$会取一个**较小的值，然后用过交叉验证来确定**
> 这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后$k$分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的$k$

### 距离的度量

KNN算法使用的距离一般是欧式距离，也可以是更一般的$L_p$距离或者马氏距离，其中$L_p$距离定义如下：
$$
L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |^p)^{\frac{1}{p}}
$$
这里$x_i = (x_i^{(1)}, x_i^{(2)},...x_i^{(n)})^T, x_j = (x_j^{(1)}, x_j^{(2)}, ... , x_j^{(n)})^T$，然后$p \ge 1$。

当$p=2$，称为欧式距离，即
$$
L_2(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |^2)^{\frac{1}{2}}
$$
当$p=1$，称为曼哈顿距离，即
$$
L_1(x_i, x_j) = \sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |
$$
当$p = \infty$，它是各个坐标距离的最大值，即
$$
L_\infty(x_i, x_j) =max_l |x_i^{(l)} - x_j^{(l)} |
$$
马氏距离如下定义：

![这里写图片描述](http://img.blog.csdn.net/20170219212021208?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### KNN的回归

在找到最近的$k$个实例之后，可以计算这$k$个实例的平均值作为预测值。或者还可以给这$k$个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。

### 优缺点

#### 优点

1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
2. 可用于非线性分类；
3. 训练时间复杂度为$O(n)$；
4. 准确度高，对数据没有假设，对outlier不敏感；

#### 缺点

1. 计算量大；
2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
3. 需要大量的内存；

### KD树

> KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）

### 构造KD树

在k维的空间上循环找子区域的中位数进行划分的过程。
假设现在有K维空间的数据集$T={x_1,x_2,x_3,…x_n},x_i={a_1,a_2,a_3..a_k}$

1. 首先构造根节点，以坐标$a_1$的中位数$b$为切分点，将根结点对应的矩形局域划分为两个区域，区域1中$a_1<b$,区域2中$a_1>b$
2. 构造叶子节点，分别以上面两个区域中$a_2$的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果$a_2=中位数$，则$a_2$的实例落在切分面）
3. 不断重复2的操作，深度为$j$的叶子节点划分的时候，索取的$a_i$ 的$i=j%k+1$，直到两个子区域没有实例时停止

### KD树的搜索

1. 首先从根节点开始递归往下找到包含$x$的叶子节点，每一层都是找对应的$x_i$
2. 将这个叶子节点认为是当前的“**近似最近点**”
3. 递归向上回退，如果以$x$圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界**相交**，则说明另一半子区域中存在与$x$**更近的点**，则进入另一个子区域中查找该点并且更新”近似最近点“
4. 重复3的步骤，直到**另一子区域与球体不相交或者退回根节点**
5. 最后更新的”近似最近点“与$x$真正的最近点

### KD树进行KNN查找

通过KD树的搜索找到与搜索目标最近的点，这样KNN的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。

### KD树搜索的复杂度

当实例随机分布的时候，搜索的复杂度为$log(N)$，$N$为实例的个数，KD树更加适用于**实例数量远大于空间维度**的KNN搜索，如果实例的**空间维度与实例个数差不多**时，它的效率基于等于**线性扫描**。

### 代码实现

使用`sklearn`的简单代码例子：

```python
#Import Library
from sklearn.neighbors import KNeighborsClassifier

#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create KNeighbors classifier object model 

KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5

# Train the model using the training sets and check score
model.fit(X, y)

#Predict Output
predicted= model.predict(x_test)
```

最后，**在用KNN前你需要考虑到：**

- KNN的计算成本很高
- 所有特征应该**标准化数量级**，否则数量级大的特征在计算距离上会有偏移。
- 在进行KNN前**预处理数据**，例如去除异常值，噪音等。

## 8 K-均值算法

### 简介



