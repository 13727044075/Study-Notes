# 机器学习算法总结2

------

## 6. 朴素贝叶斯

参考文章：

* 《统计学习方法》

* [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)

* [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)

* [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)


### 简介

> **朴素贝叶斯**是基于贝叶斯定理与特征条件独立假设的分类方法。

**贝叶斯定理**是基于条件概率来计算的，条件概率是在已知事件B发生的前提下，求解事件A发生的概率，即$P(A|B)=\frac{P(AB)}{P(B)}$，而贝叶斯定理则可以通过$P(A|B)$来求解$P(B|A)$：
$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$
其中分母$P(A)$可以根据全概率公式分解为：$P(A)=\sum_{i=1}^n P(B_i)P(A|B_i)$

而**特征条件独立假设**是指假设各个维度的特征$x_1,x_2,...,x_n$互相独立，则条件概率可以转化为：
$$
P(x|y_{k})=P(x_{1},x_{2},...,x_{n}|y_{k})=\prod_{i=1}^{n}P(x_{i}|y_{k})
$$

朴素贝叶斯分类器可表示为：
$$
f(x)=argmax_{y_{k}} P(y_{k}|x)=argmax_{y_{k}} \frac{P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})}{\sum_{k}P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})}
$$
而由于对上述公式中分母的值都是一样的，所以可以忽略分母部分，即可以表示为：
$$
f(x)=argmax P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})
$$
这里$P(y_k)$是先验概率，而$P(y_k|x)$则是后验概率，朴素贝叶斯的目标就是最大化后验概率，这等价于期望风险最小化。

### 参数估计

#### 极大似然估计

朴素贝叶斯的学习意味着估计$P(y_k)$和$P(x_i|y_k)$,可以通过**极大似然估计**来估计相应的概率。

![这里写图片描述](http://img.blog.csdn.net/20170218105353279?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

如上图所示，分别是$P(y_k)$和$P(x_i|y_k)$的极大似然估计。

当求解完上述两个概率，就可以对测试样本使用朴素贝叶斯分类算法来预测其所属于的类别，简单总结的算法流程如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170218105758081?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 贝叶斯估计/多项式模型

用**极大似然估计可能会出现所要估计的概率值为0**的情况，这会影响到后验概率的计算，使分类产生偏差。解决这个问题的办法是使用**贝叶斯估计**，也被称为多项式模型。

当**特征是离散的时候，使用多项式模型**。多项式模型在计算先验概率$P(y_k)$和条件概率$P(x_i|y_k)$时，会做一些**平滑处理**，具体公式为：
$$
P(y_k)=\frac{N_{y_k}+α}{N+kα}
$$

> $N$是总的样本个数，$k$是总的类别个数，$N_{y_k}$是类别为$y_k$的样本个数，$α$是平滑值。

$$
P(x_i|y_k) = \frac{N_{y_k,x_i} + \alpha}{N_{y_k}+n\alpha}
$$

> $N_{y_k}$是类别为$y_k$的样本个数，$n$是特征的维数，$N_{y_k,x_i}$是类别为$y_k$的样本中，第$i$维特征的值是$x_i$的样本个数，$α$是平滑值。

当$α=1$时，称作**Laplace平滑**，当$0<α<1$时，称作**Lidstone**平滑，α=0时不做平滑。

如果不做平滑，当某一维特征的值$x_i$没在训练样本中出现过时，会导致$P(x_i|y_k)=0$，从而导致后验概率为0。加上平滑就可以克服这个问题。

#### 高斯模型

当特征是连续变量的时候，运用多项式模型会导致很多$P(x_i|y_k) = 0$（不做平滑的情况下），即使做平滑，所得到的条件概率也难以描述真实情况，所以处理连续变量，应该采用高斯模型。

高斯模型是假设每一维特征都服从高斯分布（正态分布）：
$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$
$\mu_{y_{k},i}$表示类别为$y_k$的样本中，第$i$维特征的均值；
$\sigma_{y_{k},i}^{2}$表示类别为$y_k$的样本中，第$i$维特征的方差。

#### 伯努利模型

与多项式模型一样，伯努利模型适用于**离散特征**的情况，所不同的是，**伯努利模型中每个特征的取值只能是1和0**(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).

伯努利模型中，条件概率$P(x_i|y_k)$的计算方式是：

当特征值$x_i$为1时，$P(x_i|y_k)=P(x_i=1|y_k)$；

当特征值$x_i$为0时，$P(x_i|y_k)=1−P(x_i=1|y_k)$；

### 工作流程

1. 准备阶段
   确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。
2. 训练阶段
   计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计
3. 应用阶段
   使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别

### 属性特征

1. 特征为离散值时直接统计即可（表示统计概率）
2. 特征为连续值的时候假定特征符合高斯分布，则有

$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$

### 优缺点

#### 优点

1. 对小规模的数据表现很好，适合多分类任务，适合增量式训练。

#### 缺点
2. 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。


### 代码实现

下面是使用`sklearn`的代码例子，分别实现上述三种模型,例子来自[朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)。
下面是高斯模型的实现

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> iris.feature_names  # 四个特征的名字
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
>>> iris.data
array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2],
       [ 5.4,  3.9,  1.7,  0.4],
       [ 4.6,  3.4,  1.4,  0.3],
       [ 5. ,  3.4,  1.5,  0.2],
       ......
       [ 6.5,  3. ,  5.2,  2. ],
       [ 6.2,  3.4,  5.4,  2.3],
       [ 5.9,  3. ,  5.1,  1.8]]) #类型是numpy.array
>>> iris.data.size  
600  #共600/4=150个样本
>>> iris.target_names
array(['setosa', 'versicolor', 'virginica'], 
      dtype='|S10')
>>> iris.target
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,....., 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ......, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
>>> iris.target.size
150
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(iris.data, iris.target)
>>> clf.predict(iris.data[0])
array([0])   # 预测正确
>>> clf.predict(iris.data[149])
array([2])   # 预测正确
>>> data = numpy.array([6,4,6,2])
>>> clf.predict(data)
array([2])  # 预测结果很合理
```

多项式模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```

值得注意的是，多项式模型在训练一个数据集结束后可以继续训练其他数据集而无需将两个数据集放在一起进行训练。在sklearn中，MultinomialNB()类的partial_fit()方法可以进行这种训练。这种方式特别适合于训练集大到内存无法一次性放入的情况。

在第一次调用`partial_fit()`时需要给出所有的分类标号。

```python
>>> import numpy
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB() 
>>> clf.partial_fit(numpy.array([1,1]), numpy.array(['aa']), ['aa','bb'])
GaussianNB()
>>> clf.partial_fit(numpy.array([6,1]), numpy.array(['bb']))
GaussianNB()
>>> clf.predict(numpy.array([9,1]))
array(['bb'], 
      dtype='|S2')
```

伯努利模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(2, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```



朴素贝叶斯的总结就到这里为止。

## 7. K-近邻算法(KNN)

### 简介

> k近邻（KNN)是一种基本分类与回归方法。

其思路如下：给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的$k$个训练实例，然后统计最近的$k$个训练实例中**所属类别计数最多的那个类**，就是新实例的类。其流程如下所示：

1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序；
3. 选前$k$个最小距离的样本；
4. 根据这$k$个样本的标签进行投票，得到最后的分类类别；

KNN的特殊情况是$k=1$的情况，称为最近邻算法。对输入的实例点（特征向量）$x$，最近邻法将训练数据集中与$x$最近邻点的类作为其类别。

### 三要素

1. $k$值的选择
2. 距离的度量（常见的距离度量有欧式距离，马氏距离）
3. 分类决策规则（多数表决规则）

### k值的选择

1. **$k$值越小表明模型越复杂，更加容易过拟合**
2. 但是**$k$值越大，模型越简单**，如果$k=N$的时候就表明无论什么点都是训练集中类别最多的那个类

> 所以一般$k$会取一个**较小的值，然后用过交叉验证来确定**
> 这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后$k$分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的$k$

### 距离的度量

KNN算法使用的距离一般是欧式距离，也可以是更一般的$L_p$距离或者马氏距离，其中$L_p$距离定义如下：
$$
L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |^p)^{\frac{1}{p}}
$$
这里$x_i = (x_i^{(1)}, x_i^{(2)},...x_i^{(n)})^T, x_j = (x_j^{(1)}, x_j^{(2)}, ... , x_j^{(n)})^T$，然后$p \ge 1$。

当$p=2$，称为欧式距离，即
$$
L_2(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |^2)^{\frac{1}{2}}
$$
当$p=1$，称为曼哈顿距离，即
$$
L_1(x_i, x_j) = \sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |
$$
当$p = \infty$，它是各个坐标距离的最大值，即
$$
L_\infty(x_i, x_j) =max_l |x_i^{(l)} - x_j^{(l)} |
$$
马氏距离如下定义：

![这里写图片描述](http://img.blog.csdn.net/20170219212021208?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### KNN的回归

在找到最近的$k$个实例之后，可以计算这$k$个实例的平均值作为预测值。或者还可以给这$k$个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。

### 优缺点

#### 优点

1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
2. 可用于非线性分类；
3. 训练时间复杂度为$O(n)$；
4. 准确度高，对数据没有假设，对**异常值**不敏感；

#### 缺点

1. 计算量大；
2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
3. 需要大量的内存；

### KD树

> KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）

### 构造KD树

在k维的空间上循环找子区域的中位数进行划分的过程。
假设现在有K维空间的数据集$T={x_1,x_2,x_3,…x_n},x_i={a_1,a_2,a_3..a_k}$

1. 首先构造根节点，以坐标$a_1$的中位数$b$为切分点，将根结点对应的矩形局域划分为两个区域，区域1中$a_1<b$,区域2中$a_1>b$
2. 构造叶子节点，分别以上面两个区域中$a_2$的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果$a_2=中位数$，则$a_2$的实例落在切分面）
3. 不断重复2的操作，深度为$j$的叶子节点划分的时候，索取的$a_i$ 的$i=j%k+1$，直到两个子区域没有实例时停止

### KD树的搜索

1. 首先从根节点开始递归往下找到包含$x$的叶子节点，每一层都是找对应的$x_i$
2. 将这个叶子节点认为是当前的“**近似最近点**”
3. 递归向上回退，如果以$x$圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界**相交**，则说明另一半子区域中存在与$x$**更近的点**，则进入另一个子区域中查找该点并且更新”近似最近点“
4. 重复3的步骤，直到**另一子区域与球体不相交或者退回根节点**
5. 最后更新的”近似最近点“与$x$真正的最近点

### KD树进行KNN查找

通过KD树的搜索找到与搜索目标最近的点，这样KNN的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。

### KD树搜索的复杂度

当实例随机分布的时候，搜索的复杂度为$log(N)$，$N$为实例的个数，KD树更加适用于**实例数量远大于空间维度**的KNN搜索，如果实例的**空间维度与实例个数差不多**时，它的效率基于等于**线性扫描**。

### 代码实现

使用`sklearn`的简单代码例子：

```python
#Import Library
from sklearn.neighbors import KNeighborsClassifier

#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create KNeighbors classifier object model 

KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5

# Train the model using the training sets and check score
model.fit(X, y)

#Predict Output
predicted= model.predict(x_test)
```

最后，**在用KNN前你需要考虑到：**

- KNN的计算成本很高
- 所有特征应该**标准化数量级**，否则数量级大的特征在计算距离上会有偏移。
- 在进行KNN前**预处理数据**，例如去除异常值，噪音等。

## 8 K-均值算法

## 参考自：

* 《机器学习》
* [机器学习&数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）](http://www.cnblogs.com/tornadomeet/p/3395593.html)
* [K-Means Clustering](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/)
* [斯坦福大学公开课 ：机器学习课程](http://v.163.com/special/opencourse/machinelearning.html)

### 简介

> K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据集聚类成不同的组。

K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为：

1. 首先选择**K**个随机的点，称其为**聚类中心**
2. 对于数据集中的每一个数据，按照距离**K**个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一个类
3. 计算每一个组的**平均值**，将该组所关联的中心点移动到平均值的位置
4. 重复步骤2-3，直到中心点不再变化

这个过程中分两个主要步骤，第一个就是第二步，将训练集中的样本点根据其与聚类中心的距离，分配到距离最近的聚类中心处，接着第二个就是第三步，更新类中心，做法是计算每个类的所有样本的平均值，然后将这个平均值作为新的类中心值，接着继续这两个步骤，直到达到终止条件，一般是指达到设定好的迭代次数。

当然在这个过程中可能遇到有聚类中心是没有分配数据点给它的，通常的一个做法是**删除这种聚类中心，或者是重新选择聚类中心，保证聚类中心数还是初始设定的K个**。

### 优化目标

K-均值最小化问题，就是**最小化所有的数据点与其所关联的聚类中心之间的距离之和**，因此K-均值的代价函数（又称为**畸变函数**）为： 
$$
J(c^{(1)},c^{(2)},…,c^{(m)},μ_1,μ_2,…,μ_m)=\frac{1}{m}∑_{i=1}^m||x^(i)−μ_{c^(i)}||^2
$$


其中$\mu_{c^{(i)}}$代表与$x^{(i)}$最近的聚类中心点。

所以我们的优化目标是找出是的代价函数最小的$c^{(1)},c^{(2)},\ldots,c^{(m)}和\mu_1,\mu_2,\ldots,\mu_m$: 
$$
min_{c^{(1)},c^{(2)},\ldots,c^{(m)}, \mu_1,\mu_2,\ldots,\mu_m}J(c^{(1)},c^{(2)},\ldots,c^{(m)},\mu_1,\mu_2,\ldots,\mu_m)
$$
回顾K-均值迭代算法的过程可知，第一个循环就是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小$μ_i$引起的代价，因此，**迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。**

### 随机初始化

在运行K-均值算法之前，首先需要随机初始化所有的聚类中心点，做法如下：

1. 首先应该选择$K<m$,即**聚类中心点的个数要小于所有训练集实例的数量**
2. 随机选择$K$个训练实例，然后令$K$个聚类中心分别于这K个训练实例相等

K-均值的一个问题在于，**它有可能会停留在一个局部最小值处，而这取决于初始化的情况。**

为了解决这个问题，通常需要**多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。**这种方法在**K较小（2-10）**的时候还是可行的，但是如果K较大，这种做法可能不会有明显地改善。

### 优缺点

#### 优点

1. k-means算法是解决聚类问题的一种经典算法，**算法简单、快速**。
2. 对处理大数据集，该算法是**相对可伸缩的和高效率**的，因为它的复杂度大约是$O(nkt)$，其中$n$是所有对象的数目，$k$是簇的数目,$t$是迭代的次数。通常$k<<n$。这个算法通常**局部收敛**。
3. 算法尝试找出**使平方误差函数值最小的k个划分**。当簇是密集的、球状或团状的，且**簇与簇之间区别明显**时，聚类效果较好。

#### 缺点

1. k-平均方法只有在簇的**平均值被定义**的情况下才能使用，且对有些分类属性的数据不适合。
2. 要求用户必须事先给出要生成的簇的数目$k$。
3. **对初值敏感**，对于不同的初始值，可能会导致不同的聚类结果。
4. **不适合于发现非凸面形状的簇，或者大小差别很大的簇**。
5. 对于**"噪声"和孤立点数据敏感**，少量的该类数据能够对平均值产生极大影响。

### 代码实现

代码参考自[K-Means Clustering](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/)。

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2016/10/21 16:35
@Author  : cai

实现 K-Means 聚类算法
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
import os

# 寻址最近的中心点
def find_closest_centroids(X, centroids):
    m = X.shape[0]
    k = centroids.shape[0]
    idx = np.zeros(m)

    for i in range(m):
        min_dist = 1000000
        for j in range(k):
            # 计算每个训练样本和中心点的距离
            dist = np.sum((X[i, :] - centroids[j, :]) ** 2)
            if dist < min_dist:
                # 记录当前最短距离和其中心的索引值
                min_dist = dist
                idx[i] = j

    return idx

# 计算聚类中心
def compute_centroids(X, idx, k):
    m, n = X.shape
    centroids = np.zeros((k, n))

    for i in range(k):
        indices = np.where(idx == i)
        # 计算下一个聚类中心，这里简单的将该类中心的所有数值求平均值作为新的类中心
        centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel()

    return centroids

# 初始化聚类中心
def init_centroids(X, k):
    m, n = X.shape
    centroids = np.zeros((k, n))
    # 随机初始化 k 个 [0,m]的整数
    idx = np.random.randint(0, m, k)

    for i in range(k):
        centroids[i, :] = X[idx[i], :]

    return centroids

# 实现 kmeans 算法
def run_k_means(X, initial_centroids, max_iters):
    m, n = X.shape
    # 聚类中心的数目
    k = initial_centroids.shape[0]
    idx = np.zeros(m)
    centroids = initial_centroids

    for i in range(max_iters):
        idx = find_closest_centroids(X, centroids)
        centroids = compute_centroids(X, idx, k)

    return idx, centroids

dataPath = os.path.join('data', 'ex7data2.mat')
data = loadmat(dataPath)
X = data['X']

initial_centroids = init_centroids(X, 3)
# print(initial_centroids)
# idx = find_closest_centroids(X, initial_centroids)
# print(idx)

# print(compute_centroids(X, idx, 3))

idx, centroids = run_k_means(X, initial_centroids, 10)
# 可视化聚类结果
cluster1 = X[np.where(idx == 0)[0], :]
cluster2 = X[np.where(idx == 1)[0], :]
cluster3 = X[np.where(idx == 2)[0], :]

fig, ax = plt.subplots(figsize=(12, 8))
ax.scatter(cluster1[:, 0], cluster1[:, 1], s=30, color='r', label='Cluster 1')
ax.scatter(cluster2[:, 0], cluster2[:, 1], s=30, color='g', label='Cluster 2')
ax.scatter(cluster3[:, 0], cluster3[:, 1], s=30, color='b', label='Cluster 3')
ax.legend()
plt.show()

# 载入一张测试图片，进行测试
imageDataPath = os.path.join('data', 'bird_small.mat')
image = loadmat(imageDataPath)
# print(image)

A = image['A']
print(A.shape)

# 对图片进行归一化
A = A / 255.

# 重新调整数组的尺寸
X = np.reshape(A, (A.shape[0] * A.shape[1], A.shape[2]))
# 随机初始化聚类中心
initial_centroids = init_centroids(X, 16)
# 运行聚类算法
idx, centroids = run_k_means(X, initial_centroids, 10)

# 得到最后一次的最近中心点
idx = find_closest_centroids(X, centroids)
# map each pixel to the centroid value
X_recovered = centroids[idx.astype(int), :]
# reshape to the original dimensions
X_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2]))

# plt.imshow(X_recovered)
# plt.show()
```

完整代码例子和数据可以查看[Kmeans练习代码](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/kMeansPractise.py)。

## 9 提升方法

参考自：

* 《统计学习方法》
* [浅谈机器学习基础（上）](http://www.jianshu.com/p/ed9ae5385b89?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io#)

### 简介

> 提升方法(boosting)是一种常用的统计学习方法，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提供分类的性能。

### boosting和bagging

**boosting和bagging**都是**集成学习（ensemble learning）**领域的基本算法，**boosting和bagging使用的多个分类器的类型是一致的。**

**bagging**也叫**自举汇聚法（bootstrap aggregating）**，比如原数据集中有N个样本，我们每次从原数据集中有放回的抽取，抽取N次，就得到了一个新的有N个样本的数据集，然后我们抽取S个N次，就得到了S个有N个样本的新数据集，然后拿这S个数据集去训练S个分类器，之后应用这S个分类器进行分类，选择分类器投票最多的类别作为最后的分类结果。

**boosting与bagging的区别在于**：

- bagging通过**有放回的抽取**得到了S个数据集，而boosting用的始终是**原数据集**，但是**样本的权重会发生改变。**
- **boosting对分类器的训练是串行的**，每个新分类器的训练都会受到上一个分类器分类结果的影响。
- **bagging里面各个分类器的权重是相等的，但是boosting不是**，每个分类器的权重代表的是其对应分类器在上一轮分类中的成功度。

**AdaBoost是boosting方法中最流行的版本**

### AdaBoosts算法

> **AdaBoost（adaptive boosting）是元算法，通过组合多个弱分类器来构建一个强分类器。我们为训练数据中的每一个样本都赋予其一个权重，这些权重构成了向量$D$，一开始，这些权重都初始化成相等值，然后每次添加一个弱分类器对样本进行分类，从第二次分类开始，将上一次分错的样本的权重提高，分对的样本权重降低，持续迭代。此外，对于每个弱分类器而言，每个分类器也有自己的权重，取决于它分类的加权错误率，加权错误率越低，则这个分类器的权重值$α$越高，最后综合多个弱分类器的分类结果和其对应的权重$α$得到预测结果，AdaBoost是最好的监督学习分类方法之一。**

其算法过程如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170222110646742?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中，注意：

![这里写图片描述](http://img.blog.csdn.net/20170222110716909?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 训练误差分析

**AdaBoost算法的最基本性质是在学习过程中不断减小训练误差**，对训练误差的上界有如下定理：

> 定理1：AdaBoost最终分类器的训练误差界为：
> $$
> \frac{1}{N} \sum_{i=1}^N I(G(x_i) \neq y_i) \le \frac{1}{N} \sum_i exp(-y_i f(x_i)) = \prod_m Z_m
> $$
> 定理2：二类分类问题
> $$
> \prod_{m=1}^M Z_m = \prod_{m=1}^M [2\sqrt{e_m(1-e_m)}] = \prod_{m=1}^M [\sqrt{1-4\gamma_m^2} \le exp(-2\sum_{m=1}^M \gamma_m^2)
> $$
>

#### 算法解释

**AdaBoost**算法还可以解释为模型是加法模型，损失函数是指数函数，学习算法是前向分步算法的二类分类学习方法。

加法模型是形如$f(x) = \sum_{i=1}^M \beta_m b(x; \gamma_m)$的函数形式，其中$b(x;\gamma_m)$是基函数，而$\beta_m$是基函数的系数，$\gamma_m$是基函数的参数。对于**AdaBoost**算法，其基本分类器的线性组合为$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$正是一个加法模型。

**AdaBoost**算法的损失函数是指数函数，公式为$E = \sum_{i=1}^N exp(-y_i G_m(x_i))$。

此外，经过$m$轮迭代可以得到$f_m(x) = f_{m-1}(x) + \alpha_m G_m(x)$。而前向分步算法的过程如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170222162813661?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

通过上述步骤，前向分步算法将同时求解从$m=1$到$M$所有参数$\beta_m, \gamma_m$的优化问题简化为逐步求解各个$\beta_m, \gamma_m$的优化问题。

### 优缺点

#### 优点

1. 泛化误差低
2. 容易实现，分类准确率较高，没有太多参数可以调

#### 缺点

对异常值比较敏感

