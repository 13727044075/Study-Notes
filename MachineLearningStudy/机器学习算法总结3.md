# 机器学习算法总结3

------

## 11 EM算法

参考自

* 《统计学习方法》

* [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)

* [从最大似然到EM算法浅解](http://blog.csdn.net/zouxy09/article/details/8537620)

* [（EM算法）The EM Algorithm](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html)

### 简介

> EM算法，即期望极大算法，用于**含有隐变量的概率模型的极大似然估计或极大后验概率估计**，它一般分为两步：**第一步求期望(E),第二步求极大(M)。**

如果概率模型的变量都是观测变量，那么给定数据之后就可以直接使用极大似然法或者贝叶斯估计模型参数。
但是当模型含有隐含变量的时候就不能简单的用这些方法来估计，EM就是一种含有隐含变量的概率模型参数的极大似然估计法。

应用到的地方：混合高斯模型、混合朴素贝叶斯模型、因子分析模型

### 算法推导

![这里写图片描述](http://img.blog.csdn.net/20170224165311795?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170224165327407?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

上述公式相当于决定了$L(\theta)​$的下界，而**EM**算法实际上就是通过不断求解下界的极大化来逼近对数似然函数极大化的算法。

![这里写图片描述](http://img.blog.csdn.net/20170224165456956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 算法流程

算法流程如下所示：

### ![这里写图片描述](http://img.blog.csdn.net/20170224165544390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 收敛性

收敛性部分可以主要看[（EM算法）The EM Algorithm](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html)的推导，最终可以推导得到如下公式：
$$
L(\theta^{(t+1)}) \ge \sum_i \sum_{z^{i}} Q_i^{(t)}(z^{(i)}) log \frac{p(x^{(i)}, z^{(i)} ; \theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})} \\
			\ge  \sum_i \sum_{z^{i}} Q_i^{(t)}(z^{(i)}) log \frac{p(x^{(i)}, z^{(i)} ; \theta^{(t)})}{Q_i^{(t)}(z^{(i)})} \\
			= L(\theta^{(t)})
$$

### 特点

1. 最大优点是简单性和普适性
2. **EM**算法不能保证找到全局最优点，在应用中，通常选取几个不同的初值进行迭代，然后对得到的几个估计值进行比较，从中选择最好的
3. **EM**算法对初值是敏感的，不同初值会得到不同的参数估计值

### 使用例子

**EM算法**一个常见的例子就是**GMM模型**，即高斯混合模型。而高斯混合模型的定义如下：

> 高斯混合模型是指具有如下形式的概率分布模型：
> $$
> P(y| \theta) = \sum_{k=1}^K \alpha_k \phi(y | \theta_k) \\
> 其中， \alpha_k 是系数，\alpha_k \ge 0, \sum_{k=1}^K \alpha_k = 1; \phi(y|\theta_k)是高斯分布密度，\theta_k = (\mu_k, \sigma_k^2), \\
> \phi(y|\theta_k) = \frac{1}{\sqrt{2 \pi} \sigma_k} exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})
> $$
>

$\phi(y|\theta_k)$称为第$k$个分模型。

每个样本都有可能由$k$个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（$k$个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。

GMM的E步公式如下（计算每个样本对应每个高斯的概率）：

 ![这里写图片描述](http://img.blog.csdn.net/20170224182036789?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

更具体的计算公式为：

![这里写图片描述](http://img.blog.csdn.net/20170224182218590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

M步公式如下（计算每个高斯的**比重，均值，方差**这3个参数）：

 ![这里写图片描述](http://img.blog.csdn.net/20170224182243247?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

　　

## 12 

参考自



### 简介

