## 深度学习+项目问题

#### 1. 项目后续如何优化？收获是什么？

#### 2. 单张图片测试的速度？

使用GPU，一张图片的测试速度是0.032s

#### 3. CNN中对结果影响比较大的参数？

学习率、batch-size、卷积核大小和数量、网络层数。

#### 4. 如何进行fintuning，层数不相同的情况

层数不同也可以调试，一般将不想调试的网络层修改名字即可。

#### 5. 反卷积网络是否有了解过？

#### 6. 卷积核大小的选择

* 更小的卷积核可以减小参数，节省运算开销，虽然训练时间会变长，但是总体上参数和预测时间会减少，同时小的卷积核会提取到图片中局部的特征，而大的卷积核则可以提取到更加全局的信息。
* 两个3\*3的卷积核可以达到一个5*5卷积核的作用，并且还能减少参数和计算时间。
#### 7. 1\*1 卷积核的作用

* 实现跨通道的交互和信息整合，实现多个feature map的线性组合，实现跨通道的信息整合
* 进行卷积核通道数的降维和升维
* 实现平滑操作
* 可以在保持feature map 尺寸不变（即不损失分辨率）的前提下大幅增加非线性特性，把网络做得很deep。

#### 8. CNN对图像分类效果好的原因

图像存在局部相关性，而卷积运算可以获取这种空间相关性。

#### 9.BatchNorm层的作用，为什么使用？用在哪里？什么时候使用？为什么能产生这样的效果？

（1）作用有两个，**一个是逐层尺度归一，避免梯度消失和溢出；其次是加快收敛速度，可以防止过拟合**；
（2）防止梯度弥散，在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。
（3）BN应作用在非线性映射前，即对做规范化。
（4）在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。
（5）原因在于神经网络学习过程本质就是为了**学习数据分布**，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。
对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch  Normalization，这个牛逼算法的诞生。

#### 10.  全卷积网络介绍
FCN将传统CNN中的全连接层转化成一个个的卷积层。在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个类别的概率。FCN将这3层表示为卷积层，卷积核的大小(通道数，宽，高)分别为（4096,1,1）、（4096,1,1）、（1000,1,1）。所有的层都是卷积层，故称为全卷积网络。 
可以发现，经过多次卷积（还有pooling）以后，得到的图像越来越小,分辨率越来越低（粗略的图像），那么FCN是如何得到图像中每一个像素的类别的呢？为了从这个分辨率低的粗略图像恢复到原图的分辨率，FCN使用了上采样。例如经过5次卷积(和pooling)以后，图像的分辨率依次缩小了2，4，8，16，32倍。对于最后一层的输出图像，需要进行32倍的上采样，以得到原图一样的大小。
这个上采样是通过反卷积（deconvolution）实现的。对第5层的输出（32倍放大）反卷积到原图大小，得到的结果还是不够精确，一些细节无法恢复。于是Jonathan将第4层的输出和第3层的输出也依次反卷积，分别需要16倍和8倍上采样，结果就精细一些了。

与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时也保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。论文中逐像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。

与传统用CNN进行图像分割的方法相比，**FCN有两大明显的优点：一是可以接受任意大小的输入图像，而不用要求所有的训练图像和测试图像具有同样的尺寸。二是更加高效，因为避免了由于使用像素块而带来的重复存储和计算卷积的问题。**
同时FCN的缺点也比较明显：**一是得到的结果还是不够精细**。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。**二是对各个像素进行分类，没有充分考虑像素与像素之间的关系，忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性**。


传统的基于CNN的分割方法：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入用于训练和预测。这种方法有几个缺点：**一是存储开销很大。**例如对每个像素使用的图像块的大小为15x15，然后不断滑动窗口，每次滑动的窗口给CNN进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升。**二是计算效率低下。**相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复。**三是像素块大小的限制了感知区域的大小。**通常像素块的大小比整幅图像的大小小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。
而全卷积网络(FCN)试图从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。

#### 11.BP算法？如果对wx+b的反向计算是wT*Ɛ,为什么是w的转置？

（1）
（2）维度匹配

#### 12.Caffe中im2col函数的作用？

将卷积转为矩阵相乘，加快卷积运算的计算速度。

#### 13. 卷积是如何实现的？
卷积核在卷积计算时没有“翻转”，而是与输入图片做滑动窗口“相关”计算。

#### 14. 动量的作用

计算梯度时考虑历史梯度信息，使随机梯度下降更容易跳出局部最优 ，加速收敛 

#### 15 卷积本质是什么？卷积神经网络本质是什么？

(1)
(2)

#### 16. 项目，与业界最好成绩相比？优化了哪些，或者相比更好的地方在哪里？

#### 17. CNN中，计算速度主要取决于什么？

卷积运算，并且主要是channel这个维度。

#### 18. 如何提高卷积运算速度，比如提高矩阵运算？

#### 19. 手机实现CNN，主要使用哪个实现提速？

使用ARM的汇编指令。

#### 20.如何增大卷积网络的感受野?

增大卷积核，加深网络，加pooling层
感受野的定义参考http://blog.csdn.net/wonengguwozai/article/details/73133737和https://zhuanlan.zhihu.com/p/22627224。

#### 21. 什麽样的资料集不适合用深度学习?
* **数据集太小**，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
* **数据集没有局部相关特性**，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。**对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理**。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

#### 22. Caffe为什么要用protobuff？ 

Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。

对于CNN网络而言，其主要有两部分组成：网络具体结构和网络的具体优化算法及参数。对于框架的使用者而言，用户只需输入两个描述文件即可得到对该网络的优化结果，这无疑是非常方便的。

**Caffe框架选择使用谷歌的开源protobuf工具对这两部分进行描述，解析和存储，这一部分为caffe的实现节省了大量的代码。**

参考文章[caffe数据格式（Google Protocol Buffers）](http://blog.csdn.net/langb2014/article/details/50151831)、[Caffe学习（十）protobuf及caffe.proto解析](http://blog.csdn.net/u012177034/article/details/53944901)、[Caffe 深度学习框架上手教程](http://suanfazu.com/t/caffe/281)。

#### 23. rcnn/fast rcnn/fater rcnn区别？ 

参考文章[RCNN,Fast RCNN,Faster RCNN 总结](http://shartoo.github.io/RCNN-series/)、[如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？](https://www.zhihu.com/question/35887527)、[RCNN-> SPP net -> Fast RCNN -> Faster RCNN](https://zhuanlan.zhihu.com/p/25600546)、[RCNN，Fast-RCNN，Faster-RCNN以及YOLO的区别](https://zhuanlan.zhihu.com/p/27031704)。

![](http://shartoo.github.io/images/blog/rcnn11.png)

R-CNN的流程主要分为四步：

1. 输入图像；
2. 利用选择性搜索（Selective Search）等区域生成算法在输入图像中提取Region Proposal（大概2000个）；
3. 将第一步中产生的每个Region Proposal分别resize后（也即图中的warped region，文章中是归一化为227×227）作为CNN网络的输入；
4. CNN网络提取到经过resize的region proposal的特征送入每一类的SVM分类器，判断是否属于该类；

**FRCNN针对RCNN在训练时是multi-stage pipeline和训练的过程中很耗费时间空间的问题进行改进。它主要是将深度网络和后面的SVM分类两个阶段整合到一起，使用一个新的网络直接做分类和回归。**主要做以下改进:

1. 最后一个卷积层后加了一个ROI pooling layer。ROI pooling layer首先可以将image中的ROI定位到feature map，然后是用一个单层的SPP layer将这个feature map patch池化为固定大小的feature之后再传入全连接层。
2. 损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。

**Fast-RCNN的速度瓶颈在Region proposal上。于是将Region proposal也交给CNN来做，提出了Faster-RCNN。**Fater-RCNN中的region proposal netwrok实质是一个Fast-RCNN，这个Fast-RCNN输入的region proposal的是固定的（把一张图片划分成n*n个区域，每个区域给出9个不同ratio和scale的proposal），输出的是对输入的固定proposal是属于背景还是前景的判断和对齐位置的修正（regression）。Region proposal network的输出再输入第二个Fast-RCNN做更精细的分类和Boundingbox的位置修正。Fater-RCNN速度更快了，而且用VGG net作为feature extractor时在VOC2007上mAP能到73%。

#### 24.  Tensorflow 为什么用图？ 

TensorFlow是用**数据流图(data flow graphs)**技术来进行数值计算的。**数据流图是描述`有向图`中的数值计算过程。**`有向图`中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；`有向图`中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。图中这些`tensors`的`flow`也就是TensorFlow的命名来源。节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。

**图计算在内存使用和运算时间两个方面比较高效。**

#### 25. RELU比Sigmoid，Tanh好的原因？

* 采用sigmoid等函数，算激活函数时（指数运算），**计算量大**，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，**整个过程的计算量节省很多**。
* 对于深层网络，sigmoid函数反向传播时，很容易就会出现**梯度消失**的情况（在sigmoid接近饱和区时，变换太缓慢，**导数趋于0，这种情况会造成信息丢失**，参见 @Haofeng Li  答案的第三点），从而无法完成深层网络的训练。
* Relu会使一部分神经元的输出为0，这样就造成了**网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生**


#### 26. softmax函数计算时候为什么要减去一个最大值？

参考文章[干货|softmax函数计算时候为什么要减去一个最大值？](https://mp.weixin.qq.com/s/2xYgaeLlmmUfxiHCbCa8dQ)

Softmax的计算公式如下：![](https://mmbiz.qpic.cn/mmbiz_png/nJZZib3qIQW4ic1Okdxq8v9u9caRT5eSgqicfHDuU2VE4UkGTq54OluBM25WiaQjsDiaV5iaslqEnEVsInBfO6wTlSqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

当我们运算比较小的值的时候是不会有什么问题的，但是如果运算的值比较大的时候，比如很大或很小的时候，朴素的直接计算会**上溢出或下溢出**，从而导致严重问题。

举个例子，对于[3,1,-3]，直接计算是可行的，我们可以得到(0.88,0.12,0)。

但对于[1000,1000,1000]，却并不可行，我们会得到**inf(这也是深度学习训练过程常见的一个错误，看了本文之后，以后出现inf的时候，至少可以考虑softmax运算的上溢和下溢)**；对于[-1000,-999,-1000]，还是不行，我们会得到-inf。

这是因为你的浮点数只有64位，在计算指数函数的环节，exp{1000} =inf，会发生上溢出；exp{-1000} =0，会发生下溢出。

**解决的办法如下：**

![](https://mmbiz.qpic.cn/mmbiz_png/nJZZib3qIQW4ic1Okdxq8v9u9caRT5eSgqBjOtib8gMCvpMPiafAEAAbVia0gic8OnuK9UQcYhzSSzNdHQ53Njbd5pIA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)


对任意a都成立，这意味着我们可以自由地调节指数函数的指数部分，一个典型的做法是取所有数据中的最大值：$a=max{x_1,x_2.....x_n}$

**这可以保证指数最大不会超过0，于是你就不会上溢出。即便剩余的部分下溢出了，加了a之后，你也能得到一个合理的值。**

证明：

证明softmax不受输入的常数偏移影响，即

**softmax(x)=softmax(x+c)**

也就是证明加了偏移c之后，对整个softmax层的作用不起影响。如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/nJZZib3qIQW4ic1Okdxq8v9u9caRT5eSgqZc1DAoUtHWy2xSlJnBeibNVmRIvH9aT1RFAJqliaDttkHjTg3YAkc7kA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

#### 27. 过拟合和欠拟合的定义。避免过拟合的方法。

**过拟合**：模型复杂度过高，或者训练过度，模型拟合数据分布的同时也拟合了噪声，导致模型在训练集上拟合效果很好，训练误差很低，但在测试集上误差很大，不能有效预测新样本；
**欠拟合**：模型复杂度过低，或者训练数据少，不能很好拟合训练数据，训练误差大；

**避免过拟合的途径：**
1. 根据大数定理，当数据量足够大时，模型将会无限逼近实际，即数据增强；
2. 减少参数数量，降低模型的复杂度，剔除和模型不太相关的参数；
3. 对学习参数进行**正则化**，约束参数的收敛空间，提高模型的泛化能力；
4. 在训练时，避免因训练过度导致的模型过拟合；


#### 28. 卷积层的参数量计算

计算公式是 $parameters = inputSize * kernelWidth * kernelHeight * outputSize$。也就是卷积核大小\*卷积核个数（也就是输出特征图个数）*卷积层的深度（也就是输入通道）。

------

## 机器学习算法

#### 1. 随机森林的原理？

#### 2. 随机森林如何避免过拟合，而决策树会过拟合的原因

#### 3. 决策树特征选择的方法？

#### 4. 决策树，如果特征值是连续的如何选择？

C4.5算法。首先对特征值进行升序排序，然后使用二分法，即寻找相邻两个特征值的中点作为分裂点，如1,2,3，可以选择1.5和2.5作为分裂点，选择标准是信息增益，选择信息增益最大的分裂点，然后再计算最佳分裂点的信息增益率作为该特征的信息增益率。

 

#### 5. 决策树，做回归时使用的准则是？

平方误差最小化

#### 6. KNN的三个注意点，K过大和过小时的偏差和方差？

（1）K值选择，距离度量和分类决策规则；

（2）k过小的时候，偏差小而方差大，容易过拟合；k过大时，偏差大，方差小

#### 7. LR使用L1正则化，再使用梯度下降，会出现什么问题？



#### 8. LR的损失函数，为什么使用这个？

这是似然损失函数，使用极大似然估计求解；

参数估计方法除了极大似然估计，还有EM算法、点估计。点估计是计算样本的均值，使用均值作为参数估计。

 

#### 9 随机森林需要剪枝吗？

不用，一般很多的决策树算法都有一个很重要的步骤-剪枝，这里不需要这样做，因为之前的两个随机采样的过程保证了随机性，即对数据的随机采样和特征的随机采样，就算不减枝，也不会出现过拟合。

 

#### 10. SVM的kkt条件？用于解决什么问题？

（1）![img](file:///C:\Users\cai\AppData\Local\Temp\ksohtml\wps44B8.tmp.jpg)，即《统计学习方法》P105。它是指函数的最优值必定满足下面条件：

(1) L对各个x求导为零； 
(2) h(x)=0; 
**(3)** $∑α_ig_i(x)=0，αi≥0$

（2）拉格朗日乘子法(Lagrange Multiplier)和KKT(Karush-Kuhn-Tucker)条件是求解约束优化问题的重要方法，在有等式约束时使用拉格朗日乘子法，在有不等约束时使用KKT条件。前提是：只有当目标函数为[凸函数](https://en.wikipedia.org/wiki/Convex_function)时，使用这两种方法才保证求得的是最优解。

#### 11. SVM的核函数？如果使用RBF核函数，可以将原本5维的数据映射到多少维？

（1）

（2）**映射到无限维，因为它是使用泰勒展开式，可以展开到无穷维。**泰勒展开式其实是0-n维的多项式核函数的和。我们知道多项式核函数将低维数据映射到高维(维度是有限的)，那么 对于无限个 不同维的多项式核函数之和 的高斯核，其中也包括 无穷维度 的 多项式核函数。

#### 12. AUC的含义？

AUC是指 **从一堆样本中随机抽一个，抽到正样本的概率 比 抽到负样本的概率 大的可能性**。

AUC的计算方法：**roc曲线下方的面积积分即可，或者大数定律的投点实验**

#### 13. C++实现kmeans，LR，softmax算法

分别参考

* [【机器学习实战之三】：C++实现K-均值（K-Means）聚类算法、](http://blog.csdn.net/lavorange/article/details/28854929)
* [Softmax回归C++实现](http://blog.csdn.net/sq8912/article/details/45114439)
* [c++实现logistic回归代码](http://blog.csdn.net/u014403897/article/details/45871939)


#### 14. LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？

使用的原因：

答案1: **logistic是基于Bernoulli分布的假设，也就是y|X~Bernoulli分布，而Bernoulli分布的指数族的形式就是$\frac{1}{(1+exp^{-z})}$**

答案2： 对于logistic多分类而言，
x1、x2、...、xn，属于k类的概率正比于：

![img](http://upload-images.jianshu.io/upload_images/1129359-dac233bce2e5a88b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们回到2类：
x1、x2、...xn属于1的概率是：

![img](http://upload-images.jianshu.io/upload_images/1129359-ad46fbe25e77697a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

分子分母同除以分子即为$\frac{1}{(1+exp^{-z})}$，z＝$w_{11}-w_{01}$

**优点**：

1. 数据压缩能力，将数据规约在［0，1］之间，可以很好的模仿概率值。


2. 导数形式优秀，方便计算。


3. 有阈值，并且在阈值附近不会陡升陡降克服了阶跃函数的缺点。

 **缺点**：

1. **容易梯度消失**，x稍大的情况下就趋近一条水平线
2. **非0中心化**，在神经网络算法等情况下，造成反向传播时权重的**全正全负**的情况。



#### 15. **k-means的k怎么取**

如果是有真实数据可以进行聚类效果检验，采用ARI选取当前最大值的 K作为标准。
如果是没有真实数据进行聚类效果检验，则采用calinski_harabaz_score。

![](https://uploadfiles.nowcoder.com/images/20170907/470415_1504777527807_FB5C81ED3A220004B71069645F112867)

#### 16. **Logistics推导**

具体参见博客[http://blog.csdn.net/ice_martin/article/details/61966790](http://blog.csdn.net/ice_martin/article/details/61966790)

![](https://uploadfiles.nowcoder.com/images/20170907/470415_1504777142123_4A47A0DB6E60853DEDFCFDF08A5CA249)

#### 17. 常见的正则化有什么，有什么作用，为什么l1是会把feature压缩到0而l2做不到？

**（1） L1,L2正则化**
L1对应python里面`numpy.linalg.norm(ord=1)`，形如$|w_1|+|w_2|+|w_3|+...$
L2对应python里面`numpy.linalg.norm(ord=2)`，形如$w_1^2+w_2^2+w_3^2+...$

**（2）防止过拟合**
其它防止过拟合的方法还有：

1. 增加数据量
2. 采取bagging算法，抽样训练数据
3. 深度学习里面的Dropout，早停机制
4. 交叉验证。

（3）**画图解决**

![img](http://upload-images.jianshu.io/upload_images/1129359-ffe4f3f34f33f524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

左边的L1，右边的L2.

L1的时候，只要不是特殊情况下与正方形的边相切，一定是与某个顶点优先相交，那必然存在横纵坐标轴中的一个系数为0，起到对变量的筛选的作用。

L2的时候，其实就可以看作是上面这个蓝色的圆，在这个圆的限制下，点可以是圆上的任意一点，所以q＝2的时候也叫做**岭回归，岭回归是起不到压缩变量的作用的**，在这个图里也是可以看出来的。

#### 18. 分类模型如何选择？如何判断效果？

（1)）**整体上讲：数据量越大，神经网络越好；维度越多，bagging算法越优秀；数据量上不多不少的情况下，SVM效果最好；**

（2）**常用判断：roc、auc、ks、f1值、recall等；**

#### 19. LR与随机森林的比较

相同点：

1. 都是监督学习算法；
2. 都是判别模型
3. 都是分类算法

不同点：

1. 随机森林的输出没有概率意义，但是LR有；
2. 随机森林有输入数据扰动和样本属性扰动；
3. 随机森林可以进行特征选择和处理有缺失值的特征，但LR不行；
4. 随机森林可以处理高维度特征，但LR不行；
5. 随机森林可以处理非线性特征，LR不能直接处理。

------

## 数据结构算法

#### 1. 关于最短路径问题，给出n*n格子，格子上有不同数值，求格子上一点A到点B的路径上，数值之和最短的路径？

 

#### 2. 优先队列可以使用什么实现？

使用堆排序实现

 

#### 3. 堆排序的建堆时间复杂度是？

O(n)。在构建堆的过程中，因为是从完全二叉树的最下层最右边的非叶结点开始构建，将它与其孩子进行比较和若有必要的交换，对每个非叶结点，最多进行两次比较和互换操作，这里需要进行这种操作的非叶结点数目是n/2个.

 

#### 4. 图的深度遍历和广度遍历的时间复杂度？

时间复杂度都是O(n^2)或者O(n+e)，后者是使用邻接表的复杂度，n是顶点个数，e个无向图边数，有向图中的弧数。

深度遍历一般使用栈，而广度遍历使用队列。

 

 

#### 5. 爬虫如何实现？一天可以爬取数量？

（1）

（2）一天可以爬取大约16-17万张图片，每分钟大约10张左右图片，10分钟大约115张图片，一个小时大约7000张图片。

 

#### 6. 给出一个元素无序的数组，求出一个数，使得其左边的数都小于它，右边的数都大于等于它。

举例：1,2,3,1,2,0,5,6，返回下标6（数字为5）。

思路（1）：

朴素[算法](http://lib.csdn.net/base/datastructure)，对于每一个数，都检测它的左边和右边是否满足题意。

时间复杂度为O（n^2）

思路（2）

使用变量求解：

（1）目前找到符合题意的候选值，nCandid

（2）目前已遍历数组的最大值，nMax：为了选下一次的候选值

（3）目前的候选值是否有效，bIsExist：检测是否需要重新选择候选值

思路：如果候选值有效，可以继续遍历下面的数据

如果候选值小于目前的值，则该候选失效。之后遍历元素时，就要重新选择候选值

选择候选值时，对于某一个元素，如果该元素比之前遍历过元素的最大值还要大nMax，则该元素就为候选。

复杂度：遍历一遍数组即可，时间：O（n），空间O（1）

```c++
1.#include <iostream>  
2.#include <assert.h>  
3.#include <list>  
4.using namespace std;  
5.  
6.int FindNum(int nArr[],int nLen)  
7.{  
8.    assert(nArr && nLen > 0);  
9.    int nPos = 0;  
10.    int nCandid = nArr[0];  
11.    int nMax = nArr[0];  
12.    bool bIsExist = true;  
13.    for (int i = 1;i < nLen;i++)  
14.    {  
15.        if (bIsExist)//候选有效  
16.        {  
17.            if (nCandid > nArr[i])//候选失效  
18.            {  
19.                bIsExist = false;  
20.            }  
21.            else  
22.            {  
23.                nMax = nArr[i];  
24.            }  
25.        }  
26.        else //候选失效  
27.        {  
28.            if (nArr[i] >= nMax)//重新找到候选  
29.            {  
30.                bIsExist = true;  
31.  
32.                nCandid = nArr[i];  
33.                nMax = nArr[i];   
34.                nPos = i;  
35.            }  
36.        }  
37.    }  
38.    return bIsExist ? nPos : -1;   
39.}  
```

#### 7. 给定N张扑克牌和一个随机函数，设计一个洗牌算法

```c++
void shuffle(int cards[],int n)
{
    if(cards==NULL)
        return ;
 
    srand(time(0));
 
    for(int i=0;i<n-1;++i)
    {
        //保证每次第i位的值不会涉及到第i位以前
        int index=i+rand()%(n-i);
        int temp=cards[i];
        cards[i]=cards[index];
        cards[index]=temp;
    }
}
```

#### 8. 寻找第k大的数

参考文章[寻找第k大的数](http://blog.csdn.net/panpan639944806/article/details/8167768)

这里列出使用二分法的代码。

```c++
#include <iostream>  
using namespace std ;  
const int N = 8 ;   
const int K = 4 ;  
int partition(int  a[] ,int low , int high)   
{  
  int i = low - 1 ;  
  int j = low;  

  while(j < high)  
  {  
    if(a[j] >=  a[high])  
    {  
      swap( a[i+1] , a[j]) ;          
      i++   ;       
    }  
    j++ ;        
  }  
  //最后处理a[high]   
  swap(a[i+1] , a[high]) ;    
  return i + 1;       
}  


int findk(int  a[] , int low , int high , int k)  
{  
  if(low < high)  
  {  
    int q = partition(a , low , high) ;  

    int len = q - low + 1 ; //表示第几个位置    
    if(len == k)  
      return q ; //返回第k个位置   
    else if(len < k)   
      return findk(a , q + 1 , high , k - len) ;     
    else  
      return findk(a , low , q - 1, k ) ;  
  }  
}  

int main()  
{  
  int a[N] = {5 ,2 ,66 ,23, 11 ,1 ,4 ,55} ;  
  findk(a , 0 , N - 1 , K) ;    

  for(int i = 0 ; i < K ; i++)  
    cout<<a[i]<<endl ;  

  system("pause") ;    
  return 0 ;      
} 
```





------

## C++/python 编程语言

#### 1.  C++和python的类的区别？

C++默认private，Python默认public；

C++可以实现多态，而python没有。

#### 2. 堆区和栈区的区别

 一个由C/C++编译的程序占用的内存分为以下几个部分  

1. **栈区（stack）**—   由**编译器自动分配释放   ，存放函数的参数值，局部变量的值**等。其操作方式类似于[数据结构](http://lib.csdn.net/base/datastructure)中的栈。  
2. **堆区（heap）**   —   一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。
3. **全局区（静态区）（static）**—**全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域**。程序结束后由系统释放。  
4. **文字常量区**   —**常量字符串**就是放在这里的。   程序结束后由系统释放  
5. **程序代码区**—存放**函数体的二进制代码**。  

#### 3. 多态的实现

* 多态是通过**虚函数**来实现，结合动态绑定。
* C++中虚函数使用**虚函数表和 虚函数表指针实现**，**虚函数表是一个类的虚函数的地址表**，用于索引类本身以及父类的虚函数的地 址，假如子类的虚函数重写了父类的虚函数，则对应在虚函数表中会把对应的虚函数替换为子类的 虚函数的地址；**虚函数表指针存在于每个对象中（通常出于效率考虑，会放在对象的开始地址处）， 它指向对象所在类的虚函数表的地址；**在多继承环境下，会存在多个虚函数表指针，分别指向对应 不同基类的虚函数表。


------

## 最后一个问题之你还有什么想问我的？ 

1. 我进去之后会做什么？ 

2. 团队是做什么东西的（业务是什么）？ 

3. 内部项目还是外部项目？ 

4. **就我之前的表现来看，你觉得我的优缺点在哪里？（这个问题可以侧面打探出他对你的评价，而且可以帮助你给自己查漏补缺）** 

5. 偏基础还是偏业务（简单粗暴地说，做基础就是写给程序员用的东西，做业务就是写给用户用的东西）？ 

6. 技术氛围怎么样？主要用到什么技术？有什么开源产出吗？你们做 code review 吗